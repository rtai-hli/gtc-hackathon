{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Assessment:</b> Creating A Basic Researching Agent</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929680ed-a1d5-4e23-923f-cf093b9a3659",
   "metadata": {},
   "source": [
    "**Welcome To The Assessment!** We hope you're ready to apply some of the skills you've learned so far towards building something you've probably seen floating around; a \"researching\" chatbot. The overall idea should be pretty familiar:\n",
    "\n",
    "- **The chatbot should look at your question and look around the internet for some resources.**\n",
    "- **Based on those resources, the chatbot should make an educated guess based on its retrieved information.**\n",
    "\n",
    "This is implemented often in conjunction with LLM interfaces like ChatGPT and Perplexity, and various open-source efforts have cropped up to simplify the process. With that being said, they usually don't rely on the likes of 8B models due to the finicky nature of routing them properly. As such, we will merely be testing you on your ability to implement the following primitives: \n",
    "- **A structured output interface to produce a parseable list.**\n",
    "- **A function to search for web snippets and filter out the most relevant results.**\n",
    "- **A mechanism for accumulating message beyond the control of the user.**\n",
    "- **Some basic prompt engineering artifacts.**\n",
    "\n",
    "Of note, there are many extensions which you should be able to imagine at this point. Perhaps we could have a requerying mechanism somewhere? Or maybe either the user or the agent could criticize and remove entries from the history? Long-term memory does sound appealing, after all. However, we will be focusing on just our simple features as required for two key reasons:\n",
    "- **First, we really don't want to force you to do more engineering than you have to.** Frameworks like LangGraph may have many levers and introduce new primitives very quickly in an attempt to simplify the interface, so any overengineering we do now may become deprecated by the time you're reading this with some simpler off-the-shelf options.\n",
    "- **Secondly, our Llama-3.1-8B model inherently makes this more challenging for us due to its limitations.** This level of challenge is important to understand and work with, since you are better-equipped to decompose harder challenges and leverage your tools to their fullest as you scale up. With that said, a multi-turn long-term-memory research agent implemented with Llama-8B is quite tedious at the moment, with many of the streamlines interfaces assuming a stronger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524c160-7261-445d-b624-90927f69c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581704cf-5fc4-43ce-93f8-ae5f9a1738e6",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Define The Planner\n",
    "\n",
    "For the initial system, please make a minimal-viable \"supervisor\"-style element which tries to delegate tasks. This is a very vague definition, so technically a module that generates a list of tasks is technically viable. So let's start with that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3da50-2dcd-4263-9b26-41a92fb2d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "from course_utils import SCHEMA_HINT\n",
    "\n",
    "##################################################################\n",
    "## TODO: Create an LLM client with the sole intention of generating a plan.\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    ## TODO: Define a variable of choice, including useful prompt engineering/restrictions\n",
    "    pass\n",
    "\n",
    "planning_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a master planner system who charts out a plan for how to solve a problem.\"\n",
    "        ## TODO: Perform some more prompt engineering. Maybe consider including the schema_hint\n",
    "    )),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "## TODO: Construct the necessary components to create the chain\n",
    "planning_chain = None\n",
    "\n",
    "input_msgs = {\"messages\": [(\"user\", \"Can you help me learn more about LangGraph?\")]}\n",
    "\n",
    "## For convenience, we have defined a \n",
    "step_buffer = []\n",
    "for chunk in planning_chain.stream(input_msgs):\n",
    "    if \"steps\" in chunk:\n",
    "        if len(chunk.get(\"steps\")) > len(step_buffer):\n",
    "            if step_buffer:\n",
    "                print(flush=True)\n",
    "            step_buffer += [\"\"]\n",
    "            print(\" - \", end='', flush=True)\n",
    "        dlen = len(chunk.get(\"steps\")[-1]) - len(step_buffer[-1])\n",
    "        step_buffer[-1] = chunk.get(\"steps\")[-1]\n",
    "        print(step_buffer[-1][-dlen:], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cb811-5c88-4454-9034-132eccf3ffb5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In an effort to help modularize this process for later, feel free to use this generator wrapper. This is effectively just the same process, but now yielding the results out to be processed by its caller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23e178-53cf-4965-9b8c-c52065a4be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thoughts(input_msgs, config=None):\n",
    "    step_buffer = [\"\"]\n",
    "    for chunk in planning_chain.stream(input_msgs, config=config):\n",
    "        if \"steps\" in chunk and chunk.get(\"steps\"):\n",
    "            if len(chunk.get(\"steps\")) > len(step_buffer):\n",
    "                yield step_buffer[-1]\n",
    "                step_buffer += [\"\"]\n",
    "            dlen = len(chunk.get(\"steps\")[-1]) - len(step_buffer[-1])\n",
    "            step_buffer[-1] = chunk.get(\"steps\")[-1]\n",
    "    yield step_buffer[-1]\n",
    "    print(\"FINISHED\", flush=True)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "for thought in generate_thoughts(input_msgs):\n",
    "    \n",
    "    print(\"-\", thought)\n",
    "    \n",
    "    ## Example Use-Case: Slowing down the generation\n",
    "    # for token in thought:\n",
    "    #     print(token, end=\"\", flush=True)\n",
    "    #     sleep(0.02)\n",
    "    # print(flush=True)\n",
    "    # sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fa5b0-1423-490a-9861-fbc2d85508d5",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Task 2:** Define The Retrieval Sub-Process Mechanism\n",
    "\n",
    "Now that we have a list of steps that we would like to consider, let's use them as a basis for searching the internet. Try implementing a searching mechanism of choice, and try to parallelize/batch this process if possible. \n",
    "\n",
    "- Feel free to implement `search_internet` and `retrieve_via_query` in a manner consistent with the warm-up (`DDGS` + `NVIDIARerank`), or maybe write up your own scheme that you think would be interesting. It may be interesting to implement a loop (agent-as-a-tool?) where you search, expand context, filter, and search again. Conceptually easy, but implementationally more involved.\n",
    "- You may use the `tools` format if you want, but it will not be necessary. Do as you think is interesting.\n",
    "- Our solutions did use `RunnableLambda(...).batch` at some point. Some solutions may also try to leverage `RunnableParallel`. Either-or may be useful, but are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40e7bc-4976-47f6-9f23-0a0e10a5b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.runnables import RunnableLambda\n",
    "# from ddgs import DDGS\n",
    "# import functools\n",
    "\n",
    "####################################################################\n",
    "## TODO: Implement a \"step researcher\" mechanism of choice\n",
    "## We incorporated a 2-step process similar to the example notebook.\n",
    "\n",
    "# @functools.cache  # <- useful for caching duplicate results\n",
    "# def search_internet(final_query: str): \n",
    "#     ## OPTIONAL: We ended up defining this method\n",
    "#     pass \n",
    "     \n",
    "def research_options(steps):\n",
    "    return [] ## TODO\n",
    "\n",
    "search_retrievals = research_options(step_buffer)\n",
    "# search_retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059aec92-076d-4679-b016-e429da3d5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_nvidia import NVIDIARerank\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "## Optional Scaffold\n",
    "def retrieve_via_query(context_rets, query: str, k=5):\n",
    "    return [] ## TODO\n",
    "\n",
    "filtered_results = [retrieve_via_query(search_retrievals, step) for step in step_buffer]\n",
    "# filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a3dea-dfe0-4a16-8482-d792067a0b1a",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Creating The Research Pipeline\n",
    "\n",
    "Now that we have some minimum-viable semblance of a supervisor/subordinate system, let's go ahead and orchestrate them in an interesting way. Feel free to come up with your own mechanism for \"reasoning\" about the question and \"researching\" the results. If you don't see a straightforward way to make it work, a default pool of prompts is offered below (possibly the ones we used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751b561-393a-4083-978f-764fa165bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define the structured prompt template. Doesn't have to be this!\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an agent. Please help the user out! Questions will be paired with relevant context.\"\n",
    "     \" At the end, output the most relevant sources for your outputs, being specific.\"\n",
    "    ),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "intermediate_prompt = \"I can help you look into it. Here's the retrieval: {action} -> {result}\" \n",
    "final_question = \"Great! Now use this information to solve the original question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb376b57-8e58-4b4a-9676-c3a38a8d0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can you help me learn more about LangGraph?\"\n",
    "# question = \"Can you help me learn more about LangGraph? Specifically, can you tell me about Memory Management?\"\n",
    "# question = \"Can you help me learn more about LangGraph? Specifically, can you tell me about Pregel?\"\n",
    "# question = \"Can you help me learn more about LangGraph? Specifically, can you tell me about subgraphs?\"\n",
    "# question = \"Can you help me learn more about LangGraph? Specifically, can you tell me about full-duplex communication?\"\n",
    "# question = \"Can you help me learn more about LangGraph? Specifically, can you tell me about productionalization?\"\n",
    "## TODO: Try your own highly-specialized questions that shouldn't be answerable from priors alone. \n",
    "\n",
    "input_msgs = {\"messages\": [(\"user\", question)]}\n",
    "\n",
    "#########################################################################\n",
    "## TODO: Organize a systen  to reason about your question progressively.\n",
    "## Feel free to use LangChain or LangGraph. Make sure to wind up with \n",
    "## a mechanism that that remembers the reasoning steps for your system\n",
    "\n",
    "sequence_of_actions = [thought for thought in generate_thoughts(input_msgs)]\n",
    "## ...\n",
    "\n",
    "## HINT: We ended up with a for-loop that accumulated intermediate \"question-answer\" pairs\n",
    "## You may also consider a map-reduce-style approach to operate on each step independently.\n",
    "\n",
    "# for action, result in zip(sequence_of_actions, filtered_results):  ## <- possible start-point\n",
    "#     pass\n",
    "\n",
    "input_msgs[\"messages\"] += []\n",
    "\n",
    "# ## HINT: If you wind up with a chain, this may be easy to work with...\n",
    "# print(\"*\"*64)\n",
    "# for token in chain.stream(input_msgs):\n",
    "#     if \"\\n\" in token:\n",
    "#         print(flush=True)\n",
    "#     else: \n",
    "#         print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2063c7-b8b0-4cc0-9e63-25dab62f6c43",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 4:** Accumulating Your Reasoning Traces\n",
    "\n",
    "Depending on the structure of your system, the last requirement may be trivial or might take a bit of extra effort. Please aggregate the answers to 8 diverse and reasonable questions, while also accumulating the trace (i.e. the \"reasoning\", projected to an understandable format). \n",
    "\n",
    "This output will be evaluated by an LLM to assess whether the response seems to exhibit reasonable behavior (reasoning makes sense, final output addresses question, sources are cited, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270640e7-702c-4a60-81f3-665359ecf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Aggregate 8 question-trace-answer triples. \n",
    "# [ \n",
    "#   {\"question\": str, \"trace\": list or dict or str, \"answer\": str}, \n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "submission = [{}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398f84d-e819-4754-bb9e-dd617bb47d42",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Running The Assessment\n",
    "\n",
    "To assess your submission, run the following cells to save your results and the one after to query the assessment runner.\n",
    "\n",
    "**Follow the instructions and make sure it all passes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962990a-f72d-4298-bd23-ae3f6b71c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "## Send the submission over to the assessment runner\n",
    "response = requests.post(\n",
    "    \"http://docker_router:8070/run_assessment\", \n",
    "    json={\"submission\": globals().get(\"submission\", {})},\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "try: \n",
    "    print(response.json().get(\"result\"))\n",
    "    if response.json().get(\"messages\"):\n",
    "        print(\"MESSAGES:\", \"\\n  - \".join([\"\"] + response.json().get(\"messages\")))\n",
    "    if response.json().get(\"exceptions\"):\n",
    "        print(\"EXCEPTIONS:\", \"\\n[!] \".join([\"\"] + [str(v) for v in response.json().get(\"exceptions\")]))\n",
    "except:\n",
    "    print(\"Failed To Process Assessment Response\")\n",
    "    print(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b681838-f605-4352-871c-ae6641b2c5e7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If you passed the assessment, please return to the course page (shown below) and click the **\"ASSESS TASK\"** button, which will generate your certificate for the course.\n",
    "\n",
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f34be-8483-4523-a981-00b7b2477d51",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 6:** Wrapping Up\n",
    "\n",
    "### <font color=\"#76b900\">**Congratulations On Completing The Course!!**</font>\n",
    "\n",
    "Before concluding the course, we highly recommend downloading the course material for later reference, and checking over the **\"Next Steps\"** and **Feedback** sections of the course. **We appreciate you taking the time to go through the course, and look forward to seeing you again for the next courses in the series!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b509fbe9-c24b-498a-8b78-589a6db1ae38",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
