{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Exercise 3:</b> Implementing Our Persona System In LangGraph</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "In the prior section, you explored using LangGraph to orchestrate nodes and edges for a basic agentic workflow. Now, let’s revisit our little persona agent problem from Section 1 involving the teacher, the student, and the parent. We now have a structured outputs and LangGraph experience, so maybe we can orchestrate our system using this new abstraction? You’ll see how to set up each persona’s data, create a unified prompt format, generate structured JSON responses, and chain these agents together in a state graph.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "**In this notebook, we will:**\n",
    "\n",
    "- Get some practice with LangGraph and get comfortable with its state management abstractions.\n",
    "- Implement a proper attempt at routing based on the LLM's direction (in contrast to our failed ReAct attempts in Notebook 2t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b234ae-041e-41fb-bf0f-b2375c57ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c2abb-a8e7-4aea-86e2-1eba2ab12357",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 1:** Pulling In Our Personas\n",
    "\n",
    "You may remember both our bespoke system in base Python and our streamlined system in CrewAI, so let's just pool those specifications together to create our couple of personas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a21fa-3bfa-446f-bace-b82019d9d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_args = dict(\n",
    "    role=\"John Doe (Teacher)\",\n",
    "    backstory=(\n",
    "        \"You are a computer science teacher in high school holding office hours, and you have a meeting.\"\n",
    "        \" This is the middle of the semester, and various students have various discussion topics across your classes.\"\n",
    "        \" You are having a meeting right now. Please engage with the students and help their parent.\"\n",
    "    ), \n",
    "    directive=\"You are having a meeting right now. Please engage with the other speakers and help them out with their concerns.\",\n",
    ")\n",
    "\n",
    "student1_args = dict(\n",
    "    role=\"Jensen (Student)\",\n",
    "    backstory=\"You are taking Dr. Doe's intro to algorithms course and are struggling with some of the homework problems.\", \n",
    "    directive=\"Meet with your teacher to help you understand class material. Respond and ask directed questions, contributing to discussion.\",\n",
    ")\n",
    "\n",
    "student2_args = dict(\n",
    "    role=\"Albert (Student)\",\n",
    "    backstory=\"You are taking Dr. Doe's intro to algorithms course and are struggling with some of the homework problems.\", \n",
    "    directive=\"Meet with your teacher to help you understand class material. Respond and ask directed questions, contributing to discussion.\",\n",
    ")\n",
    "\n",
    "parent_args = dict(\n",
    "    role=\"Sally (Parent)\",\n",
    "    backstory=\"You are here with your kids, who are students in the teacher's class.\", \n",
    "    directive=\"Meet with your kids and the teacher to help support the students and see what you can do better.\",\n",
    ")\n",
    "\n",
    "agent_unique_spec_dict = {args.get(\"role\"):args for args in [teacher_args, student1_args, student2_args, parent_args]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e28eeb-2b6b-4b39-82c5-6923140f76df",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, let’s build a `ChatPromptTemplate` that is flexible enough to apply to each persona. You’ll see placeholders for:\n",
    "- `{role}`, `{backstory}`, and `{directive}` from our agent specs.\n",
    "- A space for the final `schema_hint`, which we will use to help route our system/\n",
    "- The message placeholder which will contain our conversation messages so far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc719c-0123-4f4c-aae4-b7fc0e4d2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from course_utils import SCHEMA_HINT  ## <- Convenience method to get schema hint template\n",
    "\n",
    "## Define the structured prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are {role}. {backstory}\"\n",
    "        \"\\nThe following people are in the room: {role_options}.\"\n",
    "        \" {directive}\\n\" f\"{SCHEMA_HINT}\"\n",
    "    )),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2333cbe-1ef7-41db-a4ed-91d832f2dbd6",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Step 2:** Defining Our Response Schemas\n",
    "\n",
    "Using a similar logic as in Notebook 2, we can endow our systems with structured output to help us not only get a natural language response, but also generate pathing variables which we can then use to route our conversation.\n",
    "\n",
    "Guiding the decoding based on the legal pathways of the current state is a bit hard to manage, but can be controlled by tweaking the schema that is sent in to the LLM endpoint. A convenience method `get_finite_schema` is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03779b20-aa8a-4966-a322-8abb15210846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Literal\n",
    "\n",
    "## Definition of Desired Schema\n",
    "class AgentResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Defines the structured response of an agent in the conversation.\n",
    "    Ensures that each agent response includes the speaker's identity,\n",
    "    a list of response messages, and a defined routing option.\n",
    "    \"\"\"\n",
    "    speaker: Literal[\"option1\", \"option2\"] = Field(description=\"Who are you responding as?\")\n",
    "    response: List[str] = Field(description=\"Response to contribute to the conversation\")\n",
    "    route: Literal[\"option1\", \"option2\"] = Field(description=\"A choice of the next person\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_default(cls):\n",
    "        return cls(speaker=\"option1\", response=[], route=\"option1\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_finite_schema(cls, key_options: Dict[str, List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Dynamically modifies the schema to adjust the possible routing options.\n",
    "        This is useful for ensuring the model respects dynamic conversation flows.\n",
    "        \"\"\"\n",
    "        schema = cls.model_json_schema()\n",
    "        for key, options in key_options.items():\n",
    "            if \"enum\" in schema[\"properties\"].get(key, {}):\n",
    "                schema[\"properties\"][key][\"enum\"] = options\n",
    "            if \"items\" in schema[\"properties\"].get(key, {}):\n",
    "                schema[\"properties\"][key][\"items\"] = {'enum': options, 'type': 'string'}\n",
    "        return schema\n",
    "\n",
    "role_options = list(agent_unique_spec_dict.keys()) + [\"End\"]\n",
    "schema_hint = AgentResponse.get_finite_schema({\"speaker\": role_options[:1], \"route\": role_options})\n",
    "schema_hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b7cf4-4131-45c1-a915-67032bfb3771",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And just like that, we now have both the local and global specification needed to populate our prompt template. These will serve as our arguments for constructing our purpose-build **Agent** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e79153-2ff8-48fa-ba54-8b53ddbaa604",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shared parameters across agents\n",
    "shared_args = dict(\n",
    "    llm=llm, \n",
    "    schema=AgentResponse.get_default(), \n",
    "    schema_hint=schema_hint, \n",
    "    prompt=prompt, \n",
    "    routes=role_options, \n",
    "    roles=role_options\n",
    ")\n",
    "\n",
    "## Initialize agent specifications with shared parameters\n",
    "agent_spec_dict = {\n",
    "    role: {**unique_specs, **shared_args} \n",
    "    for role, unique_specs in agent_unique_spec_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9be66f-8aa0-4489-a5f0-fa68f1593115",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Step 3:** Defining Our Agent Class\n",
    "\n",
    "To help keep the complexity out of our final orchestration graph, we can implement some stateful agents just like the ones found in our CrewAI example. \n",
    "\n",
    "- To stick with our theory-guided approach towards agentics, the interface leading to and from the LLM are called `_convert_to_local` and `_convert_to_global`, respectively. If you check them out, you'll notice they look strikingly familiar.\n",
    "- You'll notice that in `_get_llm`, we parameterize the `get_finite_method` with the classes that we may want our LLM to choose between (or have no choice over). Note that this is not an officially-supported method across LangChain/LangGraph, and is just there to simplify our codebase.\n",
    "- We didn't really make noise about it last time, but you'll notice we're calling the llm with `.invoke`. It sure would be weird if it started streaming the outputs when we actually used the class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d222365-af05-42a1-9706-63060ef4df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Literal, Dict, Any\n",
    "import ast\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Represents an interactive agent that responds to messages in a structured format.\n",
    "    Each agent is initialized with an LLM and a predefined prompt to ensure consistency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, prompt, role, routes, schema=None, **kwargs):\n",
    "        self.llm = llm\n",
    "        self.role = role\n",
    "        self.prompt = prompt\n",
    "        self.routes = routes\n",
    "        self.schema = schema\n",
    "        ## Let's funnel all of our prompt arguments into the default_kwargs\n",
    "        self.default_kwargs = {**kwargs, \"role\": self.role, \"role_options\": \"/\".join(self.routes)}\n",
    "\n",
    "    def __call__(self, config=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Calls the agent with a given message and retrieves a structured response.\n",
    "        \"\"\"\n",
    "        kwarg_pool = {**self.default_kwargs, **kwargs}\n",
    "        global_inputs = kwarg_pool.get(\"messages\")\n",
    "        local_inputs = self._convert_to_local(**{**kwarg_pool, \"messages\": global_inputs})\n",
    "        local_outputs = self._invoke_llm(**{**kwarg_pool, \"messages\": local_inputs})\n",
    "        global_outputs = self._convert_to_global(**{**kwarg_pool, \"messages\": local_outputs})\n",
    "        return global_outputs\n",
    "    \n",
    "    def _get_llm(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Retrieves the LLM with the appropriate structured output schema if available.\n",
    "        \"\"\"\n",
    "        if self.schema:\n",
    "            if hasattr(self.schema, \"get_finite_schema\"):\n",
    "                current_schema = self.schema.get_finite_schema({\n",
    "                    \"speaker\": [self.role], \n",
    "                    \"route\": [r for r in self.routes if r != self.role],\n",
    "                })\n",
    "            else: \n",
    "                current_schema = getattr(self.schema, \"model_json_schema\", lambda: self.schema)()\n",
    "            return self.llm.with_structured_output(schema=current_schema, strict=True)\n",
    "        return self.llm\n",
    "    \n",
    "    def _invoke_llm(self, config=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Invokes the LLM with the constructed prompt and provided configuration.\n",
    "        Adds debugging support to inspect inputs.\n",
    "        \"\"\"\n",
    "        llm_inputs = self.prompt.invoke(kwargs)\n",
    "        # print(\"\\nINPUT TO LLM:\", \"\\n\".join(repr(m) for m in llm_inputs.messages[1:]))\n",
    "        llm_output = self._get_llm(**kwargs).invoke(llm_inputs, config=config)\n",
    "        # print(\"\\nOUTPUT FROM LLM:\", repr(llm_output))\n",
    "        return [llm_output]\n",
    "\n",
    "    def _convert_to_local(self, messages: List[tuple], **kwargs) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Converts input messages into a format suitable for processing by the LLM.\n",
    "        \"\"\"\n",
    "        dict_messages = self._convert_to_global(messages)\n",
    "        roled_messages = [(v.get(\"speaker\"), v.get(\"response\")) for v in dict_messages]\n",
    "        local_messages = list((\n",
    "            \"ai\" if speaker == self.role else \"user\", \n",
    "            f\"[{speaker}] \" + '\\n'.join(content) if isinstance(content, list) else content\n",
    "        ) for speaker, content in roled_messages)\n",
    "        return local_messages\n",
    "    \n",
    "    def _convert_to_global(self, messages, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts various response formats into a structured dictionary format.\n",
    "        Handles potential edge cases, including string responses.\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, tuple):\n",
    "                outputs += [{\"speaker\": msg[0], \"response\": msg[1]}]\n",
    "            elif isinstance(msg, dict):\n",
    "                outputs += [msg]\n",
    "            elif isinstance(msg, str) or hasattr(msg, \"content\"):   \n",
    "                try:\n",
    "                    outputs += [ast.literal_eval(getattr(msg, \"content\", msg))]  ## Strongly assumes good format\n",
    "                except (SyntaxError, ValueError) as e:\n",
    "                    print(f\"Error parsing response: {e}\")\n",
    "                    outputs += [{\"speaker\": \"Unknown\", \"response\": [\"Error encountered\"], \"route\": \"End\"}]\n",
    "            else:\n",
    "                print(f\"Encountered Unknown Message Type: {e}\")\n",
    "                outputs += [{\"speaker\": \"Unknown\", \"response\": [\"Error encountered\"], \"route\": \"End\"}]\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "## Initialize conversation\n",
    "messages = [(\"Jensen (Student)\", \"Hello! How's it going?\")]\n",
    "\n",
    "## Start with the first agent\n",
    "teacher_agent = Agent(**list(agent_spec_dict.values())[0])\n",
    "response = teacher_agent(messages=messages)[0]\n",
    "print(response)\n",
    "\n",
    "## TODO: Route to the next agent based on response\n",
    "\n",
    "## TODO: Continue the conversation for one more turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d192cb-df46-4c9e-9a3c-681448f29578",
   "metadata": {},
   "source": [
    "<details><summary><b>Hint</b></summary>\n",
    "\n",
    "Make sure to take advantage of your message buffer. Maybe the first step is to add the generated message to the buffer? From there, we just need to route to the appropriate agent based on the response...\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "## Start with the first agent\n",
    "teacher_agent = Agent(**list(agent_spec_dict.values())[0])\n",
    "response = teacher_agent(messages=messages)[0]\n",
    "print(response)\n",
    "messages.append((response.get(\"speaker\"), response.get(\"response\")))\n",
    "\n",
    "## TODO: Route to the next agent based on response\n",
    "next_agent = Agent(**agent_spec_dict.get(response.get(\"route\"), {}))\n",
    "response = next_agent(messages=messages)[0]\n",
    "print(response)\n",
    "messages.append((response.get(\"speaker\"), response.get(\"response\")))\n",
    "\n",
    "## TODO: Continue the conversation\n",
    "next_agent = Agent(**agent_spec_dict.get(response.get(\"route\"), {}))\n",
    "response = next_agent(messages=messages)[0]\n",
    "print(response)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989413d-f8b5-41ef-a219-fa5db07dc2bc",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 4:** Putting It All Together\n",
    "\n",
    "Now that we have all of these components, we can integrate them together to make our own agent system fit for the use-case. Much in the same way as before, we can do all of this with only a single agent abstraction, but each agent can just have their own class instance. As an exercise, see if you can't construct the agent class without looking at the solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0c5a3-ae40-4265-af24-08b41ca72bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from typing import Annotated, Dict, List, Optional, TypedDict\n",
    "import operator\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    messages: Annotated[list, add_messages] = []\n",
    "    agent_dict: Dict[str, dict]\n",
    "    speakers: List[str] = []  ## <- use this to keep track of routing/enqueueing\n",
    "\n",
    "##################################################################\n",
    "## Define the operations (Nodes) that can happen on your environment\n",
    "\n",
    "def agent(state: State):\n",
    "    \"\"\"Edge option where transition is generated at runtime\"\"\"\n",
    "    agent_dict = state.get(\"agent_dict\")\n",
    "    current_speaker = state.get(\"speakers\")[-1]\n",
    "    ## TODO: If a speaker is retrieved properly, construct the agent connector,\n",
    "    ## generate the response, and route to the appropriate next speaker.\n",
    "    if current_speaker in agent_dict:\n",
    "        current_agent = Agent(**agent_dict[current_speaker])\n",
    "        response = current_agent(**state)[0]\n",
    "        return Command(update={\n",
    "            \"messages\": [(\"ai\", str(response))], \n",
    "            \"speakers\": [response.get(\"route\")],\n",
    "        }, goto=\"agent\")\n",
    "\n",
    "##################################################################\n",
    "## Define the system that organizes your nodes (and maybe edges)\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_edge(START, \"agent\")  ## A start node is always necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a084e0-539e-4904-87de-b541a2262afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_utils import stream_from_app\n",
    "from functools import partial\n",
    "import uuid\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}, \"recursion_limit\": 1000}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "## We can stream over it until an interrupt is received\n",
    "\n",
    "initial_inputs = {\"messages\": [], \"agent_dict\": agent_spec_dict, \"speakers\": list(agent_spec_dict.keys())[:1]}\n",
    "for token in stream_from_app(app_stream, input_buffer=[initial_inputs], verbose=False, debug=False):\n",
    "    if token == \"[Agent]: \":\n",
    "        print(\"\\n\", flush=True)\n",
    "    print(token, end=\"\", flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7afeab-497d-46b8-b2bf-3a23eb7a7a6c",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 5:** Reflecting On This Exercise\n",
    "\n",
    "This may very well be the hardest system you've implemented today. We had to conform to the LangGraph logic, define some custom non-intuitive utilities, and validate our decisions every step of the way. By now you probably can see that this is **significantly harder** than our approach in CrewAI, and that's ok! Part of the appeal of LangGraph is that it is, in fact, a highly-customizable solution which can scale to production with relative ease and a high amount of observability/control defined at any level.\n",
    "\n",
    "You may recall that the LangGraph ReAct loop didn't work very well out-of-the-box from our model, though it was implemented in a sound way such that a different model will actually work better. The fact that we could completely undercut this abstraction and make it exactly like we wanted to, as we did here, is the real feature to appreciate. And also, we needed to offer some practice before the assessment, so... it works out!\n",
    "\n",
    "**In the next section, get ready to try out the assessment to see if you can make an interesting research agent based on the tools we've discussed today! (But before then, the warm-up may be of interest)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ae9f3e-d3eb-4164-8cea-c3f81a399354",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
