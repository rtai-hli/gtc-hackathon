{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Notebook 3:</b> Using LangGraph For Agentic Loops</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "**Welcome to the third section of the course!**\n",
    "\n",
    "We now know that we can lock an LLM client into various useful configurations; we can streamline or manually engineer interfaces around our LLM to enable routing, retrieval, software queries, etc. and can tackle challenges with this formulation at various levels of abstraction. We also briefly tried out LangGraph, which seemed to be the more recommended interface for LangChain's agentic support mechanisms. In this follow-on, we will formalize LangGraph around its pros and cons and see how we can leverage it to create a near-arbitrary agent system.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "- What is LangGraph and why we should learn how to use it.\n",
    "- How we can implement interesting agent systems using the LangGraph abstractions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0fa3f-149e-4720-b32c-f11844ed4bc0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Reasoning In A Complex Environment\n",
    "\n",
    "Recall our ideal agent definition that motivates our decomposition:\n",
    "- An ideal agent can map any input to any output reasonably well, regardless of complexity or length.\n",
    "\n",
    "**In the previous notebooks, we established that creating good output is inherently non-trivial with a typical LLM, but some techniques exist to improve the process:**\n",
    "- **Chain of thought prompting** can help to put the agent on the correct track.\n",
    "- **Algorithmic execution** can help an LLM solve problems with defined algorithms.\n",
    "- **Structured output** can help an LLM parameterize to a given structure.\n",
    "\n",
    "We've now defined how these abilities can be used to help route a system between tools, along paths, and towards desirable configurations. In this section, we're going to investigate the LangChain-recommended mechanisms for implementing such flexible systems, and also try to understand why they might actually be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581704cf-5fc4-43ce-93f8-ae5f9a1738e6",
   "metadata": {},
   "source": [
    "----\n",
    "#### **Tangent: Prerequisite Intuitions**\n",
    "\n",
    "This course strongly assumes that you've taken some of the previous courses in the series, including possibly [**Building LLM Applications with Prompt Engineering**](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+C-FX-11+V1), [**Rapid Application Development with LLMs**](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-26+V1), and [**Building RAG Agents with LLMs**](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-15+V1).\n",
    "\n",
    "**If you've taken these courses, you will know that we *DO NOT NEED* a true agentic framework to have LLM-specified control flow.** All of these systems use some flavor of bespoke systems to create agents that fit with the narrative and allow for control flow modifications.\n",
    "\n",
    "All of the following scenarios are completely viable mechanisms for implementing control flow with LLMs: \n",
    "\n",
    "```python\n",
    "##########################################################\n",
    "## Prompt Engineering-Based Fulfillment (in Prompt Eng)\n",
    "while True:\n",
    "    prompt = (\n",
    "        \"Chat with the person and be a helpful responding assistant.\"\n",
    "        \" If they want to stop, output <STOP> at the end of your message.\\n\\nUSER: \" + input(\"[User]\")\n",
    "    )\n",
    "    output = llm.invoke(prompt)\n",
    "    if output.content.strip().endswith(\"<STOP>\"):\n",
    "        break\n",
    "\n",
    "##########################################################\n",
    "## Structured-Output Fulfillment (in RAG course)\n",
    "llm_routes = {\n",
    "    \"logged_in\": struct_llm1,\n",
    "    \"logged_out\": struct_llm2,\n",
    "    \"reject_path\": reject_llm,\n",
    "}\n",
    "llm_chain = llm_routes[\"logged_out\"]\n",
    "while True:\n",
    "    prompt = \"Fill out the person schema. If found in database, you will see a response. {schema}\\n\\n{input}\"\n",
    "    output_dict = llm_chain({\"schema\": schema, \"input\": input(\"[User]\")})\n",
    "    if output_dict.get(\"name\") and to_db_query(output_dict):\n",
    "        llm_chain = llm_routes[\"logged_in\"]\n",
    "    # ...\n",
    "\n",
    "##########################################################\n",
    "## Running-State Chain (in RAG Course)\n",
    "retrieval_chain = (\n",
    "    RunnablePassthrough() \n",
    "    ## {\"input\", **}\n",
    "    | RunnablePassthrough.assign({\"retrieval\": retriever_chain}) \n",
    "    ## -> {\"input\", \"retrieval\", **}\n",
    "    | RunnablePassthrough.assign({\"requery\": rephrase_chain})    \n",
    "    ## -> {\"input\", \"retrieval\", \"requery\", **}\n",
    "    | RunnablePassthrough.assign({\"response\": prompt | llm | StrOutputParser()})\n",
    "    ## -> {\"input\", \"retrieval\", \"requery\", \"response\"}\n",
    ").invoke({\"input\" : \"hello world!\"})\n",
    "```\n",
    "\n",
    "If you haven't taken these courses, that's ok! But we will be rolling straight into LangGraph to avoid retreading the same ground. The other courses can be revisited and are all available in a self-paced context.\n",
    "\n",
    "> **The important thing to note: This is a framework made specifically for productionalizing agents!** It integrates a lot of complexity which may or may not be needed for simple LLM systems, but we will be rolling with it so that you can interface with state-of-the-art solutions by the end of this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e6331-74a6-472d-ac01-a2d723d333cb",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Section 2:** Introducing LangGraph\n",
    "\n",
    "For this course, we will be introducing **[LangGraph](https://github.com/langchain-ai/langgraph)** as a new addition that allows us to manage the conversation flow using a state graph system. By leveraging LangGraph, we can define the agent's states, transitions, and actions in a structured manner, eliminating the need for a fully-custom event loop. This framework enhances scalability and maintainability, especially when dealing with multi-agent systems or intricate workflows. \n",
    "\n",
    "As any framework, it obviously has its marketing material which communicates tons of great feature sets, so we will just provide a link to the homepage for those interested. As the course progresses, you will figure out the key value adds and make up your own mind about its pros and cons - every framework has them, and all frameworks lead with the former.\n",
    "\n",
    "> <a href=\"https://langchain-ai.github.io/langgraph/\" target=\"_blank\"><img src=\"images/langgraph-intro.png\" style=\"width: 600px\" /></a>\n",
    "> \n",
    "> [**LangGraph Home Page**](https://www.langchain.com/langgraph)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Why Is LangGraph Great Overall?\n",
    "\n",
    "<details>\n",
    "\n",
    "- **Because it considers so much**. LangGraph is highly customizable to a fault and forces you to abide by best-practices and constraints that may make no sense when you're starting out... but will become impactful later when you actually try to customize, scale up, productionalize.\n",
    "- **Because it's already widely-adopted**. There are many examples and off-the-shelf solutions floating around that people can start out with, and quite a few research projects and final deployments are made with it. \n",
    "- **Because the techniques are transferable**. If you can really understand LangGraph, the other frameworks and their pros/cons become easier to think about. If you only encounter the simplest framework, all subsequent frameworks will look overly-complicated.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Then Is LangGraph Better Than Custom?\n",
    "\n",
    "<details>\n",
    "\n",
    "- **When you know you need the abstractions:** Unlike our while-loop which we could massage into a workable multi-state system, LangGraph takes a state graph approach to modeling the agentic traversal process. As such, it incorporates design patterns which scale naturally to non-sequential and even dynamic routines.\n",
    "- **When you don't know where to start, but know you want to productionalize:** Unlike our while-loop which we could massage into a workable multi-state system, LangGraph takes a state graph approach to modeling the agentic traversal process. As such, it incorporates design patterns which scale naturally to non-sequential and even dynamic routines.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### When Is LangGraph Worse Than Custom?\n",
    "\n",
    "<details>\n",
    "\n",
    "- **When you just want to make a simple application with some LLMs and some prompts:** In order to account for various multi-agent-specific feature sets and edge cases, LangGraph implements some strong assumptions which greatly increase its learning curve. If you can implement your solutions in basic LangChain, the runnable paradigm is more than sufficient to streamline your pipeline and bypasses several layers of complexity introduced by LangGraph. If, on the other hand, you know you want to scale your application and can benefit from its well-thought-out features/examples, then perhaps it's worth diving in and getting comfortable.\n",
    "- **When you need the deepest optimizations:** While LangGraph is amazing, there is still room beyond LangGraph for deeper optimizations and stronger modularization. Those looking to make highly-specialized microservices may be interested in custom multithreading/multiprocessing schemes, advanced graph algorithms, and advanced resource management strategies which LangGraph may not offer.\n",
    "- **When you just want some persona agents:** You saw the CrewAI API. That one has a much flatter learning curve, but also is not as customizable and is quite tailored for its use-cases. If you would like a lighter approach, that's the pretty simple on-ramp (but will also be easy to backtrack to).\n",
    "\n",
    "</details>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657425ea-d7f7-4dc0-a809-6df3f4f35c69",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **Part 3: Warming Up to the LangGraph Abstraction**\n",
    "\n",
    "**LangGraph** sits on top of LangChain's lower-level runnables (LCEL) but introduces a stronger concept of **state** and **transitions**. At its core, you define:\n",
    "\n",
    "1. **A State** (in Python terms, a typed dictionary) that captures the relevant information for your application.  \n",
    "2. **Nodes**, each a Python function that *reads* from and *updates* that state.  \n",
    "3. **Edges** that link these nodes in a directed graph—indicating how control should move from one node to another.\n",
    "\n",
    "This is more than just \"build a pipeline.\" By describing nodes and edges, you're essentially describing how your agent will traverse an environment of possibilities.\n",
    "\n",
    "#### **Example:** A Simple 2-Node Graph\n",
    "\n",
    "Below is a compact illustration of what an application implemented with this framework looks like in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40674e60-2a47-45fe-a805-350411fe9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d0120-d859-4424-bc27-f42f2793368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from functools import partial\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    foo: str\n",
    "    human_value: Optional[str]\n",
    "    llm_response: Optional[str]\n",
    "    extra_kwargs: Optional[dict]\n",
    "\n",
    "##################################################################\n",
    "## Define the operations (Nodes) that can happen on your environment\n",
    "\n",
    "def edged_input(state: State):\n",
    "    \"\"\"Edge\" option where transition is generated at runtime\"\"\"\n",
    "    answer = interrupt(\"[User]: \")\n",
    "    print(f\"> Received an input from the interrupt: {answer}\")\n",
    "    return {\"human_value\": answer}           ## <- Edge determined by graph construction (default END)\n",
    "    # return Command(update={\"human_value\": answer}, goto=\"response\")\n",
    "\n",
    "def response(state: State, config=None):\n",
    "    ## Passing in config will cause connector to stream values to state modification buffer. See graph stream later\n",
    "    response = llm.invoke(state.get(\"human_value\"), config=config)\n",
    "    return {\"llm_response\": response}\n",
    "    # return Command(update={\"llm_response\": response}, goto=END)\n",
    "\n",
    "##################################################################\n",
    "## Define the system that organizes your nodes (and maybe edges)\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"input\")  ## A start node is always necessary\n",
    "builder.add_node(\"response\", response)\n",
    "builder.add_node(\"input\", edged_input)\n",
    "builder.add_edge(\"input\", \"response\")\n",
    "\n",
    "##################################################################\n",
    "## A memory management system to keep track of agent state\n",
    "checkpointer = MemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "##################################################################\n",
    "## A config to define which state you want to use (i.e. states[\"thread_id\"] will be active state pool)\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": uuid.uuid4(),\n",
    "    }\n",
    "}\n",
    "\n",
    "app_stream = partial(app.stream, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c719b-2514-4dfd-a0f8-2c72046d6f53",
   "metadata": {},
   "source": [
    "#### Breaking Down the Flow\n",
    "\n",
    "1. **START** → **input**:  \n",
    "   - The system jumps from the built-in `START` placeholder to the node named `\"input\"`.  \n",
    "   - `\"input\"` is tied to the function `edged_input()`, which *interrupts* to ask the user for input. That user input is stored as `state[\"human_value\"]`.\n",
    "       - Interrupt literally breaks the control flow and the graph execution ends into a paused state. \n",
    "\n",
    "3. **input** → **response**: Next, we go to `\"response\"`, calling the `response()` function.\n",
    "\n",
    "    - It takes `state[\"human_value\"]` (the user’s text) and passes it into `llm.invoke(...)`.  \n",
    "    - The resulting LLM output is stored in `state[\"llm_response\"]`.\n",
    "\n",
    "5. **response** → **END**: And then, we will go to some last state.\n",
    "   - We haven’t told LangGraph to proceed anywhere else. Without an additional edge from `\"response\"` to another node (or back to `\"node\"`), it will just end. In other words, the control flow halts into a rest state.\n",
    "   - You can add more edges for more steps, or a loop that returns to `\"input\"` for multi-turn usage.\n",
    "\n",
    "#### Actually Running It\n",
    "\n",
    "There are many ways to run this new chain, and superficially it may look somewhat similar to the runnable stuff we were playing with earlier. However, you'll notice that the interrupt mechanism and state buffers require much more handling. Software engineers who need to deeply integrate this system with their other projects would rejoice from those, but those starting out may admittedly feel a bit lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d253f7-32d0-4220-bf39-7bc97dd89082",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "## Simple Invocation Example\n",
    "\n",
    "## We can stream over it until an interrupt is received\n",
    "for chunk in app_stream({\"foo\": \"abc\"}):\n",
    "    print(\"Chunk Before Command:\", repr(chunk))\n",
    "\n",
    "## If an interrupt is recieved, we can resolve it and continue\n",
    "if \"__interrupt__\" in chunk:\n",
    "    command = Command(resume=input(chunk.get(\"__interrupt__\")[0].value))\n",
    "\n",
    "##################################################################\n",
    "## Follow-Up Example (Without streaming is simple case, with streaming is advanced case)\n",
    "\n",
    "stream_outputs = True\n",
    "\n",
    "if not stream_outputs:\n",
    "    ## Stream just the individual outputs from the state-writing buffer\n",
    "    print(\"\\nSending Command:\", repr(command), \"\\n\")\n",
    "    for chunk in app_stream(command):\n",
    "        print(\"\\nChunk After Command:\", repr(chunk))\n",
    "\n",
    "else:\n",
    "    ## Same thing, but actually populate the message stream with streamed responses auto-magically\n",
    "    seen_metas = dict()\n",
    "    for chunk, meta in app_stream(command, stream_mode=\"messages\"):\n",
    "        if meta.get(\"checkpoint_ns\") not in seen_metas:\n",
    "            print(f\"[{meta.get('langgraph_node')}]: \", end=\"\", flush=True)\n",
    "            # print(f\"\\nNew Buffer Stream Meta: {meta}\\n\")\n",
    "        seen_metas[meta.get(\"checkpoint_ns\")] = meta\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "        if chunk.response_metadata: # or chunk.usage_metadata: \n",
    "            print(f\"\\n\\nChunk with response_metadata: {repr(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e1c50-afc6-41dd-bc5c-2a81a98ed97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65df418-e4f8-4f5c-8b7c-75c5bdcbe67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(app.get_state_history(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055026f6-75b9-477d-95f9-4d0f387ecb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a6771-5078-4a72-b861-91eb33cd1ffe",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 4:** Why Use A Graph Abstraction?\n",
    "\n",
    "A common argument for why this abstraction is so good comes from the camp of classical dialog modeling folks who may argue that \"this is just the correct way to think about dialog.\" There is an argument to be made here, and you're free to think about it. \n",
    "\n",
    "- **Graphs/Networks:** Each **node** is a function that processes or transforms part of your conversation or data. **Edges** specify permissible flows between nodes. For instance, you may want user input first, then an LLM call, then a summarization step, etc.\n",
    "\n",
    "- **Finite State Machines (FSMs):** In classic FSM terms, each *distinct combination* of state variables and node identity can be interpreted as an FSM \"state,\" and each **edge** is a transition triggered by certain conditions. For instance, the code below transitions from `[User Input Node]` to `[LLM Response Node]` unconditionally. In a richer scenario, you might condition on the user’s text.\n",
    "\n",
    "- **Markov Chains:** If you’re dealing with stochastic or probability-driven transitions, you can think of each node and subsequent edges as forming a Markov chain. The key difference is that in Markov chains, transitions are typically governed by probability distributions. In LangGraph, you can incorporate your own branching logic—possibly using the LLM’s output to determine next steps.  \n",
    "\n",
    "> <img src=\"images/quizbot_state_machine.png\" style=\"width: 1000px;\"/>\n",
    "> \n",
    "> <b><a href=\"https://ai.stanford.edu/blog/quizbot/\" target=\"_blank\">Towards an Educational Revolution Through Chatbots (2019)</a></b>\n",
    "\n",
    "<br>\n",
    "\n",
    "**This is great and all, but some of you may not be convinced:**\n",
    "- If you're trained in CS, you'll see that this is just a way of modeling process dependencies with explicit logic. Just like any programming language could.\n",
    "- The especially critical may also find counter-examples where this abstraction, though sufficient, might be better off transposed such that the states are the nodes and the edges are functions. *Consider what a document mesh or a knowledge graphs might require, and why this transposed setup is interesting.*\n",
    "\n",
    "So again, why do we need this if we already know how to code in Python? Well, it's actually not because it's trying to hide the coding and conditional logic from you. **It's trying to hide the agentic loop and its productionalization aspects from you.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec850fe-b779-4f63-b3ce-9f806a3fa217",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **The Agent Loop (For Our Purposes)**\n",
    "\n",
    "Assume that the agentic decomposition is just a way of breaking down an intractibly-complex global function into local operations:\n",
    "\n",
    "$$F(X) = \\sum_{i=0}^n (e_i \\circ f_i \\circ d_i)(x_i) \\text{ for $i$ agents and local environments } {\\bigcup_i x_i} \\subseteq X$$\n",
    "\n",
    "Assume further that this system is especially useful to model system dynamics over time, such that we care about some future state $X_T$ transitioned from some initial state $X_0$ via our repeated application of $F(X)$. Specifically:\n",
    "\n",
    "$$X_T = X_0 + \\sum_{t=0}^{T-1}F(X_t) \\text{ where } X_{t+1} = X_t + F(X_t)$$\n",
    "\n",
    "And then assume that $t$ can be sampled at an arbitrarily-fine resolution, and you have the continuous agent equation! (Feel free to insert the integral in the $[0,T]$ interval, but not necessary). And that's how the real world functions!\n",
    "\n",
    "In computers, agent-based simulations are also structured like this, and there exists a discrete time loop at some level of abstraction. The issues are:\n",
    "- There are a discrete number of processes, and processes invoke other processes.\n",
    "- For a given time-step, the number of processes can be very large or very small. Usually, this can can only be determined by observation.\n",
    "- We like to observe, monitor, control, and version (time-travel), and this requires memory and compute overhead and hard design decisions.\n",
    "- The number of processes can scale dramatically with the number of users, complexity of macro-processes, and increase or resources.\n",
    "\n",
    "And we have reconstructed the problem of process management in computer architectures. Which we generally don't have to worry about. So why bother now?\n",
    "\n",
    "**Because now we're doing it in an high-abstraction framework like Python where discrete-time simulation is very inefficient and hard to scale manually, and need the flexibility to define our own processes propagation, monitoring, concurrent execution, replication, and versioning.** And it turns out that's a challenge, as discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aaf45b-2365-4de8-bb2e-f0b77bacb027",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Does that mean LangGraph is the Answer?**\n",
    "\n",
    "LangGraph is AN answer, and has a relatively small barrier to entry given the level of customization it supports. Therefore, we will continue to use it as our chief abstraction for this course. It still has some clear limitations associated with it, and technically there are higher barrier-of-entry options that have less of those:\n",
    "- **Custom graph systems** can be implemented using some key abstractions, and they can be truly customized for any arbitrary use-cases. This is, in fact, what LangGraph creates and supports under the hood. However, this problem explodes in complexity as you start implementing the feature sets necessary in modern-day LLM applications, and your solutions will be inherently bespoke unless released as a unified framework.\n",
    "- [**NVIDIA Morpheus**](https://www.nvidia.com/en-us/ai-data-science/products/morpheus/) is another useful abstraction which offers advanced data pipeline solutions that could be used to pipe inference streams, track analytics, and optimize pipelines with CUDA-accelerated workflows. With that being said, it doesn't have all of the niceties associated with LangGraph for the agentic use-case. The Morpheus - LangGraph divide is similar to the LangGraph - CrewAI divide in terms of complexity, so it's not actually a bad idea to use, but will have less built-in conveniences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585fa02-92f3-4414-91c5-8e8de948c0ab",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 5:** [Exercise] Adding Simple Routing\n",
    "\n",
    "Now that we're committed to our framework, let's conform to the framework's expectations and implement its signature feature; **routing**. In the immediate next notebook, we will have a more involved example which will show how to integrate the LangGraph event loop with structured output to implement our teased-upon persona agent system from Section 1. Before then, we wanted to include a simple exercise to augment the loop *just enough* to construct a stop condition. \n",
    "\n",
    "- The method `get_nth_message` is provided to get the last (or maybe some other) message from the state that is passed into it.\n",
    "- Using this, try to force the loop to end when the user says \"stop\". Or maybe when the LLM says `stop`? One or the other is ok.\n",
    "- The streaming logic has gotten more complicated and is now a generator than handles more cases (including other state buffer streams and the debug buffer). It is implemented in `stream_from_app_1`, and you should try to use it instead.\n",
    "    - You can also import `stream_from_app` from `course_utils.py` (or transpose it as `from course_utils import stream_from_app`). This one is a bit better and interfaces with a web service! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4ac21-ef16-4579-8715-a4e95df7ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    interactions: Annotated[int, operator.__add__]\n",
    "    extra_kwargs: Optional[dict]\n",
    "\n",
    "def get_nth_message(state: State, n=-1, attr=\"messages\"):\n",
    "    try: return state.get(\"messages\")[n].content\n",
    "    except: return \"\"\n",
    "    \n",
    "##################################################################\n",
    "## Define the operations (Nodes) that can happen on your environment\n",
    "\n",
    "def user(state: State):\n",
    "    \"\"\"Edge\" option where transition is generated at runtime\"\"\"\n",
    "    answer = interrupt(\"[User]:\")\n",
    "    return {\"messages\": [(\"user\", answer)]} \n",
    "\n",
    "def agent(state: State, config=None):\n",
    "    ## Passing in config will cause connector to stream values to state modification buffer. See graph stream later\n",
    "    response = llm.invoke(state.get(\"messages\"), config=config)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def route(state: State, config=None):\n",
    "    ## TODO: In the case of \"stop\" being found in the current state,\n",
    "    ## go to the end. Otherwise, route back to the user.\n",
    "    return {\"interactions\": 1}\n",
    "\n",
    "##################################################################\n",
    "## Define the system that organizes your nodes (and maybe edges)\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"user\")  ## A start node is always necessary\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_node(\"user\", user)\n",
    "builder.add_node(\"route\", route)    ## Route node declaration\n",
    "builder.add_edge(\"user\", \"agent\")\n",
    "builder.add_edge(\"agent\", \"route\")  ## Route edge declaration\n",
    "\n",
    "##################################################################\n",
    "## A memory management system to keep track of agent state\n",
    "checkpointer = MemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "##################################################################\n",
    "## Simple Invocation Example\n",
    "\n",
    "def stream_from_app_simple(app_stream, input_buffer=[{\"messages\": []}], verbose=False, debug=False):\n",
    "    \"\"\"Executes the agent system in a streaming fashion.\"\"\"\n",
    "    seen_metas = dict()\n",
    "    input_buffer = deepcopy(input_buffer)\n",
    "    \n",
    "    while input_buffer:\n",
    "        for mode, chunk in app_stream(input_buffer.pop(), stream_mode=[\"values\", \"messages\", \"updates\", \"debug\"]):\n",
    "            if mode == \"messages\":\n",
    "                chunk, meta = chunk\n",
    "                if meta.get(\"checkpoint_ns\") not in seen_metas:\n",
    "                    caller_node = meta.get(\"langgraph_node\")\n",
    "                    yield f\"[{caller_node.title()}]: \"\n",
    "                seen_metas[meta.get(\"checkpoint_ns\")] = meta\n",
    "                if chunk.content:\n",
    "                    yield chunk.content\n",
    "            elif mode == \"values\" and verbose:\n",
    "                print(\"[value]\", chunk, flush=True)\n",
    "            elif mode == \"updates\":\n",
    "                if verbose: \n",
    "                    print(\"[update]\", chunk, flush=True)\n",
    "                if \"__interrupt__\" in chunk and chunk.get(\"__interrupt__\")[0].resumable:\n",
    "                    user_input = input(\"\\n[Interrupt] \" + chunk.get(\"__interrupt__\")[0].value)\n",
    "                    input_buffer.append(Command(resume=user_input))\n",
    "            elif mode == \"debug\" and debug:\n",
    "                print(f\"[debug] {chunk}\", flush=True)\n",
    "\n",
    "# from course_utils import stream_from_app\n",
    "\n",
    "import time\n",
    "\n",
    "## We can stream over it until an interrupt is received\n",
    "for token in stream_from_app_simple(app_stream, verbose=False, debug=False):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d7d90-04d0-4994-be3d-ed8a63db8af3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**NOTE: If you happen to be using the `stream_from_app` from `course_utils`, the following interface should be accessible:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1347a-b852-4b1f-ac16-6df090a04871",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':3002';\n",
    "element.innerHTML = '<a style=\"color:#76b900;\" target=\"_blank\" href='+url+'><h2>< Link To Trace Frontend ></h2></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d34e0c-df07-4485-925e-35e7b43ed9a1",
   "metadata": {},
   "source": [
    "<br><details><summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def route(state: State, config=None):\n",
    "    ## TODO: In the case of \"stop\" being found in the current state,\n",
    "    ## go to the end. Otherwise, route back to the user.\n",
    "    if \"stop\" in get_nth_message(state, n=-2): \n",
    "        return {\"interactions\": 1}  ## Implied goto=END\n",
    "    return Command(update={\"interactions\": 1}, goto=\"user\")\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b37a09-3c7c-49b1-9d93-81986c156847",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 5:** Reflecting On This Exercise\n",
    "\n",
    "For those of you just starting out with building agent systems of any kind, LangGraph may seem intimidating. In fact, many engineers who have more limited-scope problems can probably get by with just the LangChain abstractions from the previous sections (which the previous courses go into depth on). At the same time, more experienced engineers who have a strong grasp of agentic software paradigms can straddle the line anywhere between primitive-based and framework-enabled orchestration, and can hopefully make the right design decisions to enable their solution at any scale. \n",
    "\n",
    "We like to treat LangGraph as a great starting abstraction which works **by default** for some of the largest models, **with extra effort** for a selection of models, and **with significant modification** for an even-greater set of possible interfaces. *(Unfortunately, the Llama-8B model may fall under the middle or last category, as you will see in the next notebook)*. With that said, at least it has some simple ways into the ecosystem and can scale by default into production use-cases without too much hassle. As such, we will use this framework for the rest of the course as necessary to enable our agentic loops.\n",
    "\n",
    "- In the next **Exercise Notebook**, we will leverage LangGraph to recreate our multi-persona abstraction with next-speaker selection and a custom state system to show that the framework is *at least* flexible enough to create near-arbitrary state interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824e44a-afcb-4c12-acc7-2ac868b577f4",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
