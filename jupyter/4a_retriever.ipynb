{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Assessment Warm-Up:</b> Creating A Basic Retriever Node</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929680ed-a1d5-4e23-923f-cf093b9a3659",
   "metadata": {},
   "source": [
    "We've now been introduced to ReAct as a concept, and could make a toy system that exhibits this property. However, focusing on it as the ideal paradigm isn't necessarily always correct. It is extremely flexible and does have its purposes. When organized by an overarching strong LLM, this type of loop can run for quite a while since the tools can be used to hide details from the main loop. Integrate this system in with some context re-canonicalization step, and you could theorhetically go on forever.\n",
    "\n",
    "From an implementation perspective, creating a coherent system with this is actually quite simple in principle. It's a good exercise, but isn't worth the effort to build in this course as it doesn't show off any new features:\n",
    "\n",
    "> **HINT:** It's the agent loop from Section 3, but the LLM is bound to call functions, the stop condition is when no tool is called, and some effort is needed to make sure the tool responses actually help to enforce a valid prompting strategy.\n",
    "\n",
    "This paradigm is great for ***horizontal agents*** and ***supervisor-style nodes***, where you can keep tossing more functions (\"ways to interface with the environment\") at the LLM while hoping it will pick something up. Hence, why this pattern works better with a stronger LLM where the state of \"it just works\" is easier to achieve.\n",
    "\n",
    "In this notebook, we will try implementing a ***tool-like agent*** system which is tweaked for the specific problem and aims to hide its runtime details from any other main event loops which may be overseeing it. (i.e. the user, an overarching ReAct loop, some other supervisor, etc). In doing so, we will rediscover some of the interfaced from the RAG course while recontextualizing them into our LangGraph workflows.\n",
    "\n",
    "**This exercise is specifically intended to prepare you for the assessment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524c160-7261-445d-b624-90927f69c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "llama_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\", clean_up_tokenization_spaces=True)\n",
    "def token_len(text):\n",
    "    return len(llama_tokenizer.encode(text=text))\n",
    "\n",
    "# !pip install --upgrade langgraph colorama\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581704cf-5fc4-43ce-93f8-ae5f9a1738e6",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Pulling In Some Boilerplate\n",
    "\n",
    "Let's start off by pulling in our old-reliable base specification for a simple multi-turn system. To make this and subsequent processes easier, we will switch to using an entirely Command-based routing scheme, and will try to reuse components as they need integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3da50-2dcd-4263-9b26-41a92fb2d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "from course_utils import stream_from_app\n",
    "\n",
    "##################################################################\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "def user(state: State):\n",
    "    update = {\"messages\": [(\"user\", interrupt(\"[User]:\"))]}\n",
    "    return Command(update=update, goto=\"agent\")\n",
    "    \n",
    "def agent(state: State, config=None):\n",
    "    update = {\"messages\": [llm.invoke(state.get(\"messages\"), config=config)]}\n",
    "    if \"stop\" in state.get(\"messages\")[-1].content: \n",
    "        return update\n",
    "    return Command(update=update, goto=\"start\")\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"start\", lambda state: {})\n",
    "builder.add_node(\"user\", user)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_edge(START, \"start\")\n",
    "builder.add_edge(\"start\", \"user\")\n",
    "app = builder.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "for token in stream_from_app(app_stream, verbose=False, debug=False):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf1b37-6cd1-4397-b961-e23e3e526ab4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In this notebook, we will be combining our simple LangGraph app with the logic of our DLI Instructor prompt from earlier. You may recall that implementation looked something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f30ec-31db-4d5f-a8ce-5a3aa91d5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from functools import partial\n",
    "\n",
    "## Back-and-forth loop\n",
    "core_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "## Am LCEL chain to pass into chat_with_generator\n",
    "chat_chain = core_prompt | llm | StrOutputParser()\n",
    "\n",
    "with open(\"simple_long_context.txt\", \"r\") as f:\n",
    "    full_context = f.read()\n",
    "\n",
    "long_context_state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": full_context,\n",
    "}\n",
    "\n",
    "from course_utils import chat_with_chain\n",
    "\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "chat(long_context_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539022d8-024e-4425-b3aa-e957de6d71fe",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You may also recall that this system was only able to take a couple of questions at a time because the context length would quickly exceed the deployed model's limits. We will try to fix that as part of our exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585fa02-92f3-4414-91c5-8e8de948c0ab",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 2:** Filtering Out The Details\n",
    "\n",
    "Understanding that the context length of our content chatbot is too limited, you may feel included to refine it some more and get down to an even smaller context, but our current entries are already relatively short. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f48738-68c2-43b6-a0a0-80db0cb2efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "context_entries = full_context.split(\"\\n\\n\")\n",
    "context_docs = [Document(page_content=entry) for entry in context_entries if len(entry.split(\"\\n\")) > 2]\n",
    "context_lens = [token_len(d.page_content) for d in context_docs]\n",
    "print(f\"Context Token Length: {sum(context_lens)} ({sum(context_lens)/len(context_lens):.2f} * {len(context_lens)})\")\n",
    "print(f\"Document Token Range: [{min(context_lens)}, {max(context_lens)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a39f1-846e-4ada-8a5c-61f9dae38cd7",
   "metadata": {},
   "source": [
    "Perhaps we could invoke some heuristic to help us know which ones to focus on for any given question. Lucky for us, we have several viable heuristics in the form of **embedding models**! These have been covered at length in other courses, so here's just a high-level overview:\n",
    "\n",
    "**Instead of *autoregressing* a sequence out of another sequence as a response/continuation, an encoder *embeds* the sequence into a per-token embedding, of which a subset (zero-th entry, subset, entire sequence) is used as a semantic encoding of the input.** Let's see which model options we have at our displosal.\n",
    "\n",
    "- A **reranking model** orders a set of document pairs by relevance as its default behavior. This style of model is usually implemented with a *cross-encoder*, which takes both sequences as input and directly predicts a relevance score while actively considering both sequences.\n",
    "- An **embedding model** embeds a document into a semantic embedding space as its default behavior. This style of model is usually implemented with a *bi-encoder*, which takes in one sequence at a time to produce the embedding. However, two embedded entries can be compared using some similarity metric (i.e. cosine similarity).\n",
    "\n",
    "Either model could technically be used for retrieval, so let's go ahead and try both options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec639cb0-cb93-4454-9861-c56ce75a6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## First, we can try out the embedding model, which is commonly used by first constructing a vectorstore.\n",
    "## - Pros: If you have m documents and n queries, you need n inference-time embeddings and m*n similarity comparisons. \n",
    "## - Cons: Prediction of d_i sim q_j uses learned embeddings Emb_D(d_i) and Emb_Q(q_i),\n",
    "##         not a joint learned representation Emb(d_i, q_j). In other words, somewhat less accurate.\n",
    "\n",
    "question = \"Can you tell me about multi-turn agents?\"\n",
    "\n",
    "embed_d = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", base_url='http://llm_client:9000/v1', truncate='END', max_batch_size=128)\n",
    "embed_q = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", base_url='http://llm_client:9000/v1', truncate='END', max_batch_size=128) ## Not necessary\n",
    "vectorstore = FAISS.from_documents(context_docs, embed_d)\n",
    "vectorstore.embedding_function = embed_q\n",
    "retriever = vectorstore.as_retriever()\n",
    "%time retriever.invoke(question, k=5)\n",
    "# %time retriever.invoke(question, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0f7e3-7ea9-4ddf-be2d-ab189babda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import NVIDIARerank\n",
    "\n",
    "## Next, we can try out the reranking model, which is queried directly to get predicted relevance scores.\n",
    "## - Pros: Literally predicts Emb(d_i, q_i), so better joint relationships can be learned. \n",
    "## - Cons: If you have m documents and n queries, you need n*m inference-time embeddings. \n",
    "\n",
    "question = \"Can you tell me about multi-turn agents?\"\n",
    "\n",
    "reranker = NVIDIARerank(model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\", base_url='http://llm_client:9000/v1', top_n=5, max_batch_size=128)\n",
    "%time reranker.compress_documents(context_docs, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35301e-dc2b-4b30-b666-b6ce182b4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker._client.last_inputs\n",
    "# reranker._client.last_response.json()\n",
    "# embed_d._client.last_inputs\n",
    "# embed_d._client.last_response.json()\n",
    "# embed_q._client.last_inputs\n",
    "# embed_q._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe21a57-f1d0-4b98-8fcc-909d422771a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As we can see, this process is very fast at identifying similarities and produces pretty good rankings for this small data pool! More generally:\n",
    "- **The reranking model is greatly preferred when we're dealing with a small pool of values,** since it leverage joint conditioning.\n",
    "- **The embedding model is greatly preferred when dealing with large document pools,** since we can offload much of the embedding burden to the preprocessing stage.\n",
    "\n",
    "For our limited use-case, the choice won't really matter, and you are free to use whichever option you find most compelling. With that said, go ahead and define a `retrieve` function to abstract this decision away. Furthermore, to streamline our handling later down the road, let's also return just the final string content so that we have less problems to worry about later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9b142-71c6-481a-b8d5-e0e182e659e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_via_query(query: str, k=5):\n",
    "    reranker = NVIDIARerank(model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\", base_url='http://llm_client:9000/v1', top_n=k, max_batch_size=128)\n",
    "    rets = reranker.compress_documents(context_docs, query)\n",
    "    return [entry.page_content for entry in rets]\n",
    "\n",
    "retrieve_via_query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97acc0-cbab-4c3d-9839-8db74bb355f8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "From here, we can either make it into a \"schema function,\" a \"tool,\" or a \"node,\" with the following key destinctions:\n",
    "- A **schema function** can be bound to an LLM to force the output to the schema unconditionally.\n",
    "- A **tool** is also a schema function, but is defined implicitly (i.e. structure of input is implied from signature) and is easier to toss into a tolbank.\n",
    "- A **node** operates on and writes to the state buffer of a graph, so it should take in a `state` + `config`, operate on state variables, and output a state buffer modification request.\n",
    "\n",
    "In this exercise, we're actually going to use the retrieval as an always-on feature of the \"retrieval\" agent, so we can bypass the first two and directly make our node function. Let's assume:\n",
    "- We want our node to perform a retrieval on the previous message (i.e. we want the retrieval to happen AFTER the user submits a message, and we want to retrieve based on whatever the user sent).\n",
    "- We want to write the retrieval results to a value `context` to the state buffer, as we will want the next node (an LLM generation) to use the context.\n",
    "    - And we will want to accumulate `context` over time to include all relevant retrievals. That way, we can put the retrieval into the system message and it can continue to contribute to all subsequent outputs. This implies we'll want to store the values in a set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3ab06-dd5d-4ce5-9225-32227e293299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_node(state: State, config=None, out_key=\"context\"):\n",
    "    ## NOTE: Very Naive; Assumes user question is a good query\n",
    "    ret = retrieve_via_query(state.get(\"messages\")[-1].content, k=3)\n",
    "    return {out_key: set(ret)}\n",
    "\n",
    "## After we define the node, we can assess whether or not it would work.\n",
    "\n",
    "## Given an initial empty state...\n",
    "state = {\n",
    "    \"messages\": [], \n",
    "    \"context\": set(),\n",
    "}\n",
    "\n",
    "## Given an update rule explaining how to handle state updates...\n",
    "add_sets = (lambda x,y: x.union(y))\n",
    "\n",
    "## Will the continued accumulation of messages, followed by a continued accumulation of retrievals, function properly?\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"Can you tell me about agents?\")])\n",
    "state[\"context\"] = add_sets(state[\"context\"],  retrieval_node(state)[\"context\"])\n",
    "print(f\"Retriever: {state['context']} ({len(state['context'])})\")\n",
    "\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"How about earth simulations?\")])\n",
    "state[\"context\"] = add_sets(state[\"context\"],  retrieval_node(state)[\"context\"])\n",
    "print(f\"\\nContext: {state['context']} ({len(state['context'])})\")\n",
    "\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"How about earthly agents?\")])\n",
    "state[\"context\"] = add_sets(state[\"context\"],  retrieval_node(state)[\"context\"])\n",
    "print(f\"\\nContext: {state['context']} ({len(state['context'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf4d2a-662f-4981-a094-57ae5c16fe5b",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Adding Retrieval To Our Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c9a47-85f5-4184-9be5-fd43ef0fe032",
   "metadata": {},
   "source": [
    "Now that we have a generally-applicable node with some base assumptions, let's integrate it into our dialog loop from earlier and see if it just works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77843092-fc7d-4dc7-a107-e3aeaa2d60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    context: Annotated[set, (lambda x,y: x.union(y))]\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "def user(state: State):\n",
    "    update = {\"messages\": [(\"user\", interrupt(\"[User]:\"))]}\n",
    "    return Command(update=update, goto=\"retrieval_router\")\n",
    "\n",
    "## TODO: Add the retrieval between user and agent\n",
    "def retrieval_router(state: State):\n",
    "    return Command(update=retrieval_node(state), goto=\"agent\")\n",
    "    \n",
    "def agent(state: State, config=None):\n",
    "    update = {\"messages\": [(agent_prompt | llm).invoke(state, config=config)]}\n",
    "    if \"stop\" in state.get(\"messages\")[-1].content: \n",
    "        return update\n",
    "    return Command(update=update, goto=\"start\")\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"start\", lambda state: {})\n",
    "builder.add_node(\"user\", user)\n",
    "## TODO: Register the new router to the nodepool\n",
    "builder.add_node(\"retrieval_router\", retrieval_router)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_edge(START, \"start\")\n",
    "builder.add_edge(\"start\", \"user\")\n",
    "app = builder.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "for token in stream_from_app(app_stream, verbose=False, debug=False):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bf232-3ca9-437a-8d6d-0c49d2eeab74",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "As you can see, it wasn't too hard to integrate with the way this graph system is defined. We now have an \"always-on\" retrieval system which is always going to naively take our last message and retrieve the most relevant resources for our query... allegedly. However, if you play around with it a bit, you should start to notice that the raw input may not be exactly optimal, so most setups like to first rephrase the input into the canonical input form for the embedding model... but that would introduce latency and increase the time-to-first-token, making our system less responsive. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22d87c-8346-4986-bdf4-00322e6d63cb",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Part 4:** Adding A \"Think Deeper\" Mechanism\n",
    "\n",
    "In this section, we're going to take some minor inspiration from ReAct to give our system multiple levels of thoroughness. Since our retrieval above was so light and is probably sufficient for most use-cases, let's keep it in. However, let's add a more rigorous thinking process that forces both a **query refinement** and a **web search** as part of its execution. \n",
    "\n",
    "This type of mechanism is often called a \"reflection\" mechanism since it can evaluate the output of the LLM and try to correct the execution flow. It largely works from the intuition that it's easier to verify if an output looks good than to generate the output in the first place.\n",
    "\n",
    "We can achieve the querying logic with a single structured output schema, which we can test out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b91f3-e2f7-4c1c-be49-82cc59d7dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_utils import SCHEMA_HINT\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    \"\"\"Queries to help you research across semantic and web resources for more information. Specifically focus on the most recent question.\"\"\"\n",
    "    big_questions: List[str] = Field(description=\"Outstanding questions that need research, in natural language\")\n",
    "    semantic_queries: List[str] = Field(description=\"Questions (3 or more) to ask an expert to get more info to help, expressed in different ways.\")\n",
    "    web_search_queries: List[str] = Field(description=\"Questions (3 or more) that will be sent over to a web-based search engine to gather info.\")\n",
    "\n",
    "def query_node(state: State):\n",
    "    if not state.get(\"messages\"): return {\"queries\": []}\n",
    "    chat_msgs = [\n",
    "        (\"system\", SCHEMA_HINT.format(schema_hint = Queries.model_json_schema())),\n",
    "        (\"user\", \"Corrent Conversation:\\n\" + \"\\n\\n\".join([f\"[{msg.type}] {msg.content}\" for msg in state.get(\"messages\")])),\n",
    "    ]\n",
    "    schema_llm = llm.with_structured_output(schema=Queries.model_json_schema(), strict=True)\n",
    "    response = Queries(**schema_llm.invoke(chat_msgs))\n",
    "    return {\"queries\": [response]}\n",
    "\n",
    "add_queries = (lambda l,x: l+x) \n",
    "\n",
    "state = {\n",
    "    \"messages\": [], \n",
    "    \"queries\": [],\n",
    "}\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"Can you tell me about agents?\")])\n",
    "state[\"queries\"] = add_queries(state[\"queries\"],  query_node(state)[\"queries\"])\n",
    "print(\"Queries:\", state[\"queries\"])\n",
    "\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"How about earth simulations?\")])\n",
    "state[\"queries\"] = add_queries(state[\"queries\"],  query_node(state)[\"queries\"])\n",
    "print(\"\\nQueries:\", state[\"queries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bb72c-91bc-49bf-965a-349e2498d8a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, we have to actually fulfill those requests, so let's bring in both our retrieval function from this notebook and our DDGS search tool from the previous notebook and actually fulfill those requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13668d0b-de54-4379-8bdd-897a7e806125",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HINT: You can paste the retrieval node and search tools directly and just resolve them in fulfill_query\n",
    "\n",
    "# from langchain.tools import tool\n",
    "# \n",
    "# @tool\n",
    "# def search_internet(user_question: List[str], context: List[str], final_query: str):\n",
    "#     \"\"\"Search the internet for answers. Powered by search engine, in Google search format.\"\"\"\n",
    "#     from ddgs import DDGS\n",
    "#     return DDGS().text(final_query, max_results=10)\n",
    "\n",
    "# def retrieval_node(state: State, config=None, out_key=\"context\"):\n",
    "#     ## NOTE: Very Naive; Assumes user question is a good query\n",
    "#     ret = retrieve_via_query(get_nth_message(state, n=-1), k=3)\n",
    "#     return {out_key: set(ret)}\n",
    "\n",
    "def fulfill_queries(queries: Queries, verbose=False):\n",
    "    # big_questions: List[str]\n",
    "    # semantic_queries: List[str]\n",
    "    # web_search_queries: List[str]\n",
    "    from ddgs import DDGS\n",
    "    web_queries = queries.web_search_queries + queries.big_questions\n",
    "    sem_queries = queries.semantic_queries + queries.big_questions\n",
    "    # if verbose: print(f\"Querying for retrievals via {web_queries = } and {sem_queries = }\")\n",
    "    web_ret_fn = lambda q: [\n",
    "        str(f\"{v.get('body')} [Snippet found from '{v.get('title')}' ({v.get('href')})]\") \n",
    "        for v in DDGS().text(q, max_results=4)\n",
    "    ]\n",
    "    sem_ret_fn = retrieve_via_query\n",
    "    web_retrievals = [web_ret_fn(web_query) for web_query in web_queries]\n",
    "    sem_retrievals = [sem_ret_fn(sem_query) for sem_query in sem_queries]\n",
    "    # if verbose: print(f\"Generated retrievals: {web_retrievals = } and {sem_retrievals = }\")\n",
    "    return set(sum(web_retrievals + sem_retrievals, []))\n",
    "\n",
    "retrievals = set()\n",
    "new_rets = fulfill_queries(state[\"queries\"][0], verbose=True)\n",
    "retrievals = retrievals.union(new_rets)\n",
    "print(f\"Retrieved {len(new_rets)} chunks from the internet and the knowledge base\")\n",
    "new_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c975e-4973-420c-aa17-f7899055d0ff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And perfect! We now have an unusably-long context that is, admittedly, better-thought-out but intractably long. Lucky for us, we have a pretty streamlined way of subsetting this with our retriever system, if only we generalize it a bit more.\n",
    "\n",
    "In the following cell, please implement a `format_retrieval` function to create the actual context for the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ee723-1939-4970-927d-daf7f2608751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_retrieval(\n",
    "    queries: Queries, \n",
    "    new_retrievals: list[str], \n",
    "    existing_retrievals: set[str] = set(), \n",
    "    k=5\n",
    "):\n",
    "    # big_questions: List[str]\n",
    "    # semantic_queries: List[str]\n",
    "    # web_search_queries: List[str]\n",
    "    reranker = NVIDIARerank(model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\", base_url='http://llm_client:9000/v1', top_n=(k + len(existing_retrievals)), max_batch_size=128)\n",
    "    docs = [Document(page_content = ret) for ret in new_retrievals]\n",
    "    rets = reranker.compress_documents(docs, \"\\n\".join(queries.big_questions))\n",
    "    return [entry.page_content for entry in rets if entry.page_content not in existing_retrievals][:k]\n",
    "\n",
    "filtered_retrieval = filter_retrieval(state[\"queries\"][0], new_rets)\n",
    "filtered_retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf400e02-6378-4272-976e-8018e7afb112",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And to wrap all of that up, proceed to make a single unified node call that executes this process as part of the routine, preferably without ever writing to the state buffer until the final new retrievals are generated.\n",
    "\n",
    "**We will leave the final combination as an exercise, but the solution is provided for those interested.** After all, this should be prep for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8c7f6-e031-4485-a230-35e3d815997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command, Send\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    context: Annotated[set, (lambda x,y: x.union(y))]\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "         \" If you think nothing in the context accurately answers your question and you should search deeper,\"\n",
    "         \" include the exact phrase 'let me search deeper' in your response to perform a web search.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", (\n",
    "        \"Thank you. I will not restart the conversation and will abide by the context.\"\n",
    "        \" If I need to search more, I will say 'let me search deeper' near the end of the response.\"\n",
    "    )),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "def user(state: State):\n",
    "    update = {\"messages\": [(\"user\", interrupt(\"[User]:\"))]}\n",
    "    return Command(update=update, goto=\"retrieval_router\")\n",
    "\n",
    "## TODO: Add the retrieval between user and agent\n",
    "def retrieval_router(state: State):\n",
    "    return Command(update={\"context\": \"\"}, goto=\"agent\")\n",
    "\n",
    "def agent(state: State, config=None):\n",
    "    if \"END\" in state.get(\"messages\")[-1].content: \n",
    "        return {\"messages\": []}\n",
    "    update = {\"messages\": [(agent_prompt | llm).invoke(state, config=config)]}\n",
    "    if \"New Context Retrieved:\" in state.get(\"messages\")[-1].content:\n",
    "        pass\n",
    "    elif \"let me search deeper\" in update['messages'][-1].content.lower():\n",
    "        return Command(update=update, goto=\"deep_thought_node\")\n",
    "    return Command(update=update, goto=\"start\")\n",
    "\n",
    "def deep_thought_node(state: State, config=None):\n",
    "    ## NOTE: Very Naive; Assumes user question is a good query\n",
    "    deeper_queries = query_node(state)['queries'][0]\n",
    "    new_rets = fulfill_queries(deeper_queries, verbose=True)\n",
    "    new_rets = filter_retrieval(deeper_queries, new_rets, state.get(\"context\"))\n",
    "    update = {\"messages\": [(\"user\", f\"New Context Retrieved: {new_rets}\")]}\n",
    "    return Command(update=update, goto=\"agent\")\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"start\", lambda state: {})\n",
    "builder.add_node(\"user\", user)\n",
    "## TODO: Register the new nodes to the nodepool\n",
    "builder.add_node(\"retrieval_router\", retrieval_router)\n",
    "builder.add_node(\"deep_thought_node\", deep_thought_node)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_edge(START, \"start\")\n",
    "builder.add_edge(\"start\", \"user\")\n",
    "app = builder.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "for token in stream_from_app(app_stream, verbose=False, debug=False):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e9ff4-a6fb-4d8d-8cb2-06ed364b0a1d",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>HINT:</b></summary>\n",
    "    <code>retrieval_router</code> is currently manually injecting a context of \"\", so maybe we can just run the call to our retrieval function with a minimal amount of wrapping?  \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><b>SOLUTION:</b></summary>\n",
    "\n",
    "```python\n",
    "## TODO: Add the retrieval between user and agent\n",
    "def retrieval_router(state: State):\n",
    "    return Command(update=retrieval_node(state), goto=\"agent\")\n",
    "\n",
    "def retrieval_node(state: State, config=None, out_key=\"context\"):\n",
    "    ## NOTE: Very Naive; Assumes user question is a good query\n",
    "    ret = retrieve_via_query(state.get(\"messages\")[-1].content, k=3)\n",
    "    return {out_key: set(ret)}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<hr><br>\n",
    "\n",
    "### **Part 5:** Reflecting On This Exercise\n",
    "\n",
    "And just like that, we have some semblance of a ReAct-style loop, if in a much more limited capacity. Though it's not a \"pool of tools\" approach, it is definitely a \"reflection system\" with build-in routing. It also isn't really a proper \"deep researcher\" since it doesn't actually read the full body of the articles and isn't capable of expanding on the material quite yet, but it does exhibit very basic retrieval simplification to enable a much longer exchange window.\n",
    "\n",
    "**In the next section, get ready to try out the assessment where you will be implementing a reasoning and searching feature based on the techniques presented in this notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e804c0-7785-43a2-a316-72913d8c4459",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
