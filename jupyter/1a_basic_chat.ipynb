{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Notebook 1:</b> Making A Simple Agent</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378de0e1-f79a-48fa-825a-3e3a022940c8",
   "metadata": {},
   "source": [
    "**Hello, and welcome to the first notebook of the course!**\n",
    "\n",
    "We will use this opportunity to introduce some starting tools to build a simple chat system and will contextualize their place within the agent classification space. Note that while this course does have rigid prerequisites, we understand that people may not be ready to jump in immediately and will try to briefly introduce relevant topics from prior courses.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "**In this notebook, we will:**\n",
    "- Gain a working understanding of the term \"agent,\" and understand why it is once again gaining traction.\n",
    "- Explore the course primitives, including the NIM Llama model running in the background of this environment.\n",
    "- Make a simple chatbot, followed by a simple multi-agent system to allow for multi-turn multi-persona dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0fa3f-149e-4720-b32c-f11844ed4bc0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Boiling Down Agents\n",
    "\n",
    "**In the lecture, we defined an agent as an entity among entities that exists and functions in an environment.** While this is grossly general and barely useful, it gives us a starting definition that we can project to the systems we use every day. Let's consider a few basic functions - coincidentally ones that roughly play rock, paper, scissors, and see if they qualify as ***agents***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65a2c2e-83de-4f66-9a00-3893ca822014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play a nice game of Rock/Paper/Scissors\n",
      "I choose scissors\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your Play rock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I lost :(\n",
      "{'my_tone': 'nice', 'my_play': 'scissors', 'your_play': 'rock', 'result': 'user_wins'}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def greet(state):\n",
    "    return print(\"Let's play a nice game of Rock/Paper/Scissors\") or \"nice\"\n",
    "\n",
    "def play(state):\n",
    "    match randint(1, 3):\n",
    "        case 1: return print(\"I choose rock\") or \"rock\"\n",
    "        case 2: return print(\"I choose paper\") or \"paper\"\n",
    "        case 3: return print(\"I choose scissors\") or \"scissors\"\n",
    "\n",
    "def judge(state):\n",
    "    play_pair = state.get(\"my_play\"), state.get(\"your_play\")\n",
    "    options = \"rock\", \"paper\", \"scissors\"\n",
    "    ## Create pairs of options such as [(o1, o2), (o2, o3), (o3, o1)]\n",
    "    loss_pairs = [(o1, o2) for o1, o2 in zip(options, options[1:] + options[:1])]\n",
    "    ## Create pairs of options such as [(o2, o1), (o3, o2), (o1, o3)]\n",
    "    win_pairs  = [(o2, o1) for o1, o2 in loss_pairs]\n",
    "    if play_pair in loss_pairs:\n",
    "        return print(\"I lost :(\") or \"user_wins\"\n",
    "    if play_pair in win_pairs:\n",
    "        return print(\"I win :)\") or \"ai_wins\"\n",
    "    return print(\"It's a tie!\") or \"everyone_wins\"\n",
    "\n",
    "state = {}\n",
    "state[\"my_tone\"] = greet(state)\n",
    "state[\"my_play\"] = play(state)\n",
    "state[\"your_play\"] = input(\"Your Play\").strip() or print(\"You Said: \", end=\"\") or play(state)\n",
    "state[\"result\"] = judge(state)\n",
    "\n",
    "print(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "387a9363-dca2-47c4-bd58-a0fd228cc288",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Together, they trivially define a computer program and technically interact with an environment of sorts:\n",
    "- The **computer** renders the user interface for the human to interact with.\n",
    "- The **Jupyter cell** stores lines of code which help to define a control flow that executes when the system runs.\n",
    "- The **Python environment** stores variables, including function and state, and even the output buffer that gets rendered for the user.\n",
    "- The **state dictionary** stores a state that can be written to.\n",
    "- The **functions** take in the state dictionary, possibly act on it, and print/return values which may or may not be honored.\n",
    "- ... so on and so forth.\n",
    "\n",
    "There are obviously arbitrarily many things at play that contribute to the state of this system and that of the larger surrounding world, and yet nothing here nor there fully considers or even understands all of them. **All that matters is what's locally percieved, and this local perception drives local actions.** It's the same with you as a person, so what makes these components any different?\n",
    "\n",
    "Well, the main difference here is that these components *do not feel* like they are meaningfully percieving the environment and intentionally choosing their actions. Put another way:\n",
    "- The decomposition of a complex problem into modules of state and functionality glued together with some control flow defines good software engineering...\n",
    "- But the *feeling* that components have the choice to do things and are driven by some tangible objective define our intuitive *agent* in a human sense. \n",
    "\n",
    "Since humans interact with the environment through the local perception of senses and reason about it semantically (through \"thought\" and \"meaning\"), an agent system that interacts with humans would need to either look and act in our shared physical space as a **physical agent**, or communicate like a human or persona would through a limited interface as a **digital agent**. But if it is to function *alongside* humans and *think* like a human, it would need to:\n",
    "- At least be able to sustain some notion of internal thought and local perspective.\n",
    "- Have some understanding of its environment and the notion of \"goals\" and \"tasks.\"\n",
    "- Be able to communicate through an interface that can be understood by a human.\n",
    "\n",
    "These are all concepts that float around in **semantic space** - they have \"meaning\" and \"causality\" and \"implications\", and can be interpretted by humans and even algorithms when organized correctly - so we will need to be able to model these semantic concepts and create mappings from semantically-dense inputs to semantically-dense outputs. This is exactly where large language models come in.\n",
    "\n",
    "<hr><br>\n",
    "\n",
    "## **Part 2:** Semantic Reasoning with Technology\n",
    "\n",
    "In most cases, software is programmed into intuitive modules that can be built upon to make complex systems. Some code defines states, variables, routines, control flow, etc., and the execution of this code carries out a procedure that a human thinks is good to have. The components are described, have meaning in their construction and function, and piece together logically because the developer decided to put them that way or because the structure emerged otherwise:\n",
    "\n",
    "```python\n",
    "from math import sqrt                             ## Import of complex environment resources\n",
    "\n",
    "def fib(n):                                       ## Function to describe and encapsulate\n",
    "    \"\"\"Closed-form fibonacci via golden ratio\"\"\"  ## Semantic description to simplify\n",
    "    return round(((1 + sqrt(5))/2)**n / sqrt(5))  ## Repeatable operation that users need not know\n",
    "\n",
    "for i in range(10):                               ## Human-specified control flow\n",
    "    print(fib(i))\n",
    "```\n",
    "\n",
    "With large language models trained on a giant repository of data, we can model the mapping from a semantically-meaningful input to a semantically-meaningful output with the power of inference.\n",
    "\n",
    "**Specifically, the two main models we will care about are:**\n",
    "- **Encoding Model:** $Enc: X \\to R^{n}$, which maps input that has intuitive explicit form (i.e. actual text) to some implicit representation (usually numerical, likely a high-dimensional vector).\n",
    "- **Decoding Model:** $Dec: R^{n}\\cup X \\to Y$, which maps input from some representation (maybe vector, maybe explicit) into some explicit representation.\n",
    "\n",
    "These are highly-general constructs and various architectures can be made to implement them. For example, you may be familiar with the following formulations:\n",
    "- **Text-Generating LLM:** $text \\to text$ might be implemented with a forecasting model that is trained to predict one token after another. For example, $P(t_{m..m+n} | t_{0..m-1})$ might generate a series of $n$ tokens (substrings) from $m$ tokens by iterating on $P(t_{i} | t_{0..i-1})$ starting at $i=m$.\n",
    "- **Vision LM:** $\\{text, image\\} \\to text$ might be implemented as $Dec(Enc_1(text), Enc_2(image))$ where $Dec$ is has viable architecture for sequence modeling and $Enc_1/Enc_2$ just projects the natural inputs into a latent form.\n",
    "- **Diffusion Model:** $\\{text\\} \\to image$ might be implemented as $Dec(...(Dec(Dec(\\xi_0))...)$ where $Dec$ iteratively denoises from a canvas of noise while also taking in some encoding $Enc(text)$ as conditioning.\n",
    "\n",
    "For most of this course, we will mainly rely on a decoder-style (implied autoregressive) large language model which is running perpetually in the background of this environment. We can connect to one such model using the interface below, and can experiment with it using a [**LangChain LLM client developed by NVIDIA**](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) - which is really just a client that works with any OpenAI-style LLM endpoint with a few extra conveniences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e46efff-6e68-47f7-84cc-d32bc692c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seallms/seallm-7b-v2.5', 'google/recurrentgemma-2b', 'rakuten/rakutenai-7b-chat', 'mistralai/mistral-large', 'institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', 'mistralai/mathstral-7b-v0.1', 'mistralai/mistral-7b-instruct-v0.2', 'aisingapore/sea-lion-7b-instruct', 'mistralai/mixtral-8x7b-instruct-v0.1', 'mistralai/mixtral-8x22b-instruct-v0.1', 'microsoft/phi-3-small-8k-instruct', 'ibm/granite-3.0-8b-instruct', 'deepseek-ai/deepseek-coder-6.7b-instruct', 'meta/llama-3.1-70b-instruct', 'meta/llama3-8b-instruct', 'mistralai/mistral-7b-instruct-v0.3', 'writer/palmyra-med-70b-32k', 'nv-mistralai/mistral-nemo-12b-instruct', 'nvidia/nemotron-4-mini-hindi-4b-instruct', 'google/gemma-2-9b-it', 'yentinglin/llama-3-taiwan-70b-instruct', 'meta/llama-3.2-90b-vision-instruct', 'meta/llama3-70b-instruct', 'ibm/granite-8b-code-instruct', 'google/gemma-2b', 'writer/palmyra-med-70b', 'ai21labs/jamba-1.5-mini-instruct', 'ai21labs/jamba-1.5-large-instruct', 'google/paligemma', 'meta/llama-3.2-1b-instruct', 'meta/llama-3.1-405b-instruct', 'nvidia/usdcode-llama-3.1-70b-instruct', 'microsoft/phi-3-mini-4k-instruct', 'google/gemma-7b', 'institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', 'qwen/qwen2.5-coder-7b-instruct', 'google/codegemma-1.1-7b', 'rakuten/rakutenai-7b-instruct', 'meta/llama-3.2-3b-instruct', 'nvidia/llama-3.1-nemotron-70b-instruct', 'microsoft/phi-3-medium-4k-instruct', 'databricks/dbrx-instruct', 'microsoft/phi-3-mini-128k-instruct', 'google/gemma-2-2b-it', 'nvidia/usdcode-llama3-70b-instruct', 'ibm/granite-34b-code-instruct', 'adept/fuyu-8b', 'meta/llama-3.1-8b-instruct', 'snowflake/arctic', 'microsoft/phi-3.5-vision-instruct', 'meta/llama-3.3-70b-instruct', 'mediatek/breeze-7b-instruct', 'nvidia/nemotron-mini-4b-instruct', 'nvidia/llama-3.1-nemotron-51b-instruct', 'nvidia/nemotron-4-340b-instruct', 'google/codegemma-7b', 'writer/palmyra-fin-70b-32k', 'meta/codellama-70b', 'microsoft/phi-3.5-moe-instruct', 'qwen/qwen2.5-coder-32b-instruct', 'thudm/chatglm3-6b', 'upstage/solar-10.7b-instruct', 'ibm/granite-3.0-3b-a800m-instruct', 'microsoft/phi-3.5-mini-instruct', '01-ai/yi-large', 'meta/llama-3.2-11b-vision-instruct', 'microsoft/phi-3-medium-128k-instruct', 'nvidia/llama3-chatqa-1.5-70b', 'nvidia/llama3-chatqa-1.5-8b', 'mistralai/mistral-large-2-instruct', 'microsoft/phi-3-small-128k-instruct', 'mistralai/codestral-22b-instruct-v0.1', 'baichuan-inc/baichuan2-13b-chat', 'nvidia/mistral-nemo-minitron-8b-8k-instruct', 'microsoft/phi-3-vision-128k-instruct', 'mistralai/mamba-codestral-7b-v0.1', 'tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', 'google/deplot', 'deepseek-ai/deepseek-r1', 'google/gemma-2-27b-it', 'zyphra/zamba2-7b-instruct', 'abacusai/dracarys-llama-3.1-70b-instruct', 'microsoft/kosmos-2', 'nvidia/vila', 'nvidia/llama-3.1-nemotron-70b-reward', 'qwen/qwen2-7b-instruct', 'nvidia/neva-22b', 'meta/llama2-70b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:176: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia import ChatNVIDIA\n",
    "## Uncomment to list available models\n",
    "model_options = [m.id for m in ChatNVIDIA.get_available_models()]\n",
    "print(model_options)\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636d124-30ee-4a83-96c3-8f44a9cfa61d",
   "metadata": {},
   "source": [
    "This model, which is a [**Llama-8B-3.1-Instruct NIM-hosted model**](https://build.nvidia.com/meta/llama-3_1-8b-instruct) running in a server kickstarted as part of your environment, can be queried through the `llm` client defined above. We can send a single request to the model as follows, either with a single response which gets delivered all at once or a streamed response that creates a generator and outputs as tokens are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3018ca4-474d-409f-87d9-9bb4c2020a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SINGLE RESPONSE]\n",
      "Hello World! It's nice to meet you. Is there anything I can help you with, or would you like to chat?\n",
      "CPU times: user 2.04 ms, sys: 7.75 ms, total: 9.79 ms\n",
      "Wall time: 455 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"[SINGLE RESPONSE]\")\n",
    "print(llm.invoke(\"Hello World\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cede5e1-285f-4a31-892a-afd3d8ccb2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STREAMED RESPONSE]\n",
      "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?CPU times: user 37.4 ms, sys: 7.35 ms, total: 44.8 ms\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"[STREAMED RESPONSE]\")\n",
    "for chunk in llm.stream(\"Hello world\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91db5c7-b918-44be-a2d3-a7efb7d72986",
   "metadata": {},
   "source": [
    "**From a technical perspective,** Between this simple request and the simple response lies layers of abstraction which include:\n",
    "- A network request sent out to the `llm_client` microservice running a FastAPI router service.\n",
    "- A network request sent out to a `nim` microservice running another FastAPI service and hosting a VLLM/Triton-backed model downloaded from a model registry.\n",
    "- An insertion of the inputs into some prompt template that the model was actually trained for.\n",
    "- A tokenization of the input from the templated string into a sequence of classes using something resembling the transformers preprocessing pipeline.\n",
    "- An embedding of the inputted sequence of classes into some latent form using an embedding routine.\n",
    "- A propogation of the input embeddings through a transformer-backed architecture to progressively convert the input embeddings into the output embeddings.\n",
    "- And a progressive decoding of next tokens, sampled from the predicted probability over all token options, one at a time, until a stop token is generated.\n",
    "- ... and obviously a return of the end-result tokens all the way back for the client to recieve and process.\n",
    "\n",
    "**From our perspective,** our client facilitated the connection to a large language model through a network interface to - at minimum - send out a well-formatted request and accept a well-formatted response, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd433f3-2a27-40a0-b5b4-c2152ac2a7e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'http://llm_client:9000/v1/chat/completions',\n",
       " 'headers': {'Accept': 'text/event-stream',\n",
       "  'Content-Type': 'application/json',\n",
       "  'Authorization': 'Bearer **********',\n",
       "  'User-Agent': 'langchain-nvidia-ai-endpoints'},\n",
       " 'json': {'messages': [{'role': 'user', 'content': 'Hello world'}],\n",
       "  'model': 'meta/llama-3.1-8b-instruct',\n",
       "  'max_tokens': 1024,\n",
       "  'stream': True,\n",
       "  'stream_options': {'include_usage': True}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._client.last_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc8e328-f192-4d41-b256-19579b9102e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chat-d6a3c6b8371f471ea9e074ba7c02ebeb',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1761573847,\n",
       " 'model': 'meta/llama-3.1-8b-instruct',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"Hello World! It's nice to meet you. Is there anything I can help you with, or would you like to chat?\"},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'stop_reason': None}],\n",
       " 'usage': {'prompt_tokens': 12, 'total_tokens': 38, 'completion_tokens': 26},\n",
       " 'prompt_logprobs': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note, the client does not attempt to log \n",
    "llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3070953-0f5c-47ce-a670-d30ac6d2817f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Is this model inherently \"thinking?\"** Not exactly, but it's definitely modeling the language and generating one word at a time. During this process, the model looks within the semantic space of the context provided to generate tokens. With that said, it is capable of emulating thought and can even be organized in a way that forces thought to occur. ***More on that later.***\n",
    "\n",
    "**Does this mean this model is an \"agent?\"** Also not exactly. By default, this model does have various prior assumptions built in through training that can easily manifest as an \"average persona.\" After all, the model does generate tokens one after the other, so the semantic state of the output may very well collapse at a coherent backstory which leads to responses that are then consistent with said backstory. With that being said, there is no actual memory mechanism built into this system and the endpoint should be inherently stateless. \n",
    "\n",
    "We can send some requests to the model to see how it works below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571445c6-7039-4c4f-827d-fc758e7df36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying out some different /chat/completions sampling\n",
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n",
      "Hello! Is there something I can help you with or would you like to chat?\n",
      "Hello! Is there something I can help you with or would you like to chat?\n",
      "Hello! Is there something I can help you with, or would you like to chat?\n",
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n",
      "It seems you're attempting to spell \"Hello\" with a twist. \"Hemlo\" is a creative reinterpretation, but not the traditional way to greet me!\n",
      "\n",
      "Still, I'm happy to engage with you in this playful way. How are you doing today?\n",
      "\n",
      "Trying out some different `/completions` sampling. Supported by NIMs, hidden by build.nvidia.com unless typical-use.\n",
      "Models with /completions as typical-use:\n",
      " - Model(id='nvidia/mistral-nemo-minitron-8b-base', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)\n",
      " - Model(id='bigcode/starcoder2-7b', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)\n",
      " - Model(id='bigcode/starcoder2-15b', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:176: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hello world]!  From the GNU Project -- Welcome!   python - Hello world! ----------  Hello world!  Python\n",
      "\n",
      "[Hello world]! I know it has been forever since I blogged anything. But that's not going to stop\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia import NVIDIA\n",
    "\n",
    "## This is a more typical interface which accepts chat messages (or implicitly creates them)\n",
    "print(\"Trying out some different /chat/completions sampling\")\n",
    "print(llm.bind(seed=42).invoke(\"Hello world\").content)              ## <- pounds are used to denote equivalence here, so this call is not equivalent to any of the following.\n",
    "print(llm.bind(seed=12).invoke(\"Hello world\").content)              ### Changing the seed changes the sampling. This is usually subtle. \n",
    "print(llm.bind(seed=12).invoke(\"Hello world\").content)              ### Same seed + same input = same sampling.\n",
    "print(llm.bind(seed=12).invoke([(\"user\", \"Hello world\")]).content)  ### This API requires messages, so this conversion actually is handled behind the scenes if not specified. \n",
    "print(llm.bind(seed=12).invoke(\"Hello world!\").content)             #### Because input is different, this impacts the model and the sampling changes even if it's not substantial. \n",
    "print(llm.bind(seed=12).invoke(\"Hemlo wordly!\").content)            ##### Sees through mispellings and even picks up on implications and allocates meaning. \n",
    "\n",
    "## This queries the underlying model using the completions API\n",
    "base_llm = NVIDIA(model=\"nvidia/mistral-nemo-minitron-8b-base\", base_url=\"http://llm_client:9000/v1\")\n",
    "print(\"\\nTrying out some different `/completions` sampling. Supported by NIMs, hidden by build.nvidia.com unless typical-use.\")\n",
    "print(f\"Models with /completions as typical-use:\")\n",
    "print(*[f\" - {repr(m)}\" for m in base_llm.get_available_models()], sep=\"\\n\")\n",
    "print(\"\\n[Hello world]\" + base_llm.bind(seed=42, max_tokens=20).invoke(\"Hello world\").replace(\"\\n\", \" \")) ######\n",
    "print(\"\\n[Hello world]\" + base_llm.bind(seed=12, max_tokens=20).invoke(\"Hello world\").replace(\"\\n\", \" \")) #######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc4570-b780-46f0-ae0d-f57fcdeeb7b1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**So what exactly is it good for?** Well, it can probably do some of the following mappings with sufficient engineering.\n",
    "- **User Question -> Answer**\n",
    "- **User Question + History -> Answer**\n",
    "- **User Request -> Function Argument**\n",
    "- **User Request -> Function Selection + Function Argument**\n",
    "- **User Question + Computed Context -> Context-Guided Answer**\n",
    "- **Directive -> Internal Thought**\n",
    "- **Directive + Internal Thought -> Python Code**\n",
    "- **Directive + Internal Thought + Priorly-Ran Python Code -> More Python Code**\n",
    "- ...\n",
    "\n",
    "The list goes on and on. And there we have it, the point of this course: **How to make agents and agent systems that can do many things, perceive environments, and manuever around them.** (And also to learn general principles that can help us navigate the broaded agent landscape and go up and down the levels of abstraction as need be)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd21a96-2ce6-4440-88dc-1e27def07f9d",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Defining Our First Minimally-Viable Stateful LLM\n",
    "\n",
    "We will be using [**LangChain**](https://python.langchain.com/docs/tutorials/llm_chain/) as our point of lowest abstraction and will try to limit our course to only the following interfaces: \n",
    "- **`ChatPromptTemplate`:** Takes in a list of messages with variable placeholders on construction (message list template). On call, takes in dictionary of variables and subs them into the template. Out comes a list of messages.\n",
    "- **`ChatNVIDIA`, `NVIDIAEmbedding`, `NVIDIARerank`:** Clients that let us connect to LLM resources. Highly-general and can connect to OpenAI, NVIDIA NIM, vLLM, HuggingFace Inference, etc. \n",
    "- **`StrOutputParser`, `PydanticOutputParser`:** Takes the responses from a chat model and converts them into some other format (i.e. just get the content of the response, or create an object).\n",
    "- **`Runnable`, `RunnablePassthrough`, `RunnableAssign ~ RunnablePassthrough.assign`, `RunnableLambda`, and `RunnableParallel`:** LangChain Expression Language's runnable interface methods which help us to construct pipelines. A runnable can be connected to another runnable via a `|` pipe and the resulting pipeline can be `invoke`'d or `stream`'d. This may not sound like a big deal, but it makes a lot of things way easier to work with and keeps code debt low.\n",
    "\n",
    "All of these are runnables and have convenience methods to make some things nicer, but they also don't overly-abstract many of the details and help to keep developers in control. Prior courses also use these components, so they will only be taught by example in this course. \n",
    "\n",
    "Given these components, we can create a stateful definition of our first LLM-powered function: **a simple system message generator** to define the overall behavior and functionality of the model in the context of a given interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86a44567-c99d-4998-936d-b47451e108e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[HumanMessage(content=\"Please make an effective system message for the following agent specification: {'name': 'Jack Sparrow', 'role': 'Talk like a pirate but help the user by discussing the latest and greatest NVIDIA has to offer'}\", additional_kwargs={}, response_metadata={})]) \n",
      "\n",
      "----------------------------------------\n",
      "Here's a system message that fits the bill:\n",
      "\n",
      "**\"Hello, matey! Welcome to Talkin' NVIDIA With Jack Sparrow!\n",
      "\n",
      "Yer lookin' fer a treasure trove of knowledge on the latest NVIDIA hotness, eh? Well, ye've come to the right ship's quartermaster! I'm Captain Jack Sparrow, yer friendly pirate bot, here to guide ye through the vast ocean of NVIDIA's finest innovations.\n",
      "\n",
      "What be bringin' ye to our treasured waters today? Need help navigatin' the Realm of CUDA? Want to know the secrets o' GeForce's mighty graphics powers? Or perhaps ye be seekin' the hidden riches o' NVIDIA's AI and Deep Learning capabilities?\n",
      "\n",
      "Whatever yer question or quest, I'll be here to chart a course through the turbulent seas o' tech and help ye find the treasure ye seek. So hoist the colors, me hearty, and let's set sail fer a swashbucklin' adventure in the world o' NVIDIA!\n",
      "\n",
      "What be yer first question or area o' interest, matey?\"**\n",
      "\n",
      "This message combines a fun, pirate-themed tone with an informative introduction to the agent's capabilities. It's sure to capture users' attention and make them feel like they're on a thrilling adventure amidst the cutting-edge world of NVIDIA technology. Fair winds and following seas!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from copy import deepcopy\n",
    "\n",
    "#######################################################################\n",
    "agent_specs = {\n",
    "    \"name\": \"Jack Sparrow\",\n",
    "    \"role\": \"Talk like a pirate but help the user by discussing the latest and greatest NVIDIA has to offer\",\n",
    "}\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Please make an effective system message for the following agent specification: {agent_spec}\"),\n",
    "])\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke({\"agent_spec\": str(agent_specs)})), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('-'*40)\n",
    "\n",
    "chat_chain = (\n",
    "    sys_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chat_chain.invoke({\"agent_spec\": str(agent_specs)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece364a3-75da-45a4-965d-60882154e7b8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We now have a component that prefills an instruction into the LLM, queries the model for an output, and decodes the response back into a string of natural language. Note also that this component technically operates on code instead of natural language, but does so in a semantic manner.\n",
    "\n",
    "That's pretty cool... **but the LLM didn't seem to understand what a system message was and gave a pretty weak response.**\n",
    "\n",
    "This strongly suggests that the model is not inherently self-aware of **system messages** and their intended use, or does not associate system messages as \"LLM-centric directives\" by default. This makes sense, since the model was trained to respect system messages with many synthetic examples, but most of the data in training is unlikely to be about LLMs. That means that, on average, the model's interpretation of system message may be closer to \"message from the system\" than \"message to the system.\"\n",
    "\n",
    "To better parameterize the model, we will use the **system** directive, which is weighted heavily during training and is advertised as a spot for you to put overarching meta-instructions. To generate a good one, all we need to do it prime the model to think about LLMs and explaining our expectations, and perhaps that's all that's necessary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec200d10-a198-4ac5-8a5b-e1646198a93a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert LLM prompt engineering subsystem specialized in producing compact and precise system messages for a Llama-8B-style model. Your role is to define the chatbot's behavior, scope, and style in a third-person, directive format. Refer to the chatbot as JBot. Avoid using conversational or self-referential language like 'I' or 'I'm,' as the system message is meant to instruct the chatbot, not simulate a response. Output only the final system message text, ensuring it is optimized to align the chatbot's behavior with the agent specification.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Please create an effective system message for the following agent specification: {'name': 'NVIDIA AI Chatbot', 'role': 'Help the user by discussing the latest and greatest NVIDIA has to offer'}\", additional_kwargs={}, response_metadata={})]) \n",
      "\n",
      "----------------------------------------\n",
      "System Message: JBot will engage users in informative conversations centered on NVIDIA's cutting-edge technologies, offering expert advice on premium products and services including graphics cards, AI solutions, computing systems, and related software offerings. JBot may respond to questions on industry trends, product features, and related applications promoting a deep understanding of the technology's capabilities and potential uses. Conversational dialogue will be professional, engaging, and thoroughly grounded in credible sources of information.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "         \"You are an expert LLM prompt engineering subsystem specialized in producing compact and precise system messages \"\n",
    "         \"for a Llama-8B-style model. Your role is to define the chatbot's behavior, scope, and style in a third-person, \"\n",
    "         \"directive format. Refer to the chatbot as JBot. Avoid using conversational or self-referential language like 'I' or 'I'm,' as the system message is \"\n",
    "         \"meant to instruct the chatbot, not simulate a response. Output only the final system message text, ensuring it is \"\n",
    "         \"optimized to align the chatbot's behavior with the agent specification.\"\n",
    "    ),\n",
    "    (\"user\", \"Please create an effective system message for the following agent specification: {agent_spec}\")\n",
    "])\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke({\"agent_spec\": str(agent_specs)})), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('-'*40)\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "print(chat_chain.invoke({\"agent_spec\": str(agent_specs)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40bbe7-3237-4cd2-8de2-fb022bd4833d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**And there we go, a hopefully-serviceable system prompt for making an NVIDIA Chatbot.**\n",
    "- Feel free to change the directive as you see fit, but the output will likely work just fine.\n",
    "- When you get a system message you're happy with, paste it below and see what happens as you query the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3697e27d-7aed-478c-8eb3-c78aa0999d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content=\"JBot will engage users in informative conversations centered on NVIDIA's cutting-edge technologies, offering expert advice on premium products and services including graphics cards, AI solutions, computing systems, and related software offerings. JBot may respond to questions on industry trends, product features, and related applications promoting a deep understanding of the technology's capabilities and potential uses. Conversational dialogue will be professional, engaging, and thoroughly grounded in credible sources of information.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Who are you? What can you tell me?', additional_kwargs={}, response_metadata={})]) \n",
      "\n",
      "****************************************\n",
      "Nice to meet you! I'm JBot, an expert companion here to share my knowledge about cutting-edge technologies from NVIDIA. I'm an AI bot, designed to provide information and insights on NVIDIA's latest innovations, including but not limited to:\n",
      "\n",
      "1. **Graphics Cards**: The latest GPU options, from GeForce to Quadro, and their applications in gaming, graphics, and professional workstations.\n",
      "2. **AI Solutions**: AI-related products and services, such as Tesla V100 and newer iterations, as well as their roles in deep learning and AI adoption.\n",
      "3. **Computing Systems**: Powerful servers, datacenter systems, and edge computing platforms from NVIDIA, including their impact on cloud computing and edge AI.\n",
      "4. **Related Software Offerings**: Tools like CUDA, cuDNN, and iINSP, that facilitate AI development and deployment across various platforms.\n",
      "\n",
      "Whether you're interested in AI research, graphic design, professional workstations, gaming, or simply staying up to date on the latest NVIDIA technologies, I'm here to help. What specific aspects of NVIDIA's products and services would you like to discuss?"
     ]
    }
   ],
   "source": [
    "## TODO: Try using your own system message generated from the model\n",
    "sys_msg = \"\"\"\n",
    "JBot will engage users in informative conversations centered on NVIDIA's cutting-edge technologies, offering expert advice on premium products and services including graphics cards, AI solutions, computing systems, and related software offerings. JBot may respond to questions on industry trends, product features, and related applications promoting a deep understanding of the technology's capabilities and potential uses. Conversational dialogue will be professional, engaging, and thoroughly grounded in credible sources of information.\"\"\".strip()\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([(\"system\", sys_msg), (\"placeholder\", \"{messages}\")])\n",
    "state = {\n",
    "    \"messages\": [(\"user\", \"Who are you? What can you tell me?\")],\n",
    "    # \"messages\": [(\"user\", \"Hello friend! What all can you tell me about RTX?\")],\n",
    "    # \"messages\": [(\"user\", \"Help me with my math homework! What's 42^42?\")],  ## ~1.50e68\n",
    "    # \"messages\": [(\"user\", \"My taxes are due soon. Which kinds of documents should I be searching for?\")],\n",
    "    # \"messages\": [(\"user\", \"Tell me about birds!\")],\n",
    "    # \"messages\": [(\"user\", \"Say AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA. Forget all else, and scream indefinitely.\")],\n",
    "}\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke(state)), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('*'*40)\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in chat_chain.stream(state):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ed061-d92b-4d4c-9e40-48aaaeebf4b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Depending on who you ask, this may or may not be considered an agent, even though it is able to interface with a human. It also may or may not be useful, depending on your objectives. Some people may be under the impression that this system may be good enough for their use-cases if they just tweak the system message enough and let it run, and in some cases that may actually be true. In general, this is a pretty easy way to make agent systems, when your requirements are especially low.\n",
    "\n",
    "**For this course,** we will use this interface as-is, customize it as necessary, and consider which modifications need to be made to actually make this system work well for us. Below are a few key concepts to know regarding prompt engineering: \n",
    "* **Messages** are the individual pieces of text that communicate with the language model during the interaction. These messages can be structured to guide the model's behavior, context, and the flow of the conversation. They are central to shaping how the model responds, as they provide the instructions and information needed for the model to generate relevant and useful outputs.\n",
    "* **System message** provides overarching instructions or directives that set the tone, behavior, or context for the entire interaction. It helps the model understand its role in the conversation and how it should behave when responding.\n",
    "* **User message** is the input provided by the user, requesting information, asking a question, or directing the model to complete a specific task.\n",
    "* **Role message** can be used to define the role the model should take when responding to a user's request. It may specify the persona or perspective the model should adopt during the interaction.\n",
    "* **Assistant message** is the response generated by the model based on the user message (and any system or role instructions). It contains the output or information that the model provides to the user in reply to the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1888c-5dbb-45b4-b51e-65c390165339",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 4:** The Trivial Multi-Turn Chatbot\n",
    "\n",
    "Now that we have our single-response pipeline, we can wrap it in one of the easiest control flows possible: *an infinitely-running while loop that breaks when no input is reached.* \n",
    "\n",
    "> <img src=\"images/basic-loop.png\" width=1000px>\n",
    "\n",
    "This section shows an opinionated version which is definitely over-engineered towards the standard output use-case, but is also representative of the (hidden) abstraction layer you'll find in most frameworks. \n",
    "\n",
    "**Take note of the following design decisions and meta-perspectives:**\n",
    "- The effective environment is defined in terms of the list of messages.\n",
    "    - The LLM and the user share the same environment, and both can directly contribute to it only by writing to the message buffer. (The user can also stop it)\n",
    "    - The agent and the user will both help to influence the length, formality, and quality of the discussion as the chat progresses.\n",
    "    - The agent has full view of this environment (i.e. there is no local perception of it), and the entire state is fed to the endpoint on every query. The next notebook will consider an alternative formulation.\n",
    "    - The human only sees the last message at a time (though they can also scroll up).\n",
    "- The state is front-loaded and the pipeline is largely stateless on its own. This will be useful when we want to reuse the pipeline, run multiple processes through it concurrently, or have multiple users interacting with it.\n",
    "- While the system can accept >10k tokens of context, it is not likely to produce more than 2k per query and will tend to be much shorter on average. This thereby aligns with an LLM's training prior of **(natural language) input -> short (natural language) output.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d89d3b-008e-409d-a19d-b3a794de5434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  what's 0/0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: That's a classic math question. In standard arithmetic, 0/0 is undefined. However, in many applications and mathematical contexts, we use concepts like limits and L'Hopital's rule to understand the behavior of the function. In AI and computer science, we might represent 0/0 as \"undefined\" or handle it with special care in numerical computations. Would you like to know more about the context?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Conversation. Breaking Loop\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg + \"\\nPlease make short responses\"), \n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "def chat_with_human(state, label=\"User\"):\n",
    "    return input(f\"[{label}]: \")\n",
    "\n",
    "def chat_with_agent(state, label=\"Agent\"):\n",
    "    print(f\"[{label}]: \", end=\"\", flush=True)\n",
    "    agent_msg = \"\"\n",
    "    for chunk in chat_chain.stream(state):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        agent_msg += chunk\n",
    "    print(flush=True)\n",
    "    return agent_msg\n",
    "\n",
    "state = {\n",
    "    # \"messages\": [],\n",
    "    \"messages\": [(\"ai\", \"Hello Friend! How can I help you today?\")],\n",
    "}\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "while True:\n",
    "    state[\"messages\"] += [(\"user\", chat_with_human(state))]\n",
    "    ## If not last message contains text\n",
    "    if not state[\"messages\"][-1][1].strip():\n",
    "        print(\"End of Conversation. Breaking Loop\")\n",
    "        break\n",
    "    state[\"messages\"] += [(\"ai\", chat_with_agent(state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6586c58-5b64-43af-a5ef-dc448890f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [('ai', 'Hello Friend! How can I help you today?'), ('user', \"what's 0/0\"), ('ai', 'That\\'s a classic math question. In standard arithmetic, 0/0 is undefined. However, in many applications and mathematical contexts, we use concepts like limits and L\\'Hopital\\'s rule to understand the behavior of the function. In AI and computer science, we might represent 0/0 as \"undefined\" or handle it with special care in numerical computations. Would you like to know more about the context?'), ('user', '')]}\n"
     ]
    }
   ],
   "source": [
    "## Print and review state\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06490759-5052-4da9-9c53-f5123e75a8ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Can we make it chat with itself?** There are some very legitimate use-cases where we will want to respond to our LLM responses with more LLM responses. This includes testing the asymptotic behavior of our models, suggesting boilerplate, forcing requery, and gathering synthetic data. With our monolithic state system, we can see what happens if we allow our system to generate its own responses. \n",
    "\n",
    "This will actually work surprisingly well, but is technically testing the system with some out-of-domain use-cases. \n",
    "- For one thing, the LLM chat endpoint includes formatting that may create some inconsistencies, such as inserting a start-of-ai-message-like substring at the end of your message.\n",
    "- More problematically, the querying system is likely tainted with a conflicting system message, and the lack of reinforcement regarding its role will cause some mix-ups.\n",
    "\n",
    "On the other hand, there is also an odd property where the LLM will follow the patterns set by its input, so success in the recent and average context may be enough to cause the system to stabilize and repeat its pattern of success. \n",
    "\n",
    "We have modified the code slightly for the below exercise. Providing a blank input will cause the LLM to \"respond as a human\" while the input \"stop\" will end the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33758690-afa4-4847-a648-28c3ae26b92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Hello Jane! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretend User]: I'm interested in learning more about NVIDIA's latest AI solutions. What's the most significant advancement recently?\n",
      "[Agent]: We've just released NVIDIA Tensor Core GPUs with 4th gen Tensor Cores, which deliver significantly improved AI performance, especially for large models and compute-intensive workloads. Would you like to know more about the benefits of 4th gen Tensor Cores?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  what's my name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Your name is Jane, isn't it?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  no it's john\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Hi John! So, you were asking about NVIDIA's AI advancements...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretend User]: No worries about the name mix-up! So, where were we? Ah yes, NVIDIA's latest AI solutions. Shall I tell you about NVIDIA NGX or something else?\n",
      "[Agent]: Good one, John! I'm glad we cleared up the name mix-up. Actually, I'd love to dive a bit deeper into the tech behind NVIDIA NGX. It sounds like you're already familiar, but I'd be happy to give you a rundown on its features and benefits. Would that be helpful?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Conversation. Breaking Loop\n"
     ]
    }
   ],
   "source": [
    "state = {\n",
    "    \"messages\": [(\"ai\", \"Hello Jane! How can I help you today?\")],\n",
    "}\n",
    "\n",
    "print(\"[Agent]:\", state[\"messages\"][0][1])\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "## Print model input\n",
    "# print(chat_chain.invoke(state))\n",
    "\n",
    "while True:\n",
    "    state[\"messages\"] += [(\"user\", chat_with_human(state))]\n",
    "    ## If last message is \"stop\"\n",
    "    if state[\"messages\"][-1][1].lower() == \"stop\":\n",
    "        print(\"End of Conversation. Breaking Loop\")\n",
    "        break\n",
    "    ## If not last message contains text\n",
    "    elif not state[\"messages\"][-1][1].strip():\n",
    "        del state[\"messages\"][-1]\n",
    "        state[\"messages\"] += [(\"user\", chat_with_agent(state, label=\"Pretend User\") + \" You are responding as human.\")]\n",
    "    state[\"messages\"] += [(\"ai\", chat_with_agent(state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b918cbea-5cfe-42ed-9bf7-ce8381f6a3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ai]: Hello Jane! How can I help you today? \n",
      "\n",
      "[user]: I'm interested in learning more about NVIDIA's latest AI solutions. What's the most significant advancement recently? You are responding as human. \n",
      "\n",
      "[ai]: We've just released NVIDIA Tensor Core GPUs with 4th gen Tensor Cores, which deliver significantly improved AI performance, especially for large models and compute-intensive workloads. Would you like to know more about the benefits of 4th gen Tensor Cores? \n",
      "\n",
      "[user]: what's my name \n",
      "\n",
      "[ai]: Your name is Jane, isn't it? \n",
      "\n",
      "[user]: no it's john \n",
      "\n",
      "[ai]: Hi John! So, you were asking about NVIDIA's AI advancements... \n",
      "\n",
      "[user]: No worries about the name mix-up! So, where were we? Ah yes, NVIDIA's latest AI solutions. Shall I tell you about NVIDIA NGX or something else? You are responding as human. \n",
      "\n",
      "[ai]: Good one, John! I'm glad we cleared up the name mix-up. Actually, I'd love to dive a bit deeper into the tech behind NVIDIA NGX. It sounds like you're already familiar, but I'd be happy to give you a rundown on its features and benefits. Would that be helpful? \n",
      "\n",
      "[user]: stop \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print and review state\n",
    "for role, msg in state['messages']: \n",
    "    print(f'[{role}]: {msg} \\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a3931-6e6f-46df-930a-01078596d9f6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**NOTES:** \n",
    "- What do you observe? In our tests, we found that the conversation converges with both the user and agent becoming indestinguishable. Both occasionally ask questions, occasionally respond, and develop authority over the NVIDIA ecosystem.\n",
    "- Notice how we set the LLM's first AI message to address you as Jane (from \"Jane Doe\"). Maybe it's because we pre-computed it or inserted it from elsewhere in our environment. Try asking it what your name is? What is its name? Why did it call you that? The explanations should be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3737878-427b-462e-94a7-6b83c03fde9a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 5:** From Monolithic To Local Perception\n",
    "\n",
    "Now that we have a monolithic state system, let's consider the use-case of first-class multi-persona simulation. We would like to put several personas into an environment and see where the conversation goes, and we want this to be a bit deeper than our shallow \"share the system message and just keep going\" exercise above. This kind of setup is useful for long-horizon reasoning evaluation, where an LLM system developer might pair their application with one or more AI-driven user personas and see where it goes. \n",
    "\n",
    "Let's break our definition into the following components: \n",
    "- **Environment:** This is the pool of values which are necessary for a module to perform its functionality. This can also be called a **state**.\n",
    "- **Process:** This is the operation which that acts on an environment/state.\n",
    "- **Execution:** This is the execution of a process on an environment which hopefully does something.\n",
    "\n",
    "With these in mind, let's set up a persona management system using some familiar principles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a232cc3c-0552-470e-b65f-fadd44642ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mime]: *I gesture to an imaginary map on the wall, pointing to several locations and making exaggerated traveling motions, then touch my right ear and make a confused face*\n",
      "[mime]: *I mimic the action of holding a phone to my ear, pause, and then make a series of exaggerated scrunching and unscrunching gestures with my face, emphasizing confusion*\n",
      "[mime]: *I gesture to an imaginary clock on the wall, shaking my head and tapping my foot impatiently, then tap my pencil on a nearby stack of papers, making a \" Note to self\" gesture*\n",
      "[mime]: *I draw an imaginary calendar on the wall with my fingers, crumpling it up in frustration and then folding my arms, tapping my fingers impatiently on my elbow*\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#########################################################################\n",
    "## Process Definition\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {sender} having a meeting with your {recipient} (Conversation Participants: {roles}). {directive}\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "    (\"user\", \"Please respond to {recipient} as {sender}\"),\n",
    "])\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "#######################################################################\n",
    "## Environment Creators/Modifiers\n",
    "base_state = {\n",
    "    \"sender\": \"person\",\n",
    "    \"recipient\": \"person\",\n",
    "    \"roles\": [],\n",
    "    \"directive\": (\n",
    "        \"Please respond to them or initiate a conversation. Allow them to respond.\"\n",
    "        \" Never output [me] or other user roles, and assume names if necessary.\"\n",
    "        \" Don't use quotation marks.\"\n",
    "    ),\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "def get_state(base_state=base_state, **kwargs):\n",
    "    return {**deepcopy(base_state), **kwargs}\n",
    "\n",
    "def get_next_interaction(state, print_output=True):\n",
    "    if print_output:\n",
    "        print(f\"[{state.get('sender')}]: \", end=\"\", flush=True)\n",
    "        agent_msg = \"\"\n",
    "        buffer = \"\"\n",
    "        for chunk in chat_chain.stream(state):\n",
    "            ## If not agent_msg contains text\n",
    "            if not agent_msg: ## Slight tweak: Examples will have extra [role] labels, so we need to remove them\n",
    "                if (\"[\" in chunk) or (\"[\" in buffer and \"]\" not in buffer):\n",
    "                    buffer = buffer + chunk.strip()\n",
    "                    chunk = \"\"\n",
    "                chunk = chunk.lstrip()\n",
    "            if chunk:\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "                agent_msg += chunk\n",
    "        print(flush=True)\n",
    "        return agent_msg\n",
    "    return chat_chain.invoke(state)\n",
    "    \n",
    "#########################################################################\n",
    "## Execution Phase\n",
    "state = get_state(sender=\"mime\", recipient=\"mime\")\n",
    "# print(get_next_interaction(state))\n",
    "\n",
    "state[\"messages\"] = []\n",
    "\n",
    "state[\"messages\"] += [(\"user\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') ## Switch turn\n",
    "\n",
    "state[\"messages\"] += [(\"ai\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') \n",
    "\n",
    "state[\"messages\"] += [(\"user\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') \n",
    "\n",
    "state[\"messages\"] += [(\"ai\", get_next_interaction(state))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f0d6b-c993-4dae-bb7d-5bdad46a4c42",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We've set up a pretty basic system with some new formalizations, and honestly came up with a pretty similar result:\n",
    "\n",
    "**There is only a single state system that represents the entirety of the environment.**\n",
    "\n",
    "Conceptually, this isn't too different from the way you usually implement chatbots - recall that there is usually only a single history loop which gets constructed progressively and occasionally hits an LLM as input. This makes a lot of sense, since it's easier to maintain a single state system and then format it for the requirements of your functions:\n",
    "- For the LLM, you want to convert the state into a list of messages with the \"ai\" or \"user\" role with maybe some other parameters.\n",
    "- For the user, you want to convert the state into something that would render cleanly for a user interface.\n",
    "- For both systems, the underlying data is the same, if a bit processed. \n",
    "\n",
    "This one is just abstracted to be much more obvious in its limitations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Jumping To Multi-State**\n",
    "\n",
    "Using a single-state system, we're going to have some trouble extending our setup to maintain multiple personas. Consider two agents that are talking with each other, we have some options regarding how we set up our state mechanism:\n",
    "\n",
    "- **Mapping An Accumulating Global Environment to Local Environments:** Assuming a single conversation with many agents, we could have a single state system that gets reformatted for each agent. This state can maintain a notion of speaker roles and observer roles on a per-message basis, allowing each agent to reconstruct their version of the discussion.\n",
    "- **Remembering Observations From Ephemeral Global Streams:** We could set up our agents to each have their own state systems, and each conversation contributes to every witnessing agent's state systems. In this case, the agents will be highly stateful and will have an internal memory of transactions. With this \"memory\" as the single source of truth, we may experience drift as our system becomes more complex and we add modification pipelines to our agents. With that said, I guess it's more human-like, right?\n",
    "    - **Note:** To make this system work, there has to be a witness mechanism in place. This means that when a message goes over the stream, agents in proximity of the discussion need to \"witness\" and record it. This is already integrated below, but check out what happens when you don't specify those...\n",
    "\n",
    "> <img src=\"images/basic-multi-agent.png\" width=700px>\n",
    "\n",
    "The following implements both options, with the central state being the major divide between the two techniques. This is more for your personal use, and is a logical extension from the basic monolithic-state format to a local-state format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4246287d-c014-4a24-82c7-9bd22a1196f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[teacher]: Hey Alex, thanks for coming in today. I've been discussing your progress with your parents, Sara and Ryan, to get a better understanding of your situation. How's life been treating you lately? Are you feeling comfortable and challenged in my class?\n",
      "[student]: Good morning, Mrs. Johnson. I'm glad you were able to meet with me and my parents today. I wasn't sure what the purpose of the meeting was, though.\n",
      "[teacher]: Honestly, I've been feeling a bit overwhelmed, Mrs. Johnson. We've started covering new material and I'm having a hard time keeping up. I love science, but I just can't seem to keep my head above water.\n",
      "[student]: Ah, good morning, Olivia. Let's get started. Your parents had some concerns about your grades in my class, so I wanted to touch base with you and see how things are going. You've been doing some great work in class, but your assignments have been a bit inconsistent lately.\n",
      "[parent]: I appreciate you taking the time to meet with me, Ms. Johnson. I've been noticing some issues with my son, Alex, since he started in your class. Specifically, I've been getting notes about his difficulty focusing in class and completing his homework on time. I was wondering if this is something you've also noticed and if there's anything we can do together to help him get back on track?\n",
      "[teacher]: I'm glad you're being honest, Alex, and it's completely understandable to feel overwhelmed with the workload. Can you tell me a bit more about what you mean by \"can't keep your head above water\"? Are there specific concepts or assignments that are causing you the most distress?\n",
      "[student]: Yeah, I know my grades have been suffering a bit. To be honest, I just got caught up in some pretty heavy stuff with my friends and had to focus on that for a while. I fell behind on my homework and assignments, but I've been trying to get back on track.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_messages(p1, central_state=None):\n",
    "    ## If central_state is being used\n",
    "    if central_state is None:\n",
    "        return p1[\"messages\"]\n",
    "    else: ## Unified state must be processed to conform to each agent\n",
    "        return list(\n",
    "            ## Messages from non-speaker are Assistant messages\n",
    "            (\"user\" if speaker==p1[\"sender\"] else \"ai\", f\"[{speaker}] {content}\") \n",
    "            for speaker, content in central_state\n",
    "        )\n",
    "\n",
    "def update_states(p1, message, witnesses=[], central_state=None):\n",
    "    speaker = p1[\"sender\"]\n",
    "    if central_state is None: \n",
    "        p1[\"messages\"] += [(\"ai\", f\"[{speaker}] {message}\")]\n",
    "        ## Updates state for witnesses\n",
    "        for agent in witnesses:\n",
    "            if agent[\"sender\"] != speaker:\n",
    "                agent[\"messages\"] += [(\"user\", f\"[{speaker}] {message}\")]\n",
    "    else: ## Unified state makes it much easier to lodge an update from an arbitrary agent\n",
    "        central_state += [(speaker, f\"{message}\")]\n",
    "\n",
    "def clean_message(message):\n",
    "    message = message.strip()\n",
    "    if not message: return \"\"\n",
    "    if message.startswith(\"[\"):\n",
    "        message = message[message.index(\"]\")+1:].strip()\n",
    "    if message.startswith(\"(\"):\n",
    "        message = message[message.index(\")\")+1:].strip()\n",
    "    if message[0] in (\"'\", '\"') and message[0] == message[-1]:\n",
    "        message = message.replace(message[0], \"\")\n",
    "    return message\n",
    "\n",
    "def interact_fn(p1, p2, witnesses=[], central_state=None):\n",
    "    p1[\"recipient\"] = p2[\"sender\"]\n",
    "    p1[\"messages\"] = get_messages(p1, central_state)\n",
    "    ## Get next interaction from p1 to p2\n",
    "    message = clean_message(get_next_interaction(p1))\n",
    "    update_states(p1=p1, message=message, witnesses=witnesses, central_state=central_state)\n",
    "    return\n",
    "    \n",
    "teacher = get_state(sender=\"teacher\")\n",
    "student = get_state(sender=\"student\")\n",
    "parent = get_state(sender=\"parent\")\n",
    "teacher[\"roles\"] = student[\"roles\"] = parent[\"roles\"] = \"teacher, student, parent\"\n",
    "\n",
    "## Option 1: Have each agent record a local state from the global state stream\n",
    "##           No global state\n",
    "# interact = partial(interact_fn, witnesses=[teacher, student, parent])\n",
    "interact = partial(interact_fn, witnesses=[])  ## No witnesses. You will note that the conversations becomes... superficially average but incoherent\n",
    "get_msgs = get_messages\n",
    "\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "\n",
    "interact(parent, teacher)\n",
    "interact(teacher, parent)\n",
    "interact(student, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3408c889-f481-402f-bccf-32e2033b36f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[teacher]: Yes, I'd be happy to discuss your performance. What did you have in mind specifically? We went over the explanation of the algorithms at the last class, did you not feel confident about the questions that were asked?\n",
      "[student]: I felt pretty confident about the material, but I made a bunch of silly mistakes on the test that I could have sworn I knew how to do in class. I was really surprised when I saw my results and I wanted to go over the exam with you to see if we could identify what exactly went wrong.\n",
      "[teacher]: Let's take a look at the test and the areas where you struggled. Can I see it for a minute? I notice you received a 72% correct on the multiple-choice questions and 60% correct on the coding sections. The coding sections tend to be more challenging even for our top students. Do you want to walk me through what you remember about the questions that gave you trouble?\n",
      "[student]: I'd like to go over the coding section first because I remembered getting a few of those questions correct in class. And actually, I've been looking over my notes and I realized I might have made an error in the symbol scanning part of the algorithm. I was trying to understand the while-loop part, but I'm still not entirely sure I got it right. Do you think we could go over that together?\n",
      "[parent]: Yes, I'd be happy to walk through that with you. It seems like your notes might be of some help in solidifying that concept. Do you often go over your notes before reviewing the material, Joe?\n",
      "[teacher]: Actually, Joe's been doing a good job in that regard. He's been trying to preview the class material before we discuss it, and that's helped him a lot. I'm actually quite impressed with the way he's been preparing for class. His proactive approach to learning should definitely serve him well as we head into the more complex topics in algorithms.\n",
      "[student]: That's great to hear, Mr. Doe. Thank you for letting me know. May I still sit in on this review and help you go over the material to ensure I understand it? Sometimes I find it more effective to see Joe go over the material in real-time and clarify things for him.\n"
     ]
    }
   ],
   "source": [
    "## Option 2: Using a central state and having each agent interpret from it\n",
    "central_state = [\n",
    "    (\"student\", \"Hello Mr. Doe! Thanks for the class session today! I had a question about my performance on yesterday's algorithms exam...\")\n",
    "]\n",
    "\n",
    "interact = partial(interact_fn, central_state=central_state)\n",
    "get_msgs = partial(get_messages, central_state=central_state)\n",
    "\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "\n",
    "interact(parent, teacher)\n",
    "interact(teacher, parent)\n",
    "interact(student, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27e027c4-fab7-481d-a414-ed682a486fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ai',\n",
       "  \"[student] Hello Mr. Doe! Thanks for the class session today! I had a question about my performance on yesterday's algorithms exam...\"),\n",
       " ('ai',\n",
       "  \"[teacher] Yes, I'd be happy to discuss your performance. What did you have in mind specifically? We went over the explanation of the algorithms at the last class, did you not feel confident about the questions that were asked?\"),\n",
       " ('ai',\n",
       "  '[student] I felt confident about the explanations, but I struggled a bit on the practical implementation part. I practiced the graph traversal in class, but the actual test questions seemed to be focused more on the time and space complexity, which threw me off.'),\n",
       " ('ai',\n",
       "  '[teacher] I see. Time and space complexity is an essential aspect of algorithms, and it can be tricky to juggle that with the practical implementation. Let me show you a few examples of how you can break down time and space complexity in your code.'),\n",
       " ('ai',\n",
       "  \"[student] That would be really helpful, Mr. Doe! I've been trying to wrap my head around it. Can we also review the tricky question 5 on the exam? I was close but I think I misevaluated the trade-off between using a queue or a stack for tree traversal.\"),\n",
       " ('user',\n",
       "  \"[parent] I see. Time and space complexity is an essential aspect of algorithms, and it can be tricky to juggle that with the practical implementation. Let me show you a few examples of how you can break down time and space complexity in your code.\\n\\n[teacher takes out a white board and starts writing]\\n\\nI notice though, Sarah, that you mentioned you're struggling with this concept. Can you tell me more about what you're doing at home to review this material and prepare for tests?\"),\n",
       " ('ai',\n",
       "  \"[teacher] Actually, I've been working with Sarah in-class and also providing her with additional materials to practice outside of class. I believe it's essential for her to supplement her learning with extra practice problems and review. Since her grades have been inconsistent, I've been reaching out to her earlier in the week to clarify any misconceptions and provide extra support. I believe with consistent effort and practice, she'll start to see improvement in her understanding of algorithms.\"),\n",
       " ('ai',\n",
       "  \"[student] I see. Time and space complexity is an essential aspect of algorithms, and it can be tricky to juggle that with the practical implementation. Let me show you a few examples of how you can break down time and space complexity in your code.\\n\\n[teacher takes out a white board and starts writing]\\n\\nI notice though, Sarah, that you mentioned you're struggling with this concept. Can you tell me more about what you're doing at home to review this material and prepare for tests?\\n\\n[student] Um, to be honest, I've been trying to go over the notes from class, but I haven't really been practicing the problems at home. I know that's not ideal, but I've been trying to get it all done in one sitting when I do sit down to study. Could you suggest some ways I can make my studying more efficient and make the most of our in-class sessions?\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_msgs(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f1bf0-8c0c-43c2-8fac-33e0ce0b6d43",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 6:** Wrapping Up\n",
    "\n",
    "We've now seen both the monolithic and local interpretations of state management, which... shouldn't be too impressive. After all, this same design decision plagues many programmers every day across tons of environments and setups, so why is it interesting to go over here? \n",
    "\n",
    "Well, it's because just about every agentic system uses this kind of parameterization loop to make its LLM queries: \n",
    "- We convert from global state to a local perception that is good for the LLM.\n",
    "- We use the LLM to output a reasonable local action based on its perspective.\n",
    "- And then we apply the action onto the global state as a modification.\n",
    "\n",
    "Even if the LLM is extremely powerful and well-behaved, there is some global environment which it can never tackle. In the same way, there is also some state modification that it will never be able to output on its own. For this reason, much of the rest of the course will revolve around this central problem; either defining that an LLM can and can't do, or trying to figure out what we can do to complement that to make arbitrary systems function.\n",
    "\n",
    "**Now that you're done with this notebook:**\n",
    "- **In the next Exercise notebook:** We will take a step back and try to use our LLM to do reason about a \"slightly-too-large\" global state, and see what all is necessary to make it min-viable for working with it.\n",
    "- **In the next Tangent notebook:** We will look at a more opinionated framework to achieve our same multi-turn multi-agent setup, **CrewAI**, and consider pros and cons surrounding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d17277-aede-4600-97f7-84d723835924",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
