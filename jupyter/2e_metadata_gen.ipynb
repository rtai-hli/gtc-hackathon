{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Exercise 2:</b> Metadata Generation</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "**Welcome to the second exercise!**\n",
    "\n",
    "This is a lean exercise intended to reinforce the concepts of structured output to try and work with course material and even long-form markdown as an exercise medium. Specifically, we will consider how we can generate first realistic metadata, and then an actual jupyter notebook using the tools from our previous section.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "**In this notebook, we will:**\n",
    "\n",
    "- Consider a more involved example of structured output which could be directly applied to synthetic content (*if used responsibly*).\n",
    "- Push beyond the generative priors of your LLM system to improve a longer-form document in an iterative fashion.\n",
    "\n",
    "### **Setup**\n",
    "\n",
    "Before doing this, let's load in our setup from the previous notebook and continue working with it as useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d974c15-ef48-4b64-9006-331a2530f3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: what's going on\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: You started by providing a context about NVIDIA's GTC 2025 Conference workshops. There are no specific user questions posed yet. Would you like to ask a question about any of the workshops listed, or would you like more information about a specific workshop?"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: list the first 3 sessions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: Here are the first 3 sessions listed:\n",
      "\n",
      "1. **Harness NVIDIAâ€™s Advanced Tools for Gen AI in Digital Health** - This training lab explores the capabilities of NVIDIA's AI tools in transforming digital health solutions.\n",
      "2. **Advanced Medical AI Development with MONAI: From Interactive Annotation to Foundation Models** - This hands-on training lab demonstrates how to build end-to-end medical AI workflows using MONAI tools.\n",
      "3. **Building Digital Twin Environments With OpenUSD, NVIDIA Isaac Sim, and ROS: A Hands-On Approach to Robotics Simulation** - This lab focuses on creating a digital twin environment using OpenUSD, NVIDIA Omniverse, and USD Search NIM, and simulating robots within it using NVIDIA Isaac Sim and ROS."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from functools import partial\n",
    "\n",
    "from course_utils import chat_with_chain\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")\n",
    "\n",
    "## Minimum Viable Invocation\n",
    "# print(llm.invoke(\"How is it going? 1 sentence response.\").content)\n",
    "\n",
    "## Back-and-forth loop\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "## Am LCEL chain to pass into chat_with_generator\n",
    "chat_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "with open(\"simple_long_context.txt\", \"r\") as f:\n",
    "    full_context = f.read()\n",
    "\n",
    "long_context_state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": full_context,\n",
    "}\n",
    "\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "chat(long_context_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8501c5b-482d-437a-9c8f-7d9840648cb0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 1:** Generating Simple Metadata\n",
    "\n",
    "In the lecture notebook, we picked up some techniques to generate data simply by asking nicely and enforcing a style. This relied on the model's priors. We noted how every model has some kinds of limitations in this regard. To make things easy, let's start out with an actual productionalizable use-case where even the 8B model shines; **Short-Form Data Extraction**.\n",
    "\n",
    "Our dataset of workshops has a lot of natural-language descriptions and we have a website frontend that requires it to have some sort of a schema, so wouldn't it be great if we could use an LLM to initialize those values?\n",
    "\n",
    "Well, we could define a schema to help us generate these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c11f050-592f-4edf-87e0-ee3b701f4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class MetaCreator(BaseModel):\n",
    "    short_abstract: str = Field(description=(\n",
    "        \"A concise, SEO-optimized summary (1-2 sentences) of the course for students.\"\n",
    "        \" Ensure accuracy and relevance without overstating the workshop's impact.\"\n",
    "    ))\n",
    "    topics_covered: List[str] = Field(description=(\n",
    "        \"A natural-language list of key topics, techniques, and technologies covered.\"\n",
    "        \" Should start with 'This workshop' and follow a structured listing format that lists at least 4 points.\"\n",
    "    ))\n",
    "    abstract_body: str = Field(description=(\n",
    "        \"A detailed expansion of the short abstract, providing more context and information.\"\n",
    "    ))\n",
    "    long_abstract: str = Field(description=(\n",
    "        \"An extended version of the short abstract, followed by the objectives.\"\n",
    "        \" The first paragraph should introduce the topic with a strong hook and highlight its relevance.\"\n",
    "    ))\n",
    "    objectives: List[str] = Field(description=(\n",
    "        \"Key learning outcomes that students will achieve, emphasizing big-picture goals rather than specific notebook content.\"\n",
    "    ))\n",
    "    outline: List[str] = Field(description=(\n",
    "        \"A structured sequence of key topics aligned with major course sections, providing a clear learning path.\"\n",
    "    ))\n",
    "    on_completion: str = Field(description=(\n",
    "        \"A brief summary of what students will be able to accomplish upon completing the workshop.\"\n",
    "    ))\n",
    "    prerequisites: List[str] = Field(description=(\n",
    "        \"Essential prior knowledge and skills expected from students before taking the course.\"\n",
    "    ))\n",
    "\n",
    "def get_schema_hint(schema):\n",
    "    schema = getattr(schema, \"model_json_schema\", lambda: None)() or schema\n",
    "    return ( # PydanticOutputParser(pydantic_object=Obj.model_schema_json()).get_format_instructions()\n",
    "        'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema'\n",
    "        ' {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}},'\n",
    "        ' \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema.'\n",
    "        ' The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n' + str(schema) + '\\n```'\n",
    "    )\n",
    "\n",
    "schema_hint = get_schema_hint(MetaCreator)\n",
    "# schema_hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9789591-993f-4884-a66c-a215499394bf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Then, if we just bind our LLM client to abide by the schema, then we should be able to generate it. \n",
    "The code below not only does that, but also shows how one might go about streaming the data or even filtering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e540b23a-dcb0-489a-907a-515b4cf60f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short_abstract: This workshop combines the latest advances in Generative AI (Gen AI) from NVIDIA's Earth-2 platform with expert techniques for creating high-quality static and dynamic 3D scenes using Neural Radiance Fields (NeRF) and 3D Gaussian Splats (3DGS). By the end of this course, you'll be able to integrate Earth-2's AI-driven weather and climate modeling capabilities with state-of-the-art 3D scene reconstruction methods.\n",
      "\n",
      "topics_covered: ['Earth-2 AI-driven weather and climate modeling', 'Neural Radiance Fields (NeRF) for static 3D scene reconstruction', '3D Gaussian Splats (3DGS) for dynamic 3D scene reconstruction', 'Integrating Earth-2 with NeRF/3DGS for high-quality 3D scene creation', 'Applying advanced methods for static and dynamic scene reconstruction in interactive 3D worlds']\n",
      "\n",
      "abstract_body: In this workshop, we'll delve into the latest advancements in Earth-2, a platform that combines AI-driven weather and climate modeling with high-performance computing. We'll explore how to use Earth-2 to create complex and accurate weather simulations, including storm patterns, ocean currents, and more. We'll also cover modern 3D scene reconstruction techniques using Neural Radiance Fields (NeRF) and 3D Gaussian Splats (3DGS). By the end of the course, you'll have a thorough understanding of how to integrate these techniques to create stunning and realistic 3D scenes. This workshop is perfect for developers, researchers, and engineers who want to create cutting-edge visual effects and interactive 3D experiences.\n",
      "\n",
      "long_abstract: In today's world of virtual reality and real-time graphics, creating high-quality 3D scenes is more important than ever. But, generating accurate and realistic scenes can be a daunting task. By leveraging the power of NVIDIA\\'s Earth-2 platform and combining it with advanced 3D scene reconstruction techniques like NeRF and 3DGS, we can push the boundaries of what's possible in interactive 3D worlds. This workshop will provide a comprehensive introduction to these cutting-edge techniques, allowing participants to integrate Earth-2's weather and climate modeling capabilities with NeRF/3DGS methods. By the end of this workshop, you'll be able to create complex and accurate weather simulations, apply these simulations to high-quality static and dynamic 3D scene reconstruction using Earth-2 and NeRF/3DGS, and visualize your results with interactive 3D worlds. This workshop focuses on equipping participants with practical knowledge to effectively explore 3D scene creation in climate modeling and real-world application of Generative AI for Environmental Visualization.\n",
      "\n",
      "objectives: [\"Integrate Earth-2's AI-driven weather and climate modeling with NeRF/3DGS methods\", 'Apply NeRF/3DGS techniques for high-quality static and dynamic 3D scene reconstruction', \"Utilize Earth-2's capabilities to create accurate and realistic weather simulations\", 'Visualize and interact with 3D scenes using Earth-2 and NeRF/3DGS']\n",
      "\n",
      "outline: ['Introduction to Earth-2 and its capabilities', 'Weather and Climate Modeling with Earth-2', 'Introduction to 3D scene reconstruction techniques (NeRF/3DGS)', 'Integrating Earth-2 with NeRF/3DGS for 3D scene creation', 'Practical Application of NeRF/3DGS on Earth-2', 'Scaling Earth-2+NeRF/3DGS visualization experiences', 'Conclusion and next steps']\n",
      "\n",
      "on_completion: Upon completing this workshop, participants will be able to generate accurate and realistic 3D scenes using Earth-2's AI-driven weather and climate modeling, NeRF, and 3DGS methods. They will be able to visualize these 3D scenes with interactive 3D worlds, simulating real-world applications in fields such as climate modeling, weather forecasting, and real-time graphics.\n",
      "\n",
      "prerequisites: ['Basic understanding of 3D graphics and computer graphics concepts', 'Familiarity with software development and programming environments (Python, C++)', 'Experience with Earth-2 platform and/or NeRF/3DGS methods is recommended but not required']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'object': 'list',\n",
       " 'data': [{'id': '01-ai/yi-large',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': '01-ai'},\n",
       "  {'id': 'abacusai/dracarys-llama-3.1-70b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'abacusai'},\n",
       "  {'id': 'ai21labs/jamba-1.5-large-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ai21labs'},\n",
       "  {'id': 'ai21labs/jamba-1.5-mini-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ai21labs'},\n",
       "  {'id': 'aisingapore/sea-lion-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'aisingapore'},\n",
       "  {'id': 'baichuan-inc/baichuan2-13b-chat',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'baichuan-inc'},\n",
       "  {'id': 'bigcode/starcoder2-15b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'bigcode'},\n",
       "  {'id': 'bigcode/starcoder2-7b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'bigcode'},\n",
       "  {'id': 'bytedance/seed-oss-36b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'bytedance'},\n",
       "  {'id': 'databricks/dbrx-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'databricks'},\n",
       "  {'id': 'deepseek-ai/deepseek-coder-6.7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-r1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-r1-0528',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-r1-distill-llama-8b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-r1-distill-qwen-14b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-r1-distill-qwen-32b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-r1-distill-qwen-7b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-v3.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'deepseek-ai/deepseek-v3.1-terminus',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'deepseek-ai'},\n",
       "  {'id': 'google/codegemma-1.1-7b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/codegemma-7b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-2-27b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-2-2b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-2-9b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-3-12b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-3-1b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-3-27b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-3-4b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-3n-e2b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-3n-e4b-it',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/gemma-7b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/recurrentgemma-2b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'google/shieldgemma-9b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'gotocompany/gemma-2-9b-cpt-sahabatai-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'gotocompany'},\n",
       "  {'id': 'ibm/granite-3.0-3b-a800m-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ibm'},\n",
       "  {'id': 'ibm/granite-3.0-8b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ibm'},\n",
       "  {'id': 'ibm/granite-3.3-8b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ibm'},\n",
       "  {'id': 'ibm/granite-34b-code-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ibm'},\n",
       "  {'id': 'ibm/granite-8b-code-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ibm'},\n",
       "  {'id': 'ibm/granite-guardian-3.0-8b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'ibm'},\n",
       "  {'id': 'igenius/colosseum_355b_instruct_16k',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'igenius'},\n",
       "  {'id': 'igenius/italia_10b_instruct_16k',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'igenius'},\n",
       "  {'id': 'institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'institute-of-science-tokyo'},\n",
       "  {'id': 'institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'institute-of-science-tokyo'},\n",
       "  {'id': 'marin/marin-8b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'marin'},\n",
       "  {'id': 'mediatek/breeze-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mediatek'},\n",
       "  {'id': 'meta/codellama-70b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.1-405b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.1-70b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.1-8b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.2-11b-vision-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.2-1b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.2-3b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.2-90b-vision-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-3.3-70b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-4-maverick-17b-128e-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-4-scout-17b-16e-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama-guard-4-12b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama2-70b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama3-70b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'meta/llama3-8b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'meta'},\n",
       "  {'id': 'microsoft/phi-3-medium-128k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3-medium-4k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3-mini-128k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3-mini-4k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3-small-128k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3-small-8k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3-vision-128k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3.5-mini-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3.5-moe-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-3.5-vision-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-4-mini-flash-reasoning',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-4-mini-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'microsoft/phi-4-multimodal-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'microsoft'},\n",
       "  {'id': 'mistralai/codestral-22b-instruct-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/magistral-small-2506',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mamba-codestral-7b-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mathstral-7b-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-7b-instruct-v0.2',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-7b-instruct-v0.3',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-large',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-large-2-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-medium-3-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-nemotron',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-small-24b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mistral-small-3.1-24b-instruct-2503',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mixtral-8x22b-instruct-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'mistralai'},\n",
       "  {'id': 'moonshotai/kimi-k2-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'moonshotai'},\n",
       "  {'id': 'moonshotai/kimi-k2-instruct-0905',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'moonshotai'},\n",
       "  {'id': 'nv-mistralai/mistral-nemo-12b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nv-mistralai'},\n",
       "  {'id': 'nvidia/embed-qa-4',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemoguard-8b-content-safety',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemoguard-8b-topic-control',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemotron-51b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemotron-70b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemotron-70b-reward',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemotron-nano-4b-v1.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemotron-nano-8b-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemotron-nano-vl-8b-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.1-nemotron-ultra-253b-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.2-nemoretriever-300m-embed-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.2-nemoretriever-300m-embed-v2',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.2-nv-embedqa-1b-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.2-nv-embedqa-1b-v2',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.3-nemotron-super-49b-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama-3.3-nemotron-super-49b-v1.5',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama3-chatqa-1.5-70b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/llama3-chatqa-1.5-8b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/mistral-nemo-minitron-8b-8k-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/mistral-nemo-minitron-8b-base',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nemoretriever-parse',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nemotron-4-340b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nemotron-4-mini-hindi-4b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nemotron-mini-4b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/neva-22b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nv-embed-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nv-embedcode-7b-v1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nv-embedqa-e5-v5',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nv-embedqa-mistral-7b-v2',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nvclip',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/nvidia-nemotron-nano-9b-v2',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/riva-translate-4b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/usdcode-llama-3.1-70b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'nvidia/vila',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'nvidia'},\n",
       "  {'id': 'openai/gpt-oss-120b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'openai/gpt-oss-20b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'openai'},\n",
       "  {'id': 'opengpt-x/teuken-7b-instruct-commercial-v0.4',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'opengpt-x'},\n",
       "  {'id': 'qwen/qwen2-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwen2.5-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwen2.5-coder-32b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwen2.5-coder-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwen3-235b-a22b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwen3-coder-480b-a35b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwen3-next-80b-a3b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwen3-next-80b-a3b-thinking',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'qwen/qwq-32b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'qwen'},\n",
       "  {'id': 'rakuten/rakutenai-7b-chat',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'rakuten'},\n",
       "  {'id': 'rakuten/rakutenai-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'rakuten'},\n",
       "  {'id': 'sarvamai/sarvam-m',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'sarvamai'},\n",
       "  {'id': 'snowflake/arctic-embed-l',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'snowflake'},\n",
       "  {'id': 'speakleash/bielik-11b-v2.3-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'speakleash'},\n",
       "  {'id': 'speakleash/bielik-11b-v2.6-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'speakleash'},\n",
       "  {'id': 'stockmark/stockmark-2-100b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'stockmark'},\n",
       "  {'id': 'thudm/chatglm3-6b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'thudm'},\n",
       "  {'id': 'tiiuae/falcon3-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'tiiuae'},\n",
       "  {'id': 'tokyotech-llm/llama-3-swallow-70b-instruct-v0.1',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'tokyotech-llm'},\n",
       "  {'id': 'upstage/solar-10.7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'upstage'},\n",
       "  {'id': 'utter-project/eurollm-9b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'utter-project'},\n",
       "  {'id': 'writer/palmyra-creative-122b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'writer'},\n",
       "  {'id': 'writer/palmyra-fin-70b-32k',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'writer'},\n",
       "  {'id': 'writer/palmyra-med-70b',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'writer'},\n",
       "  {'id': 'writer/palmyra-med-70b-32k',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'writer'},\n",
       "  {'id': 'zyphra/zamba2-7b-instruct',\n",
       "   'object': 'model',\n",
       "   'created': 735790403,\n",
       "   'owned_by': 'zyphra'},\n",
       "  {'id': 'nv-rerank-qa-mistral-4b:1'},\n",
       "  {'id': 'nvidia/llama-3.2-nv-rerankqa-1b-v2'},\n",
       "  {'id': 'nvidia/llama-3.2-nemoretriever-500m-rerank-v2'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(\n",
    "    schema=MetaCreator.model_json_schema(), \n",
    "    strict=True\n",
    ")\n",
    "\n",
    "meta_chain = prompt | structured_llm\n",
    "meta_gen_directive = (\n",
    "    # f\"Can you generate a course entry on the Earth-2 course? {schema_hint}\"\n",
    "    # f\"Can you combine the topics of the Earth-2 course and the NeRF/3DGS courses and generate a compelling course entry? {schema_hint}\"\n",
    "    f\"Can you combine the topics of the Earth-2 course and the NeRF/3DGS courses and generate a compelling course entry? Make sure to explain how they combine. {schema_hint}\"\n",
    ") \n",
    "meta_gen_state = {\n",
    "    \"messages\": [(\"user\", meta_gen_directive)],\n",
    "    \"context\": full_context,\n",
    "}\n",
    "\n",
    "# answer = meta_chain.invoke(meta_gen_state)\n",
    "# print(answer)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "answer = {}\n",
    "for chunk in meta_chain.stream(meta_gen_state):\n",
    "    clear_output(wait=True)\n",
    "    for key, value in chunk.items():\n",
    "        print(f\"{key}: {value}\", end=\"\\n\\n\", flush=True)\n",
    "        answer[key] = value\n",
    "\n",
    "llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e1ba0-375b-42e3-a987-ceb205097278",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Ok! That's not bad! It's reflective of the same limitations that we discussed in the lecture, but it does seem to be making good use of its context (while not degenerating into nonsense). Maybe we can ask it to improve upon it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e8c76-d2ad-4c08-a516-e16b336a6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: See if you can't prompt-engineer this solution to lead to an improved autoregression.\n",
    "meta_gen_state = {\n",
    "    \"messages\": [\n",
    "        (\"user\", meta_gen_directive),\n",
    "        (\"ai\", str(answer)),\n",
    "        (\"user\", \"Great! Can you please correct any mistakes and flesh out some vagueness?\")\n",
    "    ],\n",
    "    # \"context\": full_context,  ## Maybe we don't need the full context\n",
    "    \"context\": \"\",\n",
    "}\n",
    "\n",
    "answer2 = {}\n",
    "for chunk in meta_chain.stream(meta_gen_state):\n",
    "    clear_output(wait=True)\n",
    "    for key, value in chunk.items():\n",
    "        print(f\"{key}: {value}\", end=\"\\n\\n\", flush=True)\n",
    "        answer2[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db1b05-f731-4982-954f-5875e7d667ed",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Yeah... it can get better to a point.** \n",
    "- If we incorporate chat history, you'll start running into issues fast as the model starts to reach its context limit.\n",
    "- If we don't, we can still squeeze some customization from the LLM and can reasonably generate a better or longer outline... to a point.\n",
    "\n",
    "For this use-case, this model actually isn't that bad, but for something a bit longer, the limitations clearly start to show..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83f222-4721-4ed5-8ffe-cf3ec7f31da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pick your preferred option\n",
    "final_answer = answer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6945e34-1fda-47ed-a09d-692c9defa10c",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 2:** Generating A Notebook\n",
    "\n",
    "We've seen some fuzzy limitations when trying to generate our metadata, so let's see if we start to see more obvious problems when we get more ambitious. Below, we show an attempt at using the GPT-4o model to generate a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7baec8fb-6f11-484f-a442-ed5bd2f5a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short_abstract: Accelerate AI Weather Models and 3D Visualizations with NVIDIA Earth-2, NeRFs, and Gaussian Splatting\n",
      "\n",
      "topics_covered: This workshop combines the power of NVIDIA Earth-2, Neural Radiance Fields (NeRFs), and 3D Gaussian Splatting to accelerate AI weather modeling and 3D visualizations. You'll learn how to leverage these technologies to downscale models, generate super-resolution outputs, and speed up complex 3D rendering pipelines.\n",
      "\n",
      "abstract_body: AI-driven climate modeling and weather forecasting rely on complex simulations that can be computationally expensive. To address this challenge, NVIDIA Earth-2 provides a scalable platform for accelerating AI workloads. This workshop focuses on three key technologies: Neural Radiance Fields (NeRFs), a technique for rendering 3D scenes from scratch; 3D Gaussian Splatting, a method for efficiently rendering large datasets; and NVIDIA Earth-2, a platform for accelerating AI workloads. By combining these technologies, participants will learn how to downscale weather models, generate super-resolution outputs, and speed up complex 3D rendering pipelines, enabling more accurate and timely climate and weather simulations.\n",
      "\n",
      "long_abstract: Climate modeling and weather forecasting are increasingly dependant on AI-driven simulations. However, traditional computing methods can be slow and inaccurate. NVIDIA Earth-2 addresses this challenge by providing a platform for accelerated AI workloads. In this workshop, we'll delve into the intersection of NeRFs, 3D Gaussian Splatting, and NVIDIA Earth-2, exploring how to apply these technologies to accelerate AI weather modeling and 3D visualizations. You'll learn how to use NeRFs to render complex 3D scenes, leverage 3D Gaussian Splatting to efficiently render large datasets, and utilize NVIDIA Earth-2 to speed up your workloads. With a focus on practical applications and real-world case studies, this workshop will equip participants with the knowledge needed to accelerate AI weather modeling and 3D visualizations, leading to more accurate and timely climate and weather simulations. Upon completing this workshop, participants will be able to efficiently apply NeRFs and 3D Gaussian Splatting on NVIDIA Earth-2, enabling the creation of high-quality 3D visualizations and accelerated AI weather simulations.\n",
      "\n",
      "objectives: ['Apply Neural Radiance Fields (NeRFs) to render complex 3D scenes', 'Use 3D Gaussian Splatting to efficiently render large datasets', 'Accelerate AI weather modeling and 3D visualizations using NVIDIA Earth-2']\n",
      "\n",
      "outline: ['Introduction to NeRFs and 3D Gaussian Splatting for 3D rendering and visualization', 'Downscaling weather models with NeRFs and 3D Gaussian Splatting on NVIDIA Earth-2', 'Best practices for leveraging NeRFs, 3D Gaussian Splatting, and NVIDIA Earth-2 for accelerated AI weather modeling and 3D visualizations', 'Case studies and real-world applications of NeRFs, 3D Gaussian Splatting, and NVIDIA Earth-2 in climate and weather modeling']\n",
      "\n",
      "on_completion: By the end of this workshop, students will be able to apply NeRFs and 3D Gaussian Splatting to accelerate AI weather modeling and 3D visualizations on NVIDIA Earth-2, leading to more accurate and timely climate and weather simulations.\n",
      "\n",
      "prerequisites: ['Familiarity with AI and machine learning concepts', 'Basic understanding of weather and climate modeling principles', 'Knowledge of 3D visualization and rendering techniques']\n",
      "\n",
      "Please generate a starter jupyter notebook notebook for this course. Assume it's a DLI course, please add some amount of interactivity, and make sure to give your output as markdown with code inside ```python ...``` blocks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "with open(\"chats/make_me_a_notebook/input.txt\", \"r\") as f:\n",
    "    notebook_input_full = f.read()\n",
    "    notebook_input_prompt = notebook_input_full.split(\"\\n\\n\")[-1]\n",
    "print(notebook_input_full)\n",
    "# print(notebook_input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e21d42-cb10-4f9a-8c1b-228158f109ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### NVIDIA DLI Course: Accelerate AI Weather Models and 3D Visualizations\n",
       "\n",
       "# Introduction\n",
       "\n",
       "This notebook will guide you through the key concepts of NVIDIA Earth-2, Neural Radiance Fields (NeRFs), and 3D Gaussian Splatting. By the end, you'll be able to:\n",
       "- Render complex 3D scenes using NeRFs.\n",
       "- Efficiently render large datasets with 3D Gaussian Splatting.\n",
       "- Use NVIDIA Earth-2 to accelerate AI weather modeling.\n",
       "\n",
       "Let's get started!\n",
       "\n",
       "## 1. Setup Environment\n",
       "\n",
       "```python\n",
       "# Install necessary libraries (uncomment the lines if running locally)\n",
       "# !pip install torch torchvision torchaudio\n",
       "# !pip install numpy matplotlib opencv-python-headless\n",
       "\n",
       "import torch\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import cv2\n",
       "```\n",
       "\n",
       "## 2. Introduction to Neural Radiance Fields (NeRFs)\n",
       "\n",
       "NeRFs enable the synthesis of photorealistic 3D scenes from sparse 2D images by learning a volumetric scene representation.\n",
       "\n",
       "```python\n",
       "# Sample code to define a simple NeRF-like function\n",
       "\n",
       "def nerf_function(xyz):\n",
       "    \"\"\"Simple function simulating NeRF behavior.\"\"\"\n",
       "    return np.exp(-np.linalg.norm(xyz, axis=-1))\n",
       "\n",
       "# Generate a sample 3D coordinate grid\n",
       "xyz = np.random.rand(100, 3) * 2 - 1  # Values between -1 and 1\n",
       "nerf_values = nerf_function(xyz)\n",
       "\n",
       "# Visualizing the NeRF function output\n",
       "plt.scatter(xyz[:, 0], xyz[:, 1], c=nerf_values, cmap='viridis')\n",
       "plt.colorbar(label='Intensity')\n",
       "plt.xlabel('X-axis')\n",
       "plt.ylabel('Y-axis')\n",
       "plt.title('NeRF Function Output')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "## 3. Introduction to 3D Gaussian Splatting\n",
       "\n",
       "3D Gaussian Splatting allows efficient rendering of large point cloud datasets, commonly used for weather and environmental simulations.\n",
       "\n",
       "```python\n",
       "# Simulating 3D Gaussian Splatting\n",
       "\n",
       "def gaussian_splatting(points, sigma=0.1):\n",
       "    \"\"\"Apply Gaussian smoothing to a 3D point cloud.\"\"\"\n",
       "    return np.exp(-np.linalg.norm(points, axis=-1) / (2 * sigma**2))\n",
       "\n",
       "# Generating a synthetic 3D point cloud\n",
       "points = np.random.randn(500, 3) * 0.5\n",
       "splat_values = gaussian_splatting(points)\n",
       "\n",
       "# Visualizing the Gaussian splatting effect\n",
       "fig = plt.figure(figsize=(6, 6))\n",
       "ax = fig.add_subplot(111, projection='3d')\n",
       "ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=splat_values, cmap='coolwarm')\n",
       "ax.set_title('3D Gaussian Splatting')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "## 4. Using NVIDIA Earth-2 for AI Weather Modeling\n",
       "\n",
       "NVIDIA Earth-2 provides acceleration for AI weather models. Below is an interactive example where you can modify weather parameters.\n",
       "\n",
       "```python\n",
       "# Simulating a simplified AI weather model\n",
       "from ipywidgets import interact\n",
       "\n",
       "def simple_weather_model(temperature, humidity, wind_speed):\n",
       "    \"\"\"A basic model for simulating temperature impact on climate.\"\"\"\n",
       "    return temperature * 0.5 + humidity * 0.3 + wind_speed * 0.2\n",
       "\n",
       "interact(simple_weather_model, temperature=(0, 50, 5), humidity=(0, 100, 10), wind_speed=(0, 50, 5));\n",
       "```\n",
       "\n",
       "## 5. Real-World Applications\n",
       "\n",
       "Case studies will be covered in the workshop, but hereâ€™s an example of how these methods can be applied in climate simulations.\n",
       "\n",
       "```python\n",
       "# Combining NeRFs, Gaussian Splatting, and AI weather models\n",
       "\n",
       "def combined_model(xyz, temp, humidity, wind):\n",
       "    \"\"\"Simulates a weather model integrating NeRFs and Gaussian Splatting.\"\"\"\n",
       "    nerf_effect = nerf_function(xyz)\n",
       "    splatting_effect = gaussian_splatting(xyz)\n",
       "    weather_effect = simple_weather_model(temp, humidity, wind)\n",
       "    return nerf_effect * splatting_effect * weather_effect\n",
       "\n",
       "# Sample combined simulation\n",
       "xyz_sample = np.random.rand(200, 3) * 2 - 1\n",
       "weather_output = combined_model(xyz_sample, temp=30, humidity=60, wind=15)\n",
       "\n",
       "# Visualization\n",
       "plt.scatter(xyz_sample[:, 0], xyz_sample[:, 1], c=weather_output, cmap='plasma')\n",
       "plt.colorbar(label='Simulation Output')\n",
       "plt.xlabel('X-axis')\n",
       "plt.ylabel('Y-axis')\n",
       "plt.title('Combined AI Weather Model Output')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this notebook, we explored:\n",
       "- NeRFs for rendering 3D scenes.\n",
       "- 3D Gaussian Splatting for efficient large dataset visualization.\n",
       "- NVIDIA Earth-2â€™s role in AI weather modeling.\n",
       "\n",
       "Continue exploring these topics and apply them to real-world weather simulations!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<hr><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !cat chats/make_me_a_notebook/output.txt\n",
    "display(Markdown(\"chats/make_me_a_notebook/output.txt\"))\n",
    "display(Markdown(\"<hr><br><br>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa761a6-60b2-43b5-ad1a-673f8a8185c1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The notebook output in [`chats/make_me_a_notebook/output.txt`](./chats/make_me_a_notebook/output.txt) is the first-attempt output that came out of GPT-4o when I asked it to generate a notebook per [`chats/make_me_a_notebook/input.txt`](./chats/make_me_a_notebook/input.txt). It's serviceable enough with such a vague input, and can be improved **to some point** by just asking it for better output, criticizing it, and giving it enough information to work with. \n",
    "\n",
    "The common anecdote \"garbage in, garbage out\" comes to mind, since the LLM is just mirroring the style of reasonable output given your input specific input. But due to the conversational nature of the training (not helped by the chat prompts into which the messages are being funneled), your output will usually be uncomfortably short and just imprecise enough for many advanced use cases.\n",
    "\n",
    "Still, let's see if we can improve on this output by giving our LLM a style reference and asking it to rephrase the notebook a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dca0c15a-9cde-46bb-9f82-88c919f4f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: construct a good notebook in markdown format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: # Comprehensive Workshop on Generative AI and 3D Scene Reconstruction\n",
      "\n",
      "## Introduction\n",
      "\n",
      "This workshop focuses on utilizing the NVIDIA Earth-2 platform and combining it with expert techniques in creating high-quality static and dynamic 3D scenes using Neural Radiance Fields (NeRF) and 3D Gaussian Splats (3DGS).\n",
      "\n",
      "## Topics Covered\n",
      "\n",
      "* Earth-2's AI-driven weather and climate modeling capabilities\n",
      "* State-of-the-art 3D scene reconstruction using Neural Radiance Fields (NeRF)\n",
      "* Expert techniques for 3D Gaussian Splats (3DGS) and dynamic 3D scene reconstruction\n",
      "* Integrating Earth-2 with NeRF/3DGS for photorealistic 3D scene creation\n",
      "* Real-time rendering and visualization of dynamic weather simulations and 3D scenes\n",
      "\n",
      "## Objectives\n",
      "\n",
      "* Integrate Earth-2's AI-driven weather and climate modeling with NeRF/3DGS methods\n",
      "* Apply NeRF/3DGS techniques to produce high-quality static and dynamic 3D scenes\n",
      "* Utilize Earth-2 to create accurate and realistic weather simulations\n",
      "* Produce photorealistic 3D visualizations of dynamic weather phenomena and scenes\n",
      "* Grasp full-stack development, commencing from Earth-2-based training to Finals in real-time rendering engines (optimized for web and standalone experiences)\n",
      "\n",
      "## Hands-on Project 1: Weather and Climate Modeling with Earth-2\n",
      "\n",
      "* Create realistic weather simulations using Earth-2\n",
      "* Learn Earth-2's capabilities for creating photorealistic weather simulations\n",
      "\n",
      "## Hands-on Project 2: Combining Earth-2 with NeRF/3DGS for Photorealistic 3D Scene Creation\n",
      "\n",
      "* Integrate Earth-2 with NeRF/3DGS for high-quality 3D scene creation\n",
      "* Learn to apply advanced techniques for 3D scene reconstruction\n",
      "\n",
      "## Advanced Techniques: Real-time Rendering and Visualization of Dynamic Weather Simulations and 3D Scenes\n",
      "\n",
      "* Learn to efficiently render dynamic weather phenomena and 3D scenes\n",
      "* Understand how to achieve photorealistic 3D visualizations\n",
      "\n",
      "## Advanced Hands-on Project: Application of Combined Earth-2 & NeRF/3DGS Techniques\n",
      "\n",
      "* Develop a project that integrates Earth-2 and NeRF/3DGS\n",
      "* Learn how to create immersive and interactive 3D experiences\n",
      "\n",
      "## Best Practices for Production-ready Code and Next Steps in Real-world Applications\n",
      "\n",
      "* Learn ways to optimize and deploy code in real-time scenarios\n",
      "* Understand how to apply gained skills to real-world applications\n",
      "\n",
      "## Requirements\n",
      "\n",
      "* Prior experience with software development and C++ coding\n",
      "* Familiarity with Python environment\n",
      "* Hands-on experience with computer graphics principles\n",
      "* Existing knowledge about 3D modeling basics and Physics for High detail engines\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Upon completing this comprehensive workshop, participants will have acquired hands-on skills to integrate Earth-2 AI-driven weather and climate modeling capabilities with state-of-the-art 3D scene reconstruction (NeRF/3DGS) to present users with system insights for the reasoned inspection of emerging environmental risks in urban-to-natural scenarios utilizing hone in experience, accurate dynamic way rendering via 'Photorealistic 3D Visualizations' appropriate in Climate Science, Disaster Response, and Real-time Graphics."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: flesh out each section within an end-to-end narrative including code chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: # Comprehensive Workshop on Generative AI and 3D Scene Reconstruction\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Welcome to our comprehensive workshop on generative AI and 3D scene reconstruction! In this workshop, we will be focusing on utilizing the NVIDIA Earth-2 platform and combining it with expert techniques in creating high-quality static and dynamic 3D scenes using Neural Radiance Fields (NeRF) and 3D Gaussian Splats (3DGS). By the end of this workshop, you will have a hands-on understanding of how to integrate Earth-2's AI-driven weather and climate modeling capabilities with state-of-the-art 3D scene reconstruction methods.\n",
      "\n",
      "## Topics Covered\n",
      "\n",
      "### Earth-2's AI-driven Weather and Climate Modeling Capabilities\n",
      "\n",
      "In this section, we will be covering the basics of Earth-2 and its capabilities for creating photorealistic weather simulations. We will explore the different components of Earth-2, including its AI-driven weather and climate modeling capabilities, and learn how to create realistic weather simulations using Earth-2.\n",
      "\n",
      "```python\n",
      "import earth2 as e2\n",
      "\n",
      "# Create a new Earth-2 object\n",
      "earth = e2.Earth2()\n",
      "\n",
      "# Set the location and time of the simulation\n",
      "location = [40.7128, -74.0060]  # coordinates for New York City\n",
      "time = 12  # 12:00 PM\n",
      "\n",
      "# Create a weather simulation using Earth-2\n",
      "weather_sim = earth.simulate_weather(location, time)\n",
      "\n",
      "# Print the resulting weather simulation\n",
      "print(weather_sim)\n",
      "```\n",
      "\n",
      "### State-of-the-art 3D Scene Reconstruction using Neural Radiance Fields (NeRF)\n",
      "\n",
      "In this section, we will be covering the basics of NeRF and its applications in 3D scene reconstruction. We will learn how to create high-quality 3D scenes using NeRF and integrate them with Earth-2's weather simulations.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Get the input image and the corresponding depth map\n",
      "img = torch.randn((256, 256, 3))\n",
      "depth = torch.randn((256, 256))\n",
      "\n",
      "# Create a NeRF model\n",
      "nerf_model = NeRF()\n",
      "\n",
      "# Train the NeRF model\n",
      "loss = nerf_model.train(img, depth)\n",
      "\n",
      "# Print the resulting 3D scene\n",
      "print(nerf_model.render_image())\n",
      "```\n",
      "\n",
      "### Expert Techniques for 3D Gaussian Splats (3DGS)\n",
      "\n",
      "In this section, we will be covering the basics of 3D Gaussian Splats (3DGS) and its applications in 3D scene reconstruction. We will learn how to create high-quality 3D scenes using 3DGS and integrate them with Earth-2's weather simulations.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Get the input image and the corresponding depth map\n",
      "img = torch.randn((256, 256, 3))\n",
      "depth = torch.randn((256, 256))\n",
      "\n",
      "# Create a 3DGS model\n",
      "3dgs_model = ThreeDGaussianSplats()\n",
      "\n",
      "# Train the 3DGS model\n",
      "loss = 3dgs_model.train(img, depth)\n",
      "\n",
      "# Print the resulting 3D scene\n",
      "print(3dgs_model.render_image())\n",
      "```\n",
      "\n",
      "## Hands-on Project 1: Weather and Climate Modeling with Earth-2\n",
      "\n",
      "In this section, we will be creating a hands-on project that integrates Earth-2's AI-driven weather and climate modeling capabilities with state-of-the-art 3D scene reconstruction methods. We will create a photorealistic 3D scene of a weather simulation using Earth-2 and NeRF.\n",
      "\n",
      "```python\n",
      "import earth2 as e2\n",
      "import nerf_model\n",
      "\n",
      "# Create a new Earth-2 object\n",
      "earth = e2.Earth2()\n",
      "\n",
      "# Set the location and time of the simulation\n",
      "location = [40.7128, -74.0060]  # coordinates for New York City\n",
      "time = 12  # 12:00 PM\n",
      "\n",
      "# Create a weather simulation using Earth-2\n",
      "weather_sim = earth.simulate_weather(location, time)\n",
      "\n",
      "# Create a NeRF model\n",
      "nerf_model = NeRF()\n",
      "\n",
      "# Train the NeRF model\n",
      "loss = nerf_model.train(weather_sim)\n",
      "\n",
      "# Print the resulting photorealistic 3D scene\n",
      "print(nerf_model.render_image())\n",
      "```\n",
      "\n",
      "## Hands-on Project 2: Combining Earth-2 with NeRF/3DGS for Photorealistic 3D Scene Creation\n",
      "\n",
      "In this section, we will be creating a hands-on project that integrates Earth-2's AI-driven weather and climate modeling capabilities with state-of-the-art 3D scene reconstruction methods using 3DGS. We will create a photorealistic 3D scene of a weather simulation using Earth-2 and 3DGS.\n",
      "\n",
      "```python\n",
      "import earth2 as e2\n",
      "import three_d_gaussian_splats\n",
      "\n",
      "# Create a new Earth-2 object\n",
      "earth = e2.Earth2()\n",
      "\n",
      "# Set the location and time of the simulation\n",
      "location = ["
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: I'll stop providing the code chunks and narrative for the workshop materials. If you have any further requests or questions, feel free to ask."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def notebook_to_markdown(path: str) -> str:\n",
    "    \"\"\"Load a Jupyter notebook from a given path and convert it to Markdown format.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        notebook = json.load(file)\n",
    "    markdown_content = []\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'code':          # Combine code into one block\n",
    "            markdown_content += [f'```python\\n{\"\".join(cell[\"source\"])}\\n```']\n",
    "        elif cell['cell_type'] == 'markdown':    # Directly append markdown source\n",
    "            markdown_content += [\"\".join(cell[\"source\"])]\n",
    "        # for output in cell.get('outputs', []):   # Optionally, you can include cell outputs\n",
    "        #     if output['output_type'] == 'stream':\n",
    "        #         markdown_content.append(f'```\\n{\"\".join(output[\"text\"])}\\n```')\n",
    "    return '\\n\\n'.join(markdown_content)\n",
    "\n",
    "notebook_example = notebook_to_markdown(\"extra_utils/general_representative_notebook.ipynb\")\n",
    "\n",
    "context = str(final_answer)\n",
    "# context = (\n",
    "#     f\"THE FOLLOWING IS AN EXAMPLE NOTEBOOK FOR STYLE ONLY: \\n\\n{notebook_example}\"\n",
    "#     \"\\n\\n=========\\n\\n\"\n",
    "#     f\"THE FOLLOWING IS THE TOPIC COURSE THAT WE ARE DISCUSSING:\\n\\n{final_answer}\\n\\n\"\n",
    "# )\n",
    "\n",
    "long_context_state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": context,\n",
    "}\n",
    "\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "chat(long_context_state)\n",
    "\n",
    "## EXAMPLE INPUTS ##\n",
    "## Option: Can you please construct a good notebook in markdown format?\n",
    "## Option: That's great, but there is no code. Can you please flesh out each section within an end-to-end narrative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20a90b-d1ae-4bf8-aebc-ab19955344f1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In our case, our model is quite small and we're also limiting our endpoint to a short input and short output (for its own good), so the amount of content it can generate really is quite limited. This limitation does, however, manifest in all realistic scenarios regardless of the model quality. For any modern LLM:\n",
    "- Though straight decoding of the solution can work for some contexts, they cannot scale up to arbitrarily-large inputs or outputs. \n",
    "- The quality output length is generally shorter than the quality input length when we get to longer sequences. This is enforced during training and enforces good properties for efficient cost of generation and reduction in context accumulation.\n",
    "\n",
    "In other words, **the space of things that can be given to or expected of an LLM $>>$ the space of things that an LLM can actually understand well $>>$ the space of things that the LLM can actually output well.** *($>>$ = \"far greater than\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d2f4b-84bc-44c5-8ac6-cc9fc2fc59e5",
   "metadata": {},
   "source": [
    "Given this insight, we can understand that trying to force the LLM to produce a notebook all at once might lead to incoherence at the global scale. However, it seems to be starting off at least somewhat ok, so maybe there's some merit in the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53cca4-01c2-4156-a795-dbd548c9f4e0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 3:** Using an Agent Canvas\n",
    "\n",
    "When we observe that we can't directly output the thing that we want, the next question is \"can we take in what we want.\" \n",
    "- It seemed like the LLM was able to roughly follow along with the premise when we only gave it the premise as input, but started derailing when we gave it a representative example. \n",
    "- Furthermore, it was likely able to actually improve upon your notebook through conversation, so maybe we can start there.\n",
    "\n",
    "**Canvasing Approach:** Instead of getting the model to predict the full document, get it to treat the document as  an environment and propose one of the following to the LLM:\n",
    "> - ***\"Please propose a modification that will improve the state of the document. Here are your options. Pick one/several and they will be done.\"***\n",
    "> - ***\"Here is the whole state, and you are tasked with improving JUST THIS SUBSECTION OF IT. Please output your update to that section. No other sections will be modified.\"***\n",
    "> - ***\"This is the whole document. This section is bad because of one or more of the following: {criticisms}. Replace it with an improved version.\"***\n",
    "\n",
    "If the model is capable of understanding both the full environment and the instruction, then it can directly autoregress only a small section or even a strategic modification of the output. Combine this approach with structured output or chain of thought, and you're likely to get a formulation that, while not perfect, helps to approach the potential output length towards the potential input length of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93a1c62-83c5-45e7-8422-f3a6d58d5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Insert a notebook of choice\n",
    "STARTING_NOTEBOOK = \"\"\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a3dde-3fbb-43f5-9060-244c05340dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**3. Topics Covered**\n",
      "\n",
      "In this section, we will provide a detailed overview of the topics covered in the workshop, including code examples to help solidify your understanding.\n",
      "\n",
      "**3.1 Earth-2's AI-driven Weather and Climate Modeling Capabilities**\n",
      "\n",
      "Earth-2 is a powerful platform developed by NVIDIA that enables the simulation of realistic weather and climate scenarios. We will explore the capabilities of Earth-2, including its AI-driven weather and climate modeling capabilities, and learn how to create accurate and realistic weather simulations using Earth-2.\n",
      "\n",
      "```python\n",
      "import earth2 as e2\n",
      "\n",
      "# Create a new Earth-2 object\n",
      "earth = e2.Earth2()\n",
      "\n",
      "# Set the location and time of the simulation\n",
      "location = [40.7128, -74.0060]  # coordinates for New York City\n",
      "time = 12  # 12:00 PM\n",
      "\n",
      "# Create a weather simulation using Earth-2\n",
      "weather_sim = earth.simulate_weather(location, time)\n",
      "\n",
      "# Print the resulting weather simulation\n",
      "print(weather_sim)\n",
      "```\n",
      "\n",
      "**3.2 State-of-the-art 3D Scene Reconstruction using Neural Radiance Fields (NeRF)**\n",
      "\n",
      "NeRF is a state-of-the-art method for 3D scene reconstruction that uses neural networks to represent the radiance field of a scene. We will learn how to create high-quality 3D scenes using NeRF and integrate them with Earth-2's weather simulations.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Get the input image and the corresponding depth map\n",
      "img = torch.randn((256, 256, 3))\n",
      "depth = torch.randn((256, 256))\n",
      "\n",
      "# Create a NeRF model\n",
      "nerf_model = NeRF()\n",
      "\n",
      "# Train the NeRF model\n",
      "loss = nerf_model.train(img, depth)\n",
      "\n",
      "# Print the resulting 3D scene\n",
      "print(nerf_model.render_image())\n",
      "```\n",
      "\n",
      "**3.3 Expert Techniques for 3D Gaussian Splats (3DGS)**\n",
      "\n",
      "3DGS is a technique for 3D scene reconstruction that uses Gaussian splats to represent the radiance field of a scene. We will learn how to create high-quality 3D scenes using 3DGS and integrate them with Earth-2's weather simulations.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Get the input image and the corresponding depth map\n",
      "img = torch.randn((256, 256, 3))\n",
      "depth = torch.randn((256, 256))\n",
      "\n",
      "# Create a 3DGS model\n",
      "3dgs_model = ThreeDGaussianSplats()\n",
      "\n",
      "# Train the 3DGS model\n",
      "loss = 3dgs_model.train(img, depth)\n",
      "\n",
      "# Print the resulting 3D scene\n",
      "print(3dgs_model.render_image())\n",
      "```\n",
      "\n",
      "**3.4 Integrating Earth-2 with NeRF/3DGS for Photorealistic 3D Scene Creation**\n",
      "\n",
      "In this section, we will learn how to integrate Earth-2's AI-driven weather and climate modeling capabilities with state-of-the-art 3D scene reconstruction methods using NeRF and 3DGS. We will create high-quality 3D scenes that are photorealistic and realistic.\n",
      "\n",
      "```python\n",
      "import earth2 as e2\n",
      "import nerf_model\n",
      "import three_d_gaussian_splats\n",
      "\n",
      "# Create a new Earth-2 object\n",
      "earth = e2.Earth2()\n",
      "\n",
      "# Set the location and time of the simulation\n",
      "location = [40.7128, -74.0060]  # coordinates for New York City\n",
      "time = 12  # 12:00 PM\n",
      "\n",
      "# Create a weather simulation using Earth-2\n",
      "weather_sim = earth.simulate_weather(location, time)\n",
      "\n",
      "# Create a NeRF model\n",
      "nerf_model = NeRF()\n",
      "\n",
      "# Train the NeRF model\n",
      "loss = nerf_model.train(weather_sim)\n",
      "\n",
      "# Print the resulting photorealistic 3D scene\n",
      "print(nerf_model.render_image())\n",
      "\n",
      "# Create a 3DGS model\n",
      "3dgs_model = ThreeDGaussianSplats()\n",
      "\n",
      "# Train the 3DGS model\n",
      "loss = 3dgs_model.train(weather_sim)\n",
      "\n",
      "# Print the resulting photorealistic 3D scene\n",
      "print(3dgs_model.render_image())\n",
      "```\n",
      "\n",
      "################################################################\n",
      "\n",
      "\n",
      "**3.1 Earth-2's AI-driven Weather and Climate Modeling Capabilities**\n",
      "\n",
      "### Overview of Earth-2 Capabilities\n",
      "\n",
      "In this section, we will delve into the powerful capabilities of Earth-2, a platform developed by NVIDIA that enables the simulation of realistic weather and climate scenarios. Earth-2's AI-driven weather and climate modeling capabilities allow for accurate and realistic weather simulations, making it an essential tool for various applications.\n",
      "\n",
      "### Creating a Weather Simulation with Earth-2\n",
      "\n",
      "We will demonstrate how to create a weather simulation using Earth-2 by following these steps:\n",
      "\n",
      "```python\n",
      "import earth2 as e2\n",
      "\n",
      "# Create a new Earth-2 object\n",
      "earth = e2.Earth2()\n",
      "\n",
      "# Set the location and time of the simulation\n",
      "location = [40.7128, -74.0060]  # coordinates for New York City\n",
      "time = 12  # 12:00 PM\n",
      "\n",
      "# Create a weather simulation using Earth-2\n",
      "weather_sim = earth.simulate_weather(location, time)\n",
      "\n",
      "# Print the resulting weather simulation\n",
      "print(weather_sim)\n",
      "```\n",
      "\n",
      "### Tips and Tricks\n",
      "\n",
      "* Make sure to set the location and time of the simulation accurately to ensure realistic weather conditions.\n",
      "* You can modify the weather simulation by adjusting the input parameters, such as modifying the weather patterns or temperature.\n",
      "* This code snippet provides a basic example of creating a weather simulation. You can further customize and extend this code to suit your specific needs.\n",
      "\n",
      "### Exercise\n",
      "\n",
      "* Experiment with different locations and times to see how the weather simulation changes.\n",
      "* Try modifying the weather simulation by adjusting the input parameters.\n",
      "* Use this code as a starting point and extend it to create more complex weather simulations.\n",
      "\n",
      "################################################################\n",
      "\n",
      "\n",
      "**3.2 State-of-the-art 3D Scene Reconstruction using Neural Radiance Fields (NeRF)**\n",
      "\n",
      "**Overview of NeRF**\n",
      "\n",
      "Neural Radiance Fields (NeRF) is a state-of-the-art technique for 3D scene reconstruction that uses neural networks to represent the radiance field of a scene. In this section, we will explore the basics of NeRF and its applications in 3D scene reconstruction. We will learn how to create high-quality 3D scenes using NeRF and integrate them with Earth-2's weather simulations.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Load a sample image and depth map\n",
      "img = torch.load('sample_image.pt')\n",
      "depth = torch.load('sample_depth_map.pt')\n",
      "\n",
      "# Create a NeRF model\n",
      "nerf_model = NeRF()\n",
      "\n",
      "# Train the NeRF model using the loaded image and depth map\n",
      "loss = nerf_model.train(img, depth)\n",
      "\n",
      "# Print the resulting 3D scene\n",
      "print(nerf_model.render_image())\n",
      "```\n",
      "\n",
      "**Code Insights**\n",
      "\n",
      "* We load a sample image and depth map to demonstrate the NeRF model training process.\n",
      "* We create a NeRF model using the `NeRF()` constructor and train it using the loaded image and depth map.\n",
      "* We showcase the trained NeRF model by rendering an image of the 3D scene using the `render_image()` method.\n",
      "\n",
      "**Tips and Tricks**\n",
      "\n",
      "* Make sure to load a suitable image and depth map for your training dataset.\n",
      "* Experiment with different hyperparameters to optimize the NeRF model's performance.\n",
      "* You can integrate the trained NeRF model with Earth-2's weather simulations to create photorealistic 3D scenes.\n",
      "\n",
      "################################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"user\", (\n",
    "        \"The following section needs some improvements:\\n\\n<<<SECTION>>>\\n\\n{section}\\n\\n<<</SECTION>>>\\n\\n\"\n",
    "        \"Please propose an upgrade that would improve the overall notebook quality.\"\n",
    "        \" Later sections will follow and will be adapted by other efforts.\"\n",
    "        \" You may only output modifications to the section provided here, no later or earlier sections.\"\n",
    "        \" Follow best style practices, and assume the sections before this one are more enforcing that the latter ones.\"\n",
    "        \" Make sure to number your section, continuing from the previous ones.\"\n",
    "    )),\n",
    "])\n",
    "\n",
    "## An LCEL chain to pass into chat_with_generator\n",
    "sub_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "delimiter = \"###\"  ## TODO: Pick a delimiter that works for your notebook\n",
    "WORKING_NOTEBOOK = STARTING_NOTEBOOK.split(delimiter)\n",
    "output = \"\"\n",
    "for i in range(len(WORKING_NOTEBOOK)):\n",
    "    chunk = WORKING_NOTEBOOK[i]\n",
    "    ## TODO: Knowing that the state needs \"context\" and \"section\" values,\n",
    "    ## can you construct your input state?\n",
    "    chunk_refinement_state = {\n",
    "    \"context\": \"####\".join(WORKING_NOTEBOOK),\n",
    "    \"section\": chunk,\n",
    "}\n",
    "    for token in sub_chain.stream(chunk_refinement_state):\n",
    "        print(token, end=\"\", flush=True)\n",
    "        output += token\n",
    "    WORKING_NOTEBOOK[i] = output\n",
    "    print(\"\\n\\n\" + \"#\" * 64 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337613d-67c9-4b9f-95b5-15e526102846",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details><summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "chunk_refinement_state = {\n",
    "    \"context\": \"####\".join(WORKING_NOTEBOOK),\n",
    "    \"section\": chunk,\n",
    "}\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "<hr><br>\n",
    "\n",
    "### **Part 4:** Reflecting On This Exercise\n",
    "\n",
    "As we can see, this approach is quite promising in that it's able to extend the output of the model towards a large context with only local modifications. This 8B model was pretty quickly pushed out of its training distribution with this approach, and it also likely started to go pretty aggressively into hallucination mode due to its vague inputs, but a larger model would be able to iterate on this process for much longer and could even have some error-correcting or randomization efforts thrown in to stabilize the process. \n",
    "\n",
    "This technique is also used in the wild to implement features like codebase modification and collaborative document editing (i.e. OpenAI Canvas). Additionally, even minor modifications to this approach can help you implement some surprisingly-effective and efficient solutions:\n",
    "- **Find-Replace Canvas:** Instead of autoregressing the sections of a document, you can generate find-replace pairs. Executing this process on the chunks, you will wind up with a much safer formulation as well as an easier-to-track footprint. This kind of system can be used to implement AI-enabled spell-checkers and other forms or strategic error correction.\n",
    "- **Document Translation:** More generally, this approach can also be used to translate a document, one section at a time, from one format to another. A similar approach to the one above can be used to translate a document from one language to another, with a bit of context injection thrown in to help give the translating model pipeline some style to guide it.\n",
    "\n",
    "Note that while we call this process *\"canvasing,\"* you may also run into the same or similar idea under the term *\"iterative refinement.\"* They are pretty much one-and-the-same, except the latter is much more general and could technically be applied to any LLM-enabled loop that progresses the input into the output over many iterations. Canvasing implies more strongly that you're using the current environment as a playground and can make strategic modifications to improve the state.\n",
    "\n",
    "----\n",
    "\n",
    "In any case, we've now tested out how our little model can actually help us do some surprisingly-interesting things, while also reflecting on the fact that it has clear limitations. This marks the end of our \"simple pipeline\" exercises for this course. In the next section, we will be using the primitives we've picked up to start working with a proper agents framework while sticking to our very-limited but surprisingly-flexible Llama-8B model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9454b-ce16-4a8e-bc4c-ecc406e4c7b3",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
