# (1) Use this file for docker runtime configurations that are common to both
# development and deployment.

# `version : '2.3'` lets us use the `runtime=nvidia` configuration so that our
# containers can interact with the GPU(s).
version: '2.3'

volumes: 
  assessment_results:
  pgdata:        # PostgreSQL data
  cache_data:    # SQLite fallback
  logs:          # Application logs

# `services` defines the containers you want to run. There will always be at least 2:
# a `lab` container for your content, and `nginx` as a reverse proxy.
#
# Commonly, a third container for data loading is used here, and sometimes, even more
# containers for other services like assessments, remote desktop, tensorboard, etc.
services:

# (2) To prevent name collisions, be sure to set the course id as project name in `.env`

  ## Deliver jupyter labs interface for students.
  lab:
    container_name: jupyter-notebook-server  
    init: true
    volumes:
      - ./notebooks/:/dli/task/
      - ./docker-compose.yml:/dli/task/composer/docker-compose.yml
      - ./docker-compose.override.yml:/dli/task/composer/docker-compose.override.yml
      - ./nginx.conf:/dli/task/composer/nginx.conf
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - JUPYTER_TOKEN

  # ## Deliver a server to interface with host docker orchestration
  # docker_router:
  #   container_name: docker_router
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock  ## Give access to host docker
  #     - ./assessment:/app/assessment
  #     - ./notebooks:/app/notebooks
  #   ports:
  #     - "8070:8070"

  # This container is required to reverse proxy content. It expects a config
  # file at `/etc/nginx/nginx.conf` which we provide in this repo and mount.
  #
  # All services should be proxied through nginx, and all should be accessible
  # on port 80, for compatibility with restricted corporate networks and to
  # be compatible with upcoming course requirements.
  nginx:
    image: nginx:1.15.12-alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - lab

  ########################################################################################################
  ## NIMs

  # nim:
  #   user: root # you don't have to be the root, but need to specify some user
  #   container_name: nim-llm
  #   # image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.3.3
  #   image: 224292665285.dkr.ecr.us-east-1.amazonaws.com/nim_llm:llama-3.1-8b-instruct-1.3.3
  #   runtime: nvidia
  #   shm_size: 16gb
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=0
  #     # Necessary since we are running as root on potentially-multiple GPUs
  #     - OMPI_ALLOW_RUN_AS_ROOT=1
  #     - OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
  #     - end_id
  #   volumes:
  #     - ./nim-cache/nim:/opt/nim/.cache

  # embedding:
  #   container_name: nim-embedding
  #   # image: nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.1.0
  #   image: 224292665285.dkr.ecr.us-east-1.amazonaws.com/nim_llm:nv-embedqa-e5-v5-1.1.0
  #   user: root
  #   shm_size: 16gb
  #   ports:
  #     - 8000:8000
  #   volumes: 
  #     - ./nim-cache/embedding:/opt/nim/.cache
  #   environment:
  #     - CUDA_VISIBLE_DEVICES=0
  #     - NIM_HTTP_API_PORT=8000
  #     - NIM_TRITON_LOG_VERBOSE=1
  #   runtime: nvidia

  # ranking:
  #   container_name: nim-ranking
  #   # image: "nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:1.0.2"
  #   image: 224292665285.dkr.ecr.us-east-1.amazonaws.com/nim_llm:nv-rerankqa-mistral-4b-v3-1.0.2
  #   user: root
  #   shm_size: 16gb
  #   ports:
  #     - 7007:8000
  #   volumes: 
  #     - ./nim-cache/ranking:/opt/nim/.cache
  #   environment:
  #     - NIM_HTTP_API_PORT=8000
  #     - NIM_TRITON_LOG_VERBOSE=1
  #     - CUDA_VISIBLE_DEVICES=1
  #   runtime: nvidia

  lg_viz:
    ports:
      - '3002:3002'

  ## Assessment microservice. Checks for assignment completion
  assessment:
    image: python:3.10-slim
    volumes:
      - assessment_results:/dli/assessment_results/
      - ./assessment:/dli/assessment
    entrypoint: ["/bin/sh", "-c"]
    command: [". /dli/assessment/entrypoint.assessment.sh"]

  # --- Docker Router ---
  docker_router:
    container_name: docker_router
    ports:
      - "8070:8070"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  ## Give access to host docker
      - ./assessment:/app/assessment
      - ./notebooks:/app/notebooks
    environment:
      - LOG_LEVEL=INFO
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8070/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # --- LLM Client ---
  llm_client:
    container_name: llm_client
    ports:
      - "9000:9000"
    volumes:
      - logs:/logs
    environment:
      # - NVIDIA_API_KEY=${NVIDIA_API_KEY:-}
      # - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - LOG_LEVEL=INFO
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
    depends_on:
      - jaeger
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # --- Observability: Distributed Tracing ---
  jaeger:
    image: jaegertracing/all-in-one:1.57
    container_name: jaeger
    ports:
      - "16686:16686"  # UI
      - "4317:4317"    # OTLP gRPC
      - "4318:4318"    # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - LOG_LEVEL=info
    restart: unless-stopped

  # --- Observability: Logs ---
  dozzle:
    image: amir20/dozzle:latest
    container_name: dozzle
    ports:
      - "9999:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped

  # --- Database Admin UI ---
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d ddg_cache || pg_isready -U user -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      bash -c "
      docker-entrypoint.sh postgres &
      until pg_isready -U user -d postgres; do sleep 3; done;
      psql -U user -d postgres -c 'CREATE DATABASE ddg_cache;' || true;
      wait"

  dbgate:
    image: dbgate/dbgate:latest
    container_name: dbgate
    restart: always
    ports:
      - "3000:3000"
    environment:
      CONNECTIONS: con1
      LABEL_con1: DDG Cache
      SERVER_con1: postgres
      USER_con1: user
      PASSWORD_con1: pass
      PORT_con1: 5432
      DATABASE_con1: ddg_cache
      ENGINE_con1: postgres@dbgate-plugin-postgres

  ddg-cache:
    container_name: ddg-cache
    ports:
      - "7860:7860"  # Gradio interface
      - "7861:7861"  # FastAPI (if running both)
    environment:
      - DATABASE_URL=postgresql+asyncpg://user:pass@postgres:5432/ddg_cache
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
      - LLM_BASE_URL=http://llm_client:9000/v1
      - DEBUG=0
    volumes:
      # Mount source for development/hot-reload
      - ./notebooks/composer/microservices:/app
    depends_on:
      postgres:
        condition: service_healthy
      jaeger:
        condition: service_started
      llm_client:
        condition: service_started    
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

networks:
  default:
    name: nvidia-agents
