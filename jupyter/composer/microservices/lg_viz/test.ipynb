{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc72eb59-cf87-4c55-a4ba-40739e8f5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def serialize_for_js(data):\n",
    "    \"\"\"\n",
    "    Converts a Python dictionary to a clean JSON string that JavaScript can safely parse.\n",
    "    \"\"\"\n",
    "    def custom_serializer(obj):\n",
    "        \"\"\" Handle non-serializable objects like sets, tuples, and custom classes. \"\"\"\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)  # Convert sets to lists\n",
    "        if isinstance(obj, tuple):\n",
    "            return [custom_serializer(item) for item in obj]  # Convert tuples recursively\n",
    "        if hasattr(obj, '__dict__'):  # Handle custom objects\n",
    "            return obj.__dict__\n",
    "        return str(obj)  # Fallback: Convert unknown objects to string\n",
    "\n",
    "    return json.dumps(data, default=custom_serializer, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Helper function to send update to the web service\n",
    "def send_update(mode, data):\n",
    "    try:\n",
    "        requests.post(\"http://lg_viz:3002/api/update\", json={\"mode\": mode, \"data\": serialize_for_js(data)})\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending update to web interface: {e}\")\n",
    "\n",
    "def send_clear():\n",
    "    try:\n",
    "        requests.post(\"http://lg_viz:3002/api/clear\", json={})\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending clear to web interface: {e}\")\n",
    "\n",
    "def stream_from_app(app_stream, input_buffer=[{\"messages\": []}], verbose=False, debug=False):\n",
    "    seen_metas = dict()\n",
    "    input_buffer = deepcopy(input_buffer)\n",
    "    send_clear()\n",
    "    while input_buffer:\n",
    "        for mode, chunk in app_stream(input_buffer.pop(), stream_mode = [\"values\", \"messages\", \"updates\", \"debug\"]): \n",
    "            # https://langchain-ai.github.io/langgraph/concepts/streaming/\n",
    "            if mode == \"messages\":\n",
    "                chunk, meta = chunk\n",
    "                if meta.get(\"checkpoint_ns\") not in seen_metas:\n",
    "                    caller_node = meta.get('langgraph_node')\n",
    "                    user_prompt = f\"node:{caller_node} -> message\" if verbose else caller_node.title()\n",
    "                    send_update(\"messages\", {\"meta\": meta})\n",
    "                    if verbose: \n",
    "                        print(f\"\\n[node:{meta.get('langgraph_node')}:meta -> message] {meta}\")\n",
    "                        # print(f\"\\n[messages:meta] {meta}\")\n",
    "                    print(f\"\\n[{user_prompt}] \", end=\"\")\n",
    "                seen_metas[meta.get(\"checkpoint_ns\")] = meta\n",
    "                if chunk.content:\n",
    "                    # print(chunk.content, end=\"\", flush=True)\n",
    "                    yield chunk.content\n",
    "                elif chunk.response_metadata:\n",
    "                    if verbose: \n",
    "                        print(f\"\\n\\n[message] {chunk.response_metadata=}, {chunk.usage_metadata=}\")\n",
    "            elif mode == \"values\":\n",
    "                if verbose: \n",
    "                    print(\"\\n[value]\", chunk)\n",
    "                send_update(\"values\", chunk)\n",
    "            elif mode == \"updates\":\n",
    "                if verbose:\n",
    "                    print(\"\\n[update]\", chunk)\n",
    "                send_update(\"updates\", chunk)\n",
    "                ## Handle the interrupt. If an interrupt happens, then handle the interrupt and re-queue app stream\n",
    "                if \"__interrupt__\" in chunk and chunk.get(\"__interrupt__\")[0].resumable:\n",
    "                    user_prompt = \"\\n[update -> interrupt] \" * bool(verbose) + chunk.get(\"__interrupt__\")[0].value\n",
    "                    input_buffer += [\n",
    "                        Command(resume=input(user_prompt))\n",
    "                    ]\n",
    "            elif mode == \"debug\":\n",
    "                send_update(\"debug\", chunk)\n",
    "                if debug:\n",
    "                    print(Fore.RED + f\"[debug] {chunk}\" + Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d560d88-c62e-4893-9676-cc86147ff646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:237: UserWarning: Default model is set as: meta/llama-3.1-8b-instruct. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[value] {'messages': []}\n",
      "\n",
      "[update] {'user': {'messages': [('user', 'No clue')]}}\n",
      "\n",
      "[value] {'messages': [HumanMessage(content='No clue', additional_kwargs={}, response_metadata={}, id='58da786f-5709-4e5b-afae-fa106e2c7bc8')]}\n",
      "\n",
      "[node:agent:meta -> message] {'langgraph_step': 2, 'langgraph_node': 'agent', 'langgraph_triggers': ['user'], 'langgraph_path': ('__pregel_pull', 'agent'), 'langgraph_checkpoint_ns': 'agent:2922bfd9-0a4a-5d19-ad1e-60fda2fec2da', 'checkpoint_ns': 'agent:2922bfd9-0a4a-5d19-ad1e-60fda2fec2da', 'ls_provider': 'NVIDIA', 'ls_model_name': 'meta/llama-3.1-8b-instruct', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024, 'ls_stop': None}\n",
      "\n",
      "[node:agent -> message] \n",
      "\n",
      "[message] chunk.response_metadata={'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, chunk.usage_metadata={'input_tokens': 12, 'output_tokens': 130, 'total_tokens': 142}\n",
      "\n",
      "[update] {'agent': {'messages': [AIMessage(content=\"It can be frustrating when you're stuck and don't know where to start.\\n\\nCan you tell me a bit more about what's on your mind? What are you trying to figure out or accomplish? I'm here to listen and help if I can.\\n\\nIf you're feeling stuck, we can try some exercises to get your creative juices flowing. For example, we could:\\n\\n* Play a game to get your mind off things\\n* Brainstorm ideas together\\n* Break down a problem into smaller, more manageable parts\\n* Explore a new topic or interest\\n\\nLet me know what sounds good to you, or if you have something else in mind!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-c15eb376-e765-41df-b821-a73a18d4c339', usage_metadata={'input_tokens': 1584, 'output_tokens': 8645, 'total_tokens': 10229})]}}\n",
      "\n",
      "[value] {'messages': [HumanMessage(content='No clue', additional_kwargs={}, response_metadata={}, id='58da786f-5709-4e5b-afae-fa106e2c7bc8'), AIMessage(content=\"It can be frustrating when you're stuck and don't know where to start.\\n\\nCan you tell me a bit more about what's on your mind? What are you trying to figure out or accomplish? I'm here to listen and help if I can.\\n\\nIf you're feeling stuck, we can try some exercises to get your creative juices flowing. For example, we could:\\n\\n* Play a game to get your mind off things\\n* Brainstorm ideas together\\n* Break down a problem into smaller, more manageable parts\\n* Explore a new topic or interest\\n\\nLet me know what sounds good to you, or if you have something else in mind!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-c15eb376-e765-41df-b821-a73a18d4c339', usage_metadata={'input_tokens': 1584, 'output_tokens': 8645, 'total_tokens': 10229})]}\n",
      "\n",
      "[update] {'user': {'messages': [('user', 'No clue')]}}\n",
      "\n",
      "[value] {'messages': [HumanMessage(content='No clue', additional_kwargs={}, response_metadata={}, id='58da786f-5709-4e5b-afae-fa106e2c7bc8'), AIMessage(content=\"It can be frustrating when you're stuck and don't know where to start.\\n\\nCan you tell me a bit more about what's on your mind? What are you trying to figure out or accomplish? I'm here to listen and help if I can.\\n\\nIf you're feeling stuck, we can try some exercises to get your creative juices flowing. For example, we could:\\n\\n* Play a game to get your mind off things\\n* Brainstorm ideas together\\n* Break down a problem into smaller, more manageable parts\\n* Explore a new topic or interest\\n\\nLet me know what sounds good to you, or if you have something else in mind!\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-c15eb376-e765-41df-b821-a73a18d4c339', usage_metadata={'input_tokens': 1584, 'output_tokens': 8645, 'total_tokens': 10229}), HumanMessage(content='No clue', additional_kwargs={}, response_metadata={}, id='36b778b9-7399-41d6-81b1-049c32c52fd7')]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m\n\u001b[1;32m     63\u001b[0m app_stream \u001b[38;5;241m=\u001b[39m partial(app\u001b[38;5;241m.\u001b[39mstream, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m##################################################################\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m## Simple Invocation Example\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m## We can stream over it until an interrupt is received\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_from_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(token, end=\"\", flush=True)\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m, in \u001b[0;36mstream_from_app\u001b[0;34m(app_stream, input_buffer, verbose, debug)\u001b[0m\n\u001b[1;32m     39\u001b[0m send_clear()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m input_buffer:\n\u001b[0;32m---> 41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# https://langchain-ai.github.io/langgraph/concepts/streaming/\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1724\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1724\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1731\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/pregel/runner.py:274\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    272\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_waiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/concurrent/futures/_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 305\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "# !pip install --upgrade langgraph colorama\n",
    "llm = ChatNVIDIA(temperature=0)\n",
    "\n",
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def get_nth_message(state: State, n=-1, attr=\"messages\"):\n",
    "    try: return state.get(\"messages\")[n].content\n",
    "    except: return \"\"\n",
    "    \n",
    "##################################################################\n",
    "## Define the operations (Nodes) that can happen on your environment\n",
    "\n",
    "def user(state: State):\n",
    "    \"\"\"Edge\" option where transition is generated at runtime\"\"\"\n",
    "    # answer = interrupt(\"[User]:\")\n",
    "    return {\"messages\": [(\"user\", \"No clue\")]} \n",
    "\n",
    "def agent(state: State, config=None):\n",
    "    ## Passing in config will cause connector to stream values to state modification buffer. See graph stream later\n",
    "    response = llm.invoke(state.get(\"messages\"), config=config)\n",
    "    update = {\"messages\": [response]}\n",
    "    if \"stop\" in get_nth_message(state, n=-1): \n",
    "        return update  ## Implied goto=END\n",
    "    return Command(update=update, goto=\"user\")\n",
    "\n",
    "##################################################################\n",
    "## Define the system that organizes your nodes (and maybe edges)\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"user\")  ## A start node is always necessary\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_node(\"user\", user)\n",
    "builder.add_edge(\"user\", \"agent\")\n",
    "\n",
    "##################################################################\n",
    "## A memory management system to keep track of agent state\n",
    "checkpointer = MemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "##################################################################\n",
    "## Simple Invocation Example\n",
    "\n",
    "## We can stream over it until an interrupt is received\n",
    "for token in stream_from_app(app_stream, verbose=True, debug=False):\n",
    "    # print(token, end=\"\", flush=True)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad41ec-728b-462f-9207-3f4b20044ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
